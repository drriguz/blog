<?xml version="1.0" encoding="UTF-8"?>
<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.11/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.11/ http://www.mediawiki.org/xml/export-0.11.xsd" version="0.11" xml:lang="en">
  <page>
    <title>Blog:Mac上的LaTeX环境搭建</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-02-06T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>一直希望能够自如的使用TeX来进行写作，但学习曲线还是比较高的，可惜断断续续一直没有能够入门。趁着这段时间疫情严重，待在家里又不想搞学习，那不如来重头开始学习一下吧。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3620" sha1="l0krnkgqzv0fvpku4j0wnawpk828qw2">
一直希望能够自如的使用TeX来进行写作，但学习曲线还是比较高的，可惜断断续续一直没有能够入门。趁着这段时间疫情严重，待在家里又不想搞学习，那不如来重头开始学习一下吧。

= 一些相关的概念=

TeX $/tɛx/$ 是高德纳（Donald Ervin Knuth）教授编写的排版软件，通俗来讲就是跟Word差不多的东西，但是TeX就好像是Markdown一样，跟编程语言差不多，不是那种所见即所得的。以下是一些相关的概念：

== TeX Engine==

TeX引擎就是实际可以运行TeX的二进制程序，主要有以下几种：

* Knuth的原始 TeX，只能支持plain tex格式，`tex &lt;somefile&gt;`这样。现在最新的版本是[ftp://ftp.cs.stanford.edu/pub/tex/tex14.tar.gz January 12, 2014发布的版本`3.14159265`]
* ε-TeX: 1990s后期发布的对TeX的一组增强扩展，实际上除了原始的TeX引擎其他的引擎都已经默认支持了这些特性
* pdfTeX: pdfTeX包含了PDF 和DVI格式的输出，被许多TeX的发行版用作默认的TeX引擎
* XeTeX: 同样包含了ε-TeX并原生支持Unicode和OpenType
* LuaTeX: 基于pdfTeX并支持Luau脚本的TeX引擎，最初被作为pdfTeX的下一代版本但事实上形成了一个独立的分支。同样，它支持ε-TeX，使用UTF8，并能够支持嵌入Lua脚本

== TeX 格式==

TeX是一个宏（macro）处理器，macro就像是编程语言中的函数一样，

&lt;syntaxhighlight lang="tex"&gt;
\def\foo{bar}
&lt;/syntaxhighlight&gt;
上面这个指令会将所有的`\foo`替换成`bar`。基于TeX有不同的格式，实际上就是一些macro的集合，相当于提供了一些库供用户使用，主要有以下这些：

* Plain TeX：原始的TeX发行版包含的基本指令集
* LaTeX2e: LaTeX的最新稳定版本（最新的试验版本是LaTeX3），所有的TeX程序都支持LaTeX2e
* ConTex: 另一种TeX系统

== 发行版==

TeX有许多种发行版，例如：

* [https://miktex.org/ MiKTeX]: 支持Windows的一种发行版
* [http://tug.org/texlive/ TeX Live]: 许多Linux/Unix默认的TeX系统，也支持Windows和Mac
* [http://tug.org/mactex/ MacTeX]: TeXLive的Mac版本

== 总结==

借用维基百科上的词条来总结一下吧，更加一目了然各个概念之间的区别：

[[File:tex-levels.png|600px|TeX Concepts]]

= Mac上的TeX环境=

Mac上推荐安装MacTeX。安装完成之后，可以看到一个TeXShop的编辑器，并可以在terminal中运行`tex`命令：

&lt;pre&gt;
tex
This is TeX, Version 3.14159265 (TeX Live 2019) (preloaded format=tex)
**
&lt;/pre&gt;

然后就是选择编辑器了，网上有不少教程，基于VSCode或者Sublime Text等的，在Mac上还有另一个选择就是Textmate了，在Textmate中安装`LaTeX`的Bundle即可，然后打开它的设置:

&lt;img src="/images/Textmate-latex-settings.png" style="width:300px"&gt;

编写完成之后，使用Command + R运行即可预览：

&lt;img src="/images/Textmate-latex-preview.png" style="width:800px"&gt;

值得注意的是，如果使用XeLaTeX要支持中文需要设置一下字体：

&lt;syntaxhighlight lang="tex"&gt;
\documentclass{article}
\usepackage{fontspec}
\setmainfont{Hiragino Sans GB}
\begin{document}
 Hello，中国！
\end{document}
&lt;/syntaxhighlight&gt;

参考：

* [https://en.wikipedia.org/wiki/TeX#cite_note-13 TeX]
* [https://tex.stackexchange.com/questions/49/what-is-the-difference-between-tex-and-latex What is the difference between TeX and LaTeX?]
* [http://tug.org/levels.html LaTeX vs. MiKTeX: The levels of TeX]
* [https://www.latex-tutorial.com/tutorials/ A simple guide to LaTeX - Step by Step]</text>
      <sha1>l0krnkgqzv0fvpku4j0wnawpk828qw2</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:LaTeX(1)：章节和段落</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-02-07T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>LaTeX适用于学术论文等的写作，在这类文章中一个很重要的部分就是段落和章节了。回想起当年使用Word写作的时候调整标题是何等的痛苦，那么在LaTeX中是怎样设置段落和章节的呢？</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3403" sha1="jsmgmwdaylwtaiuce1o7rd7kftk9i66">
LaTeX适用于学术论文等的写作，在这类文章中一个很重要的部分就是段落和章节了。回想起当年使用Word写作的时候调整标题是何等的痛苦，那么在LaTeX中是怎样设置段落和章节的呢？

= 标题=

通过下面的命令可以生成一个默认的标题：

&lt;syntaxhighlight lang="latex"&gt;
\title{一种新型冠状病毒的防治方法}
\author{Dr. Riguz}
\date{2020-02-07}

\maketitle % 生成标题页
\newpage   % 分页
&lt;/syntaxhighlight&gt;
这样效果如下：

&lt;img src="/images/Latex-title.png" style="width:400px"&gt;


= LaTeX段落层次=
LaTeX中可以分成如下的几个部分：

* section：一级标题
* subsection：二级标题
* subsubsection：三级标题
* paragraph：段落
* subpragraph：二级段落

下面是一个例子：

&lt;img src="/images/Latex-sections.png" style="width:400px"&gt;

对应的代码如下，非常简洁：

&lt;syntaxhighlight lang="latex"&gt;
\section{背景}
首先简要介绍一下这个项目的背景。
\subsection{冠状病毒简介}
冠状病毒是一个大型病毒家族，包括引起普通感冒的病毒以及严重急性呼吸综合征冠状病毒和中东呼吸综合征冠状病毒。这一新病毒暂时命名为2019新型冠状病毒（2019-nCoV）。
\subsection{现有的防治方法}
\subsubsection{居家隔离法}
待在家里不出门。
\subsubsection{自我安慰法}
反正也死不了。
\section{新的防治方法}
实在编不下去了：
\paragraph{气功}
是一种中华民族祖传的神功。气功又可以分为两种：
\subparagraph{硬气功}
可以开山断石，金钟罩铁布衫。
\subparagraph{内功}
可以运行体内的真气激发一种神奇的力量运行于经络之上。
&lt;/syntaxhighlight&gt;

= 生成目录=
== 目录==
想要生成目录页特别简单，一个命令就可以搞定：

&lt;syntaxhighlight lang="latex"&gt;
\tableofcontents
\newpage
&lt;/syntaxhighlight&gt;

&lt;img src="/images/Latex-contents.png" style="width:400px"&gt;

== 自定义级别==

如果在生成目录的时候，只希望生成到特定的级别，可以这样设置：

&lt;syntaxhighlight lang="latex"&gt;
\setcounter{tocdepth}{2}
\tableofcontents
&lt;/syntaxhighlight&gt;

这样生成的目录就只会到`subsection`级别。当然这样设置的话会对所有的内容生效，如果只是希望对某个章节生效怎么办呢？

&lt;syntaxhighlight lang="latex"&gt;
\addtocontents{toc}{\setcounter{tocdepth}{1}}
\section{背景}

% ...
\addtocontents{toc}{\setcounter{tocdepth}{3}}
\section{新的防治方法}
&lt;/syntaxhighlight&gt;
可以像上面这样，在需要设置的章节之前设置，然后再其他地方恢复默认级别。

== 行距==
如果希望调整行距，可以使用

&lt;syntaxhighlight lang="latex"&gt;
\doublespacing
\tableofcontents
\newpage
\singlespacing
&lt;/syntaxhighlight&gt;

单倍行距：

&lt;img src="/images/Latex-singlespacing.png" style="width:400px"&gt;

双倍行距：

&lt;img src="/images/Latex-doublespacing.png" style="width:400px"&gt;

== 图表目录==
另外，如果想对图表生成目录，可以采用：

&lt;syntaxhighlight lang="latex"&gt;
\begin{appendix}
	\listoffigures
	\listoftables
\end{appendix}	
&lt;/syntaxhighlight&gt;

参考：

* [https://www.latex-tutorial.com/tutorials/sections/ Using LaTeX paragraphs and sections]
* [http://yakeworld.myesci.com/node/1397 xelatex中文换行]
* [https://www.ntg.nl/doc/biemesderfer/ltxcrib.pdf LATEX Command Summary]</text>
      <sha1>jsmgmwdaylwtaiuce1o7rd7kftk9i66</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:LaTeX(2)：插入图片</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-02-08T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>在LaTeX中插入图片有些类似于Markdown中的方式。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4126" sha1="jpt4h03tgjw4lrnwehlag3khoqyvstt">
在LaTeX中插入图片有些类似于Markdown中的方式。

= Figure和图片=
可以使用Figure的方式插入图片：

&lt;syntaxhighlight lang="latex"&gt;
\usepackage{graphicx} % 需要导入graphicx包

\begin{figure}[h!]
	\includegraphics[width=\linewidth]{src/1.jpg}
	\caption{冠状病毒的示意图}
	\label{fig:virus}
\end{figure}

&lt;/syntaxhighlight&gt;
这样生成的效果如图所示：
&lt;img src="/images/Latex-image-figure.png" style="width:400px"&gt;

值得注意的是，在LaTeX中，图片和表格等是浮动元素，Tex会自动帮你找个地方放着。据说这样做的原因是因为它希望你专注在写作内容上面，等你完成了内容再回头来调整格式。因为没有这种经验，所以不确定这样是否是最好的方式，不过我们可能更习惯直接放在我们想放的位置上，所以这里是必须的：

&lt;syntaxhighlight lang="latex"&gt;
begin{figure}[h]
&lt;/syntaxhighlight&gt;

这里后面有一个`h!`，有这样一些可用的参数：

* $h$ere: 就放在这里
* $t$op： 放在页面顶部
* $b$ottom: 页面底部
* $p$age: 放在单独的页面
* $!$(Override)：强制放在指定的位置

虽然我们已经用了`h!`来强制将图片放在页面的位置，但是很有可能页面余下的空间并不足以放入这张图片，这个时候，这个选项会失效。如果依然确定要将图片放入到下面，可以引入`float`包，使用`H`选项：

&lt;syntaxhighlight lang="latex"&gt;
\usepackage{float}

\begin{figure}[H]
&lt;/syntaxhighlight&gt;

就像下面这样：

&lt;img src="/images/Latex-image-float-H.png" style="width:400px"&gt;

= 多张图片排版=
如果要将多张图片排版到一起，可以有多种方法。

== 使用`subfig`包==

[https://ctan.org/pkg/subfig subfig]是一个可以支持嵌套Figure或者表格的包，这个包是`subfigure`包的替代品，后者已经被弃用了。

&lt;syntaxhighlight lang="latex"&gt;
\usepackage{subfig}

\begin{figure}
	\centering
	\subfloat[信息锅]{
		\includegraphics[width=0.4\linewidth, height=80pt]{src/3.jpg}
	}
	\subfloat[硬气功]{
		\includegraphics[width=0.4\linewidth, height=80pt]{src/4.jpg}
	}
	\caption{两种气功流派}
	\label{fig:qg}
\end{figure}
&lt;/syntaxhighlight&gt;

这样排版出来的效果如下：

&lt;img src="/images/Latex-subfig.png" style="width:400px"&gt;

== 使用`subfloat`包==

[www.ctan.org/pkg/subfloat subfloat]是另一个包，用法如下：

&lt;syntaxhighlight lang="latex"&gt;
\begin{subfigures}
\begin{figure}
	\centering
	\fbox{
		\includegraphics[width=0.4\linewidth, height=80pt]{src/3.jpg}
	}
	\caption{信息锅}
\end{figure}
\begin{figure}
	\centering
	\fbox{
		\includegraphics[width=0.4\linewidth, height=80pt]{src/4.jpg}
	}
	\caption{硬气功}
\end{figure}
\end{subfigures}
&lt;/syntaxhighlight&gt;
这样出来的效果是这样：
&lt;img src="/images/Latex-subfloag.png" style="width:400px"&gt;

两张图片不在一行，没有找到怎么把他们放到一起的方法，也不知道是否支持。

== 使用`subcaption`包==

另有使用[http://www.ctan.org/pkg/caption caption]和[http://www.ctan.org/pkg/subcaption subcaption]包结合的方式。`subcaption`包更加新一些，可能更适合选用。

&lt;syntaxhighlight lang="latex"&gt;
\usepackage{caption}
\usepackage{subcaption} 

\begin{figure}
\centering
\subcaptionbox{信息锅}{
	\includegraphics[width=0.40\textwidth, height=80pt]{src/3.jpg}
}
\hfill
\subcaptionbox{硬气功}{
	\includegraphics[width=0.40\textwidth, height=80pt]{src/4.jpg}
}
\caption{两种气功流派}
\end{figure}
&lt;/syntaxhighlight&gt;
注意subcaption包貌似和前面两种方法中的某一种可能不兼容。效果如下：
&lt;img src="/images/Latex-subcaption.png" style="width:400px"&gt;

如果去掉`\hfill`之后，效果基本上和第一种差不多了。

参考：

* [https://tex.stackexchange.com/questions/16207/image-from-includegraphics-showing-up-in-wrong-location Image from \includegraphics showing up in wrong location?]
* [https://tex.stackexchange.com/questions/122314/figures-what-is-the-difference-between-using-subfig-or-subfigure Figures: What is the difference between using subfig or subfigure]</text>
      <sha1>jpt4h03tgjw4lrnwehlag3khoqyvstt</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:LaTeX(3)：使用TikZ绘制图形</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-02-09T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>[PGF](https://www.ctan.org/pkg/pgf)是一个用来进行图形绘制的（底层）包，TikZ是利用这个包实现的用户友好的接口。所以通常在LaTeX中会用TikZ来进行矢量图形的绘制。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4960" sha1="cjynakznzrlycl9cz927ld0fv5ge99t">
[https://www.ctan.org/pkg/pgf PGF]是一个用来进行图形绘制的（底层）包，TikZ是利用这个包实现的用户友好的接口。所以通常在LaTeX中会用TikZ来进行矢量图形的绘制。


= 基本概念=
== 基本语法==
要使用tikz进行图形绘制只需要简单引入tikz宏包，并将绘制代码包含在一个上下文中就可以了:

&lt;syntaxhighlight lang="latex"&gt;
\usepackage{tikz}

\begin{tikzpicture}
% ...
\end{tikzpicture}

&lt;/syntaxhighlight&gt;

同样如果希望把图片作为一个figure，那么再套一层:

&lt;syntaxhighlight lang="latex"&gt;
\usepackage{graphicx}
\usepackage{tikz}

\begin{document}
	\begin{figure}[h]
	\begin{tikzpicture}
		\draw (0, 0) -- (1, 1);
	\end{tikzpicture}
	\caption{这是一条直线}
	\end{figure}
\end{document}
&lt;/syntaxhighlight&gt;

== 坐标系和缩放==
默认情况下TikZ使用的是笛卡尔坐标系，即是这样：

&lt;img src="/images/Latex-tikz-coordinates.png" style="width:400px"&gt;

另外，TikZ提供了一个缩放的选项，可以用来缩放图形，所以不必要担心绝对坐标的问题：

&lt;syntaxhighlight lang="latex"&gt;
\begin{tikzpicture}[scale=0.5]
&lt;/syntaxhighlight&gt;

甚至还可以针对x轴和y轴分别设置缩放：

&lt;syntaxhighlight lang="latex"&gt;
\begin{tikzpicture}[xscale=0.5, yscale=0.3]
&lt;/syntaxhighlight&gt;

&lt;img src="/images/Latex-tikz-scale.png" style="width:400px"&gt;

在TikZ中，默认的单位是厘米(cm)。如果希望改变这个值，可以这样设置：

&lt;syntaxhighlight lang="latex"&gt;
\begin{tikzpicture}[x=2cm,y=1.5cm]

% 或者
\begin{tikzpicture}[x={(2cm,0cm)},y={(0cm,1.5cm)}]
&lt;/syntaxhighlight&gt;

= 图形绘制=

== 直线==

绘制直线：

&lt;syntaxhighlight lang="latex"&gt;
\draw (0, 0) -- (1, 1);
&lt;/syntaxhighlight&gt;

绘制折线：

&lt;syntaxhighlight lang="latex"&gt;
\draw (0, 0) -- (1, 1) -- (2, 2) -- (1, 0) -- (0, 3);
&lt;/syntaxhighlight&gt;

绘制背景网格:

&lt;syntaxhighlight lang="latex"&gt;
\draw[help lines] (0,0) grid (3,3);
&lt;/syntaxhighlight&gt;
&lt;img src="/images/Latex-tikz-lines.png" style="width:400px"&gt;

=== 箭头===
如果希望绘制箭头也十分方便：

&lt;syntaxhighlight lang="latex"&gt;
\draw [-&gt;] (0,0) -- (2,0);        %→
\draw [&lt;-] (0, -0.5) -- (2,-0.5); %←
\draw [|-&gt;] (0,-1) -- (2,-1);     %带尾巴的箭头
\draw [&lt;-&gt;] (0, 0) -- (1, 1);     %双向箭头
&lt;/syntaxhighlight&gt;

=== 线的粗细===

线的粗细可以用如下来表示：

&lt;syntaxhighlight lang="latex"&gt;
\draw [ultra thin] (0, 1) -- (2, 1)
&lt;/syntaxhighlight&gt;

总共可用的粗细如下：

&lt;img src="/images/Latex-line-width.png" style="width:400px"&gt;

或者直接指定线的粗细，默认的单位是点:

&lt;syntaxhighlight lang="latex"&gt;
\draw [line width=12] (0,0) -- (2,0);
\draw [line width=0.2cm] (4,.75) -- (5,.25);
&lt;/syntaxhighlight&gt;

除此之外另一个选项是`[help lines]`，用来绘制灰色的参考线。

&lt;syntaxhighlight lang="latex"&gt;
\draw [help lines] (0, 5) -- (0, 0) -- (5, 0);
\draw [line width=2pt] (0, 0) -- (5, 5);
\draw [very thin] (0, 3) -- (4, 0);
\draw [thin] (0, 2) -- (5, 2);
&lt;/syntaxhighlight&gt;

=== 样式及颜色===

样式可以分为：

* 虚线 `\draw [dashed] `
* 实线 `\draw [dotted] `

颜色有很多直接可以用的颜色表示，类似css一样：

&gt; red, green, blue, cyan, magenta, yellow, black, gray, darkgray, lightgray,brown, lime, olive, orange, pink, purple, teal, violetand white

一个较为完整的例子:

&lt;syntaxhighlight lang="latex"&gt;
\begin{tikzpicture}
	\draw [help lines] (0, 5) -- (0, 0) -- (5, 0) node [right=3]{Nice sample!};
	\draw [dashed, red, line width=2pt] (0, 0) -- (5, 5);
	\draw [blue, very thin] (0, 3) -- (4, 0);
	\draw [dotted, thin] (0, 2) -- (5, 2);
\end{tikzpicture}
&lt;/syntaxhighlight&gt;

== 几何图形==
=== 矩形===

&lt;syntaxhighlight lang="latex"&gt;
\draw [blue] (0,0) rectangle (1.5, 1);
&lt;/syntaxhighlight&gt;

=== 网格===

&lt;syntaxhighlight lang="latex"&gt;
\draw [blue] (0,0) grid (1.5, 1);
&lt;/syntaxhighlight&gt;

=== 圆===

&lt;syntaxhighlight lang="latex"&gt;
\draw [red, ultra thick] (3, 0.5) circle [radius=0.5];
&lt;/syntaxhighlight&gt;

=== 弧线===

&lt;syntaxhighlight lang="latex"&gt;
\draw [gray, ultra thick] (6,0) arc [radius=1, start angle=45, end angle= 120];
&lt;/syntaxhighlight&gt;

这个弧线的表示方法比较有意思，代表从(6, 0)出发，半径为1，初始角度为45°，当变成120°的时候停止。另一种方法：

&lt;syntaxhighlight lang="latex"&gt;
\draw[very thick] (0,0) to [out=90,in=195] (2,1.5);
&lt;/syntaxhighlight&gt;

表示从(0,0)开始， 到(2, 1.5)这个点，起始角度为90°，到达的角度为195°。

感觉很难控制这个....

=== 圆角折线===

加多一个`rounded corners`就可以把折线变成圆角的了：

&lt;syntaxhighlight lang="latex"&gt;
\draw [&lt;-&gt;, rounded corners, thick, purple] (0,2) -- (0,0) -- (3,0);
&lt;/syntaxhighlight&gt;

* [https://www.latex-tutorial.com/tutorials/tikz/ Draw pictures in LaTeX - With tikz/pgf]
* [http://cremeronline.com/LaTeX/minimaltikz.pdf A very minimal introduction to TikZ∗]
</text>
      <sha1>cjynakznzrlycl9cz927ld0fv5ge99t</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:抓取QQ空间皮肤图片</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2017-01-11T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>最近把博客重新整理了一下，博文设置featured image果然看起来现代不少，但是要去哪找这么多合适的图片呢？当然PS是一个不错的选择，但是费时费力。看到QQ空间的皮肤倒是做的不错，直接拿来用吧，反正不违法。于是想用爬虫抓取。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1949" sha1="l37b6mzuluz0s3hhjebb33zv7anzomp">最近把博客重新整理了一下，博文设置featured image果然看起来现代不少，但是要去哪找这么多合适的图片呢？当然PS是一个不错的选择，但是费时费力。看到QQ空间的皮肤倒是做的不错，直接拿来用吧，反正不违法。于是想用爬虫抓取。
先调试一下，找几个图片看看：
&lt;pre&gt;
http://i.gtimg.cn/qzone/space_item/orig/3/103603_top.jpg
http://i.gtimg.cn/qzone/space_item/orig/7/101703_top.jpg
http://i.gtimg.cn/qzone/space_item/orig/5/108725_top.jpg
http://i.gtimg.cn/qzone/space_item/orig/15/111327_top.jpg
http://i.gtimg.cn/qzone/space_item/orig/8/106904_top.jpg
http://i.gtimg.cn/qzone/space_item/orig/10/102490_top.jpg
&lt;/pre&gt;
可以看到前面的网址都是一样的`orig/%d/%d_top.jpg`，刚开始以为前面的一个数字是主题编号还是什么的，后面自然是图片ID，于是用以下的脚本抓取（Scrapy):
&lt;syntaxhighlight lang="python"&gt;
def start_requests(self):
    for i in range(0, 100):
        for j in range(100000, 119999):
            url = 'http://i.gtimg.cn/qzone/space_item/orig/%d/%d_top.jpg' % (i, j)        
            yield scrapy.Request(url=url, callback=self.parse)
&lt;/syntaxhighlight&gt;
后来抓取完成后，一共15个文件夹，每个文件夹差不多都是50-80个左右，这就有点意思了，可能腾讯利用了分布式的图片服务器，分散到0-15个不同的服务器上。随便找一个看看：
101195_top.jpg在11下面，通常取余的方式实现，于是来试试：
&lt;pre&gt;
101195% 16 = 11
&lt;/pre&gt;
另外再验证几次也都是正确的，证明的我们的猜想。于是我们可以修改一下我们的爬虫了：
&lt;syntaxhighlight lang="python"&gt;
for i in range(100000, 200000):
    url = 'http://i.gtimg.cn/qzone/space_item/orig/%d/%d_top.jpg' % (i % 16, i) 
    yield scrapy.Request(url=url, callback=self.parse)
&lt;/syntaxhighlight&gt;
这样抓取就快多了，总共抓取了1232个图片。</text>
      <sha1>l37b6mzuluz0s3hhjebb33zv7anzomp</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:TCP(1)：TCP协议概述</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-09-04T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>TCP(Transmission Control Protocol)协议是面向连接的可靠的、基于字节流的传输层通信协议，工作在OSI模型中的第四层（传输层）上。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6746" sha1="s59mim0pbtcr49aiqacns0huuf8gw0n">TCP(Transmission Control Protocol)协议是面向连接的可靠的、基于字节流的传输层通信协议，工作在OSI模型中的第四层（传输层）上。


= 基本概念=
== OSI模型==
ISO在1984年定义了网络互连的7层结构模型，分别是：

1. 物理层(Physical Layer)在局域网传输数据帧(Data Frame)，负责管理电脑通信设备和网络媒体之间的互通
=. 数据链路层（Data Link Layer）负责网络寻址、错误侦测和改错=
=. 网络层（Network Layer）决定数据的路径选择和转寄，将网络表头（NH）加至数据包，以形成报文。IP协议就在这一层。=
=. 传输层（Transport Layer）把传输表头（TH）加至数据以形成数据包。=
=. 会话层（Session Layer）负责在数据传输中设置和维护计算机网络中两台计算机之间的通信连接。=
=. 表达层（Presentation Layer）把数据转换为能与接收者的系统格式兼容并适合传输的格式。=
=. 应用层（Application Layer）提供为应用软件而设的接口，以设置与另一应用软件之间的通信。=

&lt;img src="https://miro.medium.com/max/688/1*bXWAnvmLdUPi3mx5mZZn9Q.png" style="height:300px"/&gt;

== 原理==
在IP（Internet Protocol)层的任务仅仅是根据源主机和目的主机的地址来传送数据，数据在IP层传送时会被封装为数据包，但比较独特的是在传输数据之前无需与目的主机建立连接。IP协议提供了不可靠的数据传输机制（或者称之为“尽最大努力交付”），但无法保证数据能够准确的传输，因为数据包在到达的时候可能出现故障，包括：

* 数据损坏
* 丢失
* 重复
* 顺序错乱

IP协议不能保证可靠性，即便IPv4在数据报头中有校验和的计算，但假设计算出校验和不正确则直接丢掉错误的包，而不会通知到任一个终端节点。在Ipv6中为了快速传输已经放弃了计算校验和。而TCP协议可以提供在不同的主机的应用层之间进行可靠的、管道般的连接。整个过程是：应用层向TCP层发送数据流，TCP将数据流分割适当长度的报文段（通常受限于数据链路层的最大传输单元MTU）,而后将包传输到IP层，并传送给接收端的TCP层。为了保证不丢包和顺序性，TCP给每一个包一个序号，接收端接受到包后会发送ACK信息确认；如果在往返时延（RTT）内未收到确认，那么对应的数据包就被认为已经丢失并进行重传。同时，TCP使用一个校验和来检测数据是否存在错误，在发送和接受的时候都需要计算。

= 运作方式=
TCP协议运行分为三个阶段：创建连接、数据传送和连接终止。首先来看TCP的数据包的结构：

== TCP包结构==

TCP将数据封装在IP报文中进行传输。

&lt;syntaxhighlight lang="java"&gt;
&lt;-------------------------------IP报文----------------------------------&gt;
|-------IP头部--------|-------TCP头部--------|-----------TCP数据----------|
&lt;/syntaxhighlight&gt;
其中，IPv4下IP头部为20字节（不带选项，如果带选项可能更长），IPv6为40字节。TCP头部20字节（不带选项），TCP包的结构如下：

[[File:Tcp-package.png|600px|TCP package structure]]

* 来源端口 发送连接端口，这个端口与IP报文中的IP地址组合到一起唯一标识一个TCP连接（也即Socket）
* 目的端口 接收连接端口
* 序列号：如果含有`SYN`标记，则此为最初的序列号；第一个资料的序列号将+1；如果没有则为第一个资料比特的序列号
* 确认号：确认号为最后接收到的数据字节的序列号+1
* 资料偏移：以4字节为单位计算出的数据段开始地址的偏移值
* 保留位：须置位0
* 标志位：
    - NS(ECN-nonce)：ECN显式拥塞通知(Explicit Congestion Notification)，是对TCP的扩展，用来允许拥塞控制的端对端通知而避免丢包。
    - CWR：拥塞窗口减，发送方降低发送速率
    - ECE：ECN回显，发送方接收到一个更早的拥塞通告
    - URG：紧急指针字段有效，很少被使用
    - ACK： 确认号字段有效，连接建立以后一般都是启动状态
    - PSH：推送，接收方应尽快给应用程序传送这个数据（没有被可靠实现或者用到）
    - RST：重置连接
    - SYN：初始化一个连接的同步序列号
    - FIN：该报文段的发送方已经结束向对方发送数据

== 连接创建与关闭==
TCP使用三次握手（three-way handshake）来创建一个连接。这个过程是：

1. 客户端向服务端发送`SYN`来创建一个主动打开，客服端设定这段连接的序号为随机数 $ISN(c)$
=. 服务端收到`SYN`后回复`SYN/ACK`，确认号为$ISN(c)+1$，而这个回复包含一个随机序号$ISN(s)$=
=. 最后，客户端再次发送一个`ACK`,此时包的序号为$ISN(c) + 1$，服务端回复的确认码为$ISN(s) + 1$。这样就完成了三次握手，进入了连接创建状态=

[[File:Tcp-connection.png|600px|Tcp connection flow]]

客户端和服务端任何一方都可以发起连接关闭的操作。这个过程如下：

1. 主动关闭者发送一个`FIN`指明希望看到自己当前的序列号(K)，并包含了对方最近一次的数据序列号(L)
=. 被动关闭者发送`ACK`表明已经成功接收到`FIN`，并通知上层的应用程序连接端已经提出了关闭操作=
=. 接着被动关闭者将变成主动关闭者，并发送`FIN`=
=. 收到`ACK`，完成连接的关闭=

因为TCP是双工通信，实际上支持一种半关闭的操作，也就是说一方数据发送完成后关闭传输但任然保留接收数据的能力。这种情况下只有一方发送了`FIN`，但实际很少有程序使用这种特性。

= 其他需要了解的性质=

由于TCP协议的设计，有一些相关的需要了解的性质如下：

* TCP是面向流的，无法知道（或者说不关心）消息的边界。例如在一端发送时可能先发送10个字节，而后40个；但在接收端可能是每次读取10个字节或者说一次性读入，即无法知道字节写入的边界（需要上层协议支持）
* TCP为了处理发送端和接收端的速度不匹配的问题，可以采取两种方式的流量控制，一种是基于速率（通常用于广播和组播发现），另一种是基于窗口的，通过窗口通告（或者称之为窗口更新）来更新窗口大小。
* 对于发送端和接收端的中间网络，单靠流量控制是无法避免由于中间路由器之类的设备拥塞造成丢包的问题；所以当发送方识别到这样的丢包出现时会猜测这是由于中间网络拥塞导致的，从而开始隐式的拥塞控制。</text>
      <sha1>s59mim0pbtcr49aiqacns0huuf8gw0n</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:TCP(2)：TCP报文实例</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-09-14T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>TCP每个报文都有一个序列号，这个序列号在初始的时候是随机生成的（而不是直接使用0或者1），那么这样做的原因究竟是为什么？</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7735" sha1="h33kbel8n5bve3gylev1s9qivyw1dgs">
TCP每个报文都有一个序列号，这个序列号在初始的时候是随机生成的（而不是直接使用0或者1），那么这样做的原因究竟是为什么？


= TCP 实例=
使用[https://www.wireshark.org/ Wireshark]和[https://sourceforge.net/projects/sockettest/ SocketTest]在本地进行数据发送和接收，可以抓取到TCP报文。首先使用SocketTest在本地(127.0.0.1)的10240端口上启动一个Socket服务器，然后使用它的客户端功能连接这个端口，并在Wireshark中监听本地回环网卡(localhost)，即可抓取到对应的数据包。

== 三次握手==

[[File:Wireshark-lo-1.png|600px|3-way Handshake]]

可以看出，是一个标准的三次握手的过程，握手完成之后，服务端给客户端发送了一个Window Update。

=== 1:客户端发送SYN===

&lt;syntaxhighlight lang="lua"&gt;
Frame 13: 68 bytes on wire (544 bits), 68 bytes captured (544 bits) on interface lo0, id 0
Null/Loopback
Internet Protocol Version 4, Src: 127.0.0.1, Dst: 127.0.0.1
Transmission Control Protocol, Src Port: 64255, Dst Port: 10240, Seq: 0, Len: 0
    Source Port: 64255
    Destination Port: 10240
    [Stream index: 0]
    [TCP Segment Len: 0]
    Sequence number: 0    (relative sequence number)
    Sequence number (raw): 3261314628
    [Next sequence number: 1    (relative sequence number)]
    Acknowledgment number: 0
    Acknowledgment number (raw): 0
    1011 .... = Header Length: 44 bytes (11)
    Flags: 0x002 (SYN)
    Window size value: 65535
    [Calculated window size: 65535]
    Checksum: 0xfe34 [unverified]
    [Checksum Status: Unverified]
    Urgent pointer: 0
    Options: (24 bytes), Maximum segment size, No-Operation (NOP), Window scale, No-Operation (NOP), No-Operation (NOP), Timestamps, SACK permitted, End of Option List (EOL)
        TCP Option - Maximum segment size: 16344 bytes
        TCP Option - No-Operation (NOP)
        TCP Option - Window scale: 6 (multiply by 64)
        TCP Option - No-Operation (NOP)
        TCP Option - No-Operation (NOP)
        TCP Option - Timestamps: TSval 740812626, TSecr 0
        TCP Option - SACK permitted
        TCP Option - End of Option List (EOL)
    [Timestamps]
&lt;/syntaxhighlight&gt;

=== 2:服务端回复SYN/ACK===

&lt;syntaxhighlight lang="lua"&gt;
Frame 14: 68 bytes on wire (544 bits), 68 bytes captured (544 bits) on interface lo0, id 0
Null/Loopback
Internet Protocol Version 4, Src: 127.0.0.1, Dst: 127.0.0.1
Transmission Control Protocol, Src Port: 10240, Dst Port: 64255, Seq: 0, Ack: 1, Len: 0
    Source Port: 10240
    Destination Port: 64255
    [Stream index: 0]
    [TCP Segment Len: 0]
    Sequence number: 0    (relative sequence number)
    Sequence number (raw): 1175996045
    [Next sequence number: 1    (relative sequence number)]
    Acknowledgment number: 1    (relative ack number)
    Acknowledgment number (raw): 3261314629
    1011 .... = Header Length: 44 bytes (11)
    Flags: 0x012 (SYN, ACK)
    Window size value: 65535
    [Calculated window size: 65535]
    Checksum: 0xfe34 [unverified]
    [Checksum Status: Unverified]
    Urgent pointer: 0
    Options: (24 bytes), Maximum segment size, No-Operation (NOP), Window scale, No-Operation (NOP), No-Operation (NOP), Timestamps, SACK permitted, End of Option List (EOL)
        TCP Option - Maximum segment size: 16344 bytes
        TCP Option - No-Operation (NOP)
        TCP Option - Window scale: 6 (multiply by 64)
        TCP Option - No-Operation (NOP)
        TCP Option - No-Operation (NOP)
        TCP Option - Timestamps: TSval 740812626, TSecr 740812626
        TCP Option - SACK permitted
        TCP Option - End of Option List (EOL)
    [SEQ/ACK analysis]
    [Timestamps]

&lt;/syntaxhighlight&gt;

=== 3:客户端回复ACK===

&lt;syntaxhighlight lang="lua"&gt;
Frame 15: 56 bytes on wire (448 bits), 56 bytes captured (448 bits) on interface lo0, id 0
Null/Loopback
Internet Protocol Version 4, Src: 127.0.0.1, Dst: 127.0.0.1
Transmission Control Protocol, Src Port: 64255, Dst Port: 10240, Seq: 1, Ack: 1, Len: 0
    Source Port: 64255
    Destination Port: 10240
    [Stream index: 0]
    [TCP Segment Len: 0]
    Sequence number: 1    (relative sequence number)
    Sequence number (raw): 3261314629
    [Next sequence number: 1    (relative sequence number)]
    Acknowledgment number: 1    (relative ack number)
    Acknowledgment number (raw): 1175996046
    1000 .... = Header Length: 32 bytes (8)
    Flags: 0x010 (ACK)
    Window size value: 6379
    [Calculated window size: 408256]
    [Window size scaling factor: 64]
    Checksum: 0xfe28 [unverified]
    [Checksum Status: Unverified]
    Urgent pointer: 0
    Options: (12 bytes), No-Operation (NOP), No-Operation (NOP), Timestamps
        TCP Option - No-Operation (NOP)
        TCP Option - No-Operation (NOP)
        TCP Option - Timestamps: TSval 740812626, TSecr 740812626
    [SEQ/ACK analysis]
    [Timestamps]

&lt;/syntaxhighlight&gt;

== 数据发送==
通过像服务器发送一个"helloworld"来查看数据是如何发送的：

=== 1:客户端发送PSH/ACK===
&lt;syntaxhighlight lang="lua"&gt;
Transmission Control Protocol, Src Port: 64255, Dst Port: 10240, Seq: 1, Ack: 1, Len: 12
    Source Port: 64255
    Destination Port: 10240
    [Stream index: 0]
    [TCP Segment Len: 12]
    Sequence number: 1    (relative sequence number)
    Sequence number (raw): 3261314629
    [Next sequence number: 13    (relative sequence number)]
    Acknowledgment number: 1    (relative ack number)
    Acknowledgment number (raw): 1175996046
    1000 .... = Header Length: 32 bytes (8)
    Flags: 0x018 (PSH, ACK)
    Window size value: 6379
    [Calculated window size: 6379]
    [Window size scaling factor: -1 (unknown)]
    Checksum: 0xfe34 [unverified]
    [Checksum Status: Unverified]
    Urgent pointer: 0
    Options: (12 bytes), No-Operation (NOP), No-Operation (NOP), Timestamps
        TCP Option - No-Operation (NOP)
        TCP Option - No-Operation (NOP)
        TCP Option - Timestamps: TSval 742069804, TSecr 740812626
    [SEQ/ACK analysis]
    [Timestamps]
    TCP payload (12 bytes)
&lt;/syntaxhighlight&gt;

=== 2:服务端回复ACK===
&lt;syntaxhighlight lang="lua"&gt;
Transmission Control Protocol, Src Port: 10240, Dst Port: 64255, Seq: 1, Ack: 13, Len: 0
    Source Port: 10240
    Destination Port: 64255
    [Stream index: 0]
    [TCP Segment Len: 0]
    Sequence number: 1    (relative sequence number)
    Sequence number (raw): 1175996046
    [Next sequence number: 1    (relative sequence number)]
    Acknowledgment number: 13    (relative ack number)
    Acknowledgment number (raw): 3261314641
    1000 .... = Header Length: 32 bytes (8)
    Flags: 0x010 (ACK)
    Window size value: 6379
    [Calculated window size: 6379]
    [Window size scaling factor: -1 (unknown)]
    Checksum: 0xfe28 [unverified]
    [Checksum Status: Unverified]
    Urgent pointer: 0
    Options: (12 bytes), No-Operation (NOP), No-Operation (NOP), Timestamps
        TCP Option - No-Operation (NOP)
        TCP Option - No-Operation (NOP)
        TCP Option - Timestamps: TSval 742069804, TSecr 742069804
    [SEQ/ACK analysis]
        [This is an ACK to the segment in frame: 13]
        [The RTT to ACK the segment was: 0.000040000 seconds]
    [Timestamps]

&lt;/syntaxhighlight&gt;

= 序列号与ACK=

1. (Client) SYN, Seq=3261314628
=. (Server) SYN/ACK, Seq=1175996045, ACK=3261314629=
=. (Client) ACK, Seq=3261314629, ACK=1175996046=
=. (Client) PSH/ACK, Seq=3261314629, ACK=1175996046, Data(12bytes)=
=. (Server) ACK, Seq=1175996046, ACK=3261314641=
=. (Client) FIN/ACK, Seq=3261314641, ACK=1175996046=
=. (Server) ACK, Seq=1175996046, ACK=3261314642=
=. (Server) FIN, ACK, Seq=1175996046, ACK=3261314642=
=. (Client) ACK, Seq=3261314642, ACK=1175996047=

</text>
      <sha1>h33kbel8n5bve3gylev1s9qivyw1dgs</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:有趣的 Unicode 字符</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2018-03-09T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment />
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1302" sha1="gqz5kb0f4us6z1ce0cpvx7tb0twwax4">&lt;pre&gt;

ส็็็็็็็็็็็็็็็็็็็็็็็็็༼ ຈل͜ຈ༽ส้้้้้้้้้้้้้้้้้้้้้้้
ส็็็็็็็็็็็็็็็็็็็็็็็็็༼ ಠ_ಠ ༽ส้้้้้้้้้้้้้้้้้้้้้้้
ส็็็็็็็็็็็็็็็็็็็( ͡° ͜ʖ ͡°)
ส้้้้้้้้้้้้้้้้้้้้้้้ ส็็็็็็็็็็็็็็็็็็็็็็็็็ 
S̢͎̳̞̲͈̪̳̻ͮͩt̟̳̏ͬ̔͒̈́ͦ͠a̞̤̝̟ͫ̽̂̈́ͪ͐͘n͕͐͑ͪ͐ͦ͋ͮ̅d͚̗̙̎ͫ̌â̗̬͓͍͍̳̥͆̕͠r̢̘ͣ̀d̢̢̢̘̲̺͙̂̈́̊ͬ ͎͎̫͚̣̺̤̖͊̏̀ͬ͞u̧͆ͩ́͒҉͔̠̪̖̹̠̰͎ṇ̸̛͚̟̫͎̟̣̜͋̈́ͧͯi̲̲̺͑̐ͣ͗̿̕͘͝c̦͈͇̦͈ͦ̆ͨ͝o̟̭̫̥͎̹͆́ͥ͊ͬ̏͝d̪͔̯̥̩͙̝ͩ̏͒̈́ͩ̿́̕͜ͅe͍͓̻̊͛ͅ ̸̧̻̺̤̠͙ͪ̋̽l̛̥̥ͬ͂̈́ͤ̓̀̓̚͘ͅͅͅǒ̮͓̼ͭ̂̆̇̕͘ͅl̯̯̟̗͔̳͉̰ͫ̒ͧͦͩͦ̓̓͢ͅs̝͎͚̗̮̟̒̔͛̈̊͋͒ͩͅ Cool!



&lt;/pre&gt;

我也来生成一个：
Rͨ̍̀̐iͩͤͦ̈́́̓g̃ͬ̾u̓͆ͬ̐̎ͨ͋̆z̑ͤͯ̒ͦ͗̿̍ ͤ̇̒L͒̂͑̎ͣͣͯ̉e̊e̐̏̏̆̑͗ͥ́
了解更多,参见[https://news.ycombinator.com/item?id=3665086 这里]</text>
      <sha1>gqz5kb0f4us6z1ce0cpvx7tb0twwax4</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:PyQt5入门小程序</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-05-13T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>最近有非计算机专业的同学想学PyQT，但是又不知道怎么搞，所以我做了一个简单的例子。这个例子是一个简单的图片显示器。这是一篇写给新人的入门文章，希望有所帮助。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6562" sha1="668b0jf0j0eo8mija6wr6goxjt147uz">最近有非计算机专业的同学想学PyQT，但是又不知道怎么搞，所以我做了一个简单的例子。这个例子是一个简单的图片显示器。这是一篇写给新人的入门文章，希望有所帮助。


= Qt程序的基本结构=
跟其他程序一样，所有的程序都会有一个主入口，然后一行一行的调用代码，而GUI程序跟其他程序有一个区别就是，只有你显示的关闭界面，程序才会退出。所以整个GUI的程序大致会是这样一个原理（下面是伪代码，不是实际的程序）：

&lt;syntaxhighlight lang="java"&gt;
void main() {
    while(true) {
        command = fetch_user_input_event()
        if(command == quit):
            exit();
        else
            keep_rendering_ui
    }
}
&lt;/syntaxhighlight&gt;
然而实际上不需要我们去操心这个，Qt框架会帮我们处理这个逻辑，一个Qt应用就是这样：

&lt;syntaxhighlight lang="python"&gt;
def main():
    app = QApplication(sys.argv)
    window = MainWindow()
    window.show()         # 必须显示的调用，显示一个界面
    app.exec_()           # 等待程序退出，否则界面一闪而过

if __name__ == '__main__':
    main()
&lt;/syntaxhighlight&gt;

= UI绘制=
UI绘制十分简单，我们的程序最终的样子是这样：

[[File:PyQt_image_viewer.png|600px|Image viewer]]

一个比较典型的桌面程序，一个主界面包含菜单栏、工具栏、主界面，状态栏没有。这些都好理解，而另一个比较重要的概念是布局，就是控件在UI上怎么摆放（尤其是UI可能会缩放），通常不会是写死的位置，而是由布局管理器来管理。简而言之，就是定义规则，你的控件怎么放，窗口缩放的时候，怎么处理。

== 主窗口==
所以首先我们定义出主窗口。

&lt;syntaxhighlight lang="python"&gt;
class MainWindow(QMainWindow):
    def __init__(self, *args, **kwargs):
        super(MainWindow, self).__init__(*args, **kwargs)
        self.setWindowTitle('Simple image editor')
        self.setFixedSize(640, 480) # 固定大小的窗口，禁止缩放
&lt;/syntaxhighlight&gt;

实际上你可以直接显示一个其他控件例如QWidget, QLabel等，它们只是控件级别，而主窗口包含菜单、状态栏等，更适合制作程序UI。然后我们主界面的控件只有一个，那就是显示图片。图片可以使用QLabel显示，虽然它更多用来显示文字：

&lt;syntaxhighlight lang="python"&gt;
# 在Qt里面，每一个控件都可以指定父控件，更多的是因为C++中需要自动管理内存
# 当父控件销毁时，子控件跟着销毁，所以把self（主窗口）传给主窗口上的控件
self.imageContainer = QLabel(self)
self.imageContainer.setAlignment(Qt.AlignCenter)

# qt可以写css来设置样式
self.imageContainer.setStyleSheet('background-image:url(background.jpg);')

# 将图片设置为中心控件
self.setCentralWidget(self.imageContainer)
&lt;/syntaxhighlight&gt;
== 创建菜单和工具栏==
菜单通过`self.menuBar`得到。

&lt;syntaxhighlight lang="python"&gt;
menu = self.menuBar()
# 是否使用系统菜单，如果是mac，不设置的话菜单会在屏幕顶上
menu.setNativeMenuBar(False)
aboutMenu = menu.addMenu('&amp;关于')
aboutMenu.addAction(aboutAction) 
&lt;/syntaxhighlight&gt;

工具栏跟菜单很类似，工具栏上的按钮都是一个Action，如果希望放进去别的控件可以用`addWidget`实现

&lt;syntaxhighlight lang="python"&gt;
# 创建一个新的工具栏，可以创建多个
self.toolbar = self.addToolBar('Operations')

# 图标按钮
zoomInAction = QAction(QIcon('zengjia.svg'), '放大', self)
self.toolbar.addAction(zoomInAction)

# 非按钮控件
slider = QSlider(Qt.Horizontal)
slider.setFixedWidth(200)
self.toolbar.addWidget(slider)
&lt;/syntaxhighlight&gt;

= 事件响应=
Qt是信号（Signal）/槽（Slot）机制，简单来说就是用户对界面的更改会产生事件，而事件由槽来处理，两者之间需要关联（connect）上才能正确处理。比如菜单的处理：

&lt;syntaxhighlight lang="python"&gt;
aboutAction = QAction('&amp;版本', self)
aboutAction.setShortcut('Ctrl+A')
# triggered事件 关联到槽，槽就是一个函数
aboutAction.triggered.connect(self.onAbout)
&lt;/syntaxhighlight&gt;

有时候事件是会有参数的，比如滑块变化的时候，值是可以得到的：

&lt;syntaxhighlight lang="python"&gt;
slider = QSlider(Qt.Horizontal)
slider.setFixedWidth(200)
slider.setValue(100)
slider.valueChanged.connect(self.onChangeBrightness)

# 槽
def onChangeBrightness(self, value):
    # 这个value就是变化后的值（默认0-100）
    pass
&lt;/syntaxhighlight&gt;

= 图片处理=

== 放大缩小==

方法和缩小通过QPixmap.scaled来实现，我们通过将图片缩放到一个期望的大小（保持宽高比），来显示到界面上：

&lt;syntaxhighlight lang="python"&gt;
 scaledImage = rotatedImage.scaled(self.imageSize.width(), self.imageSize.height(), Qt.KeepAspectRatio, Qt.SmoothTransformation)
self.imageContainer.setPixmap(scaledImage)
&lt;/syntaxhighlight&gt;
而缩放的时候，实际上就是在控制这个大小：

&lt;syntaxhighlight lang="python"&gt;
# 一开始给定一个默认的大小
self.imageSize = QSize(300, 200)

def onZoomOut(self):
    # 缩小按0.5计算，扩大按照乘以1.5计算
    self.imageSize *= 0.5
    self.refreshImage()
&lt;/syntaxhighlight&gt;

== 旋转==
旋转需要记录一个旋转角度，然后通过Qtransform来实现：

&lt;syntaxhighlight lang="python"&gt;
transform = QTransform()
transform.rotate(self.rotateAngle)
rotatedImage = self.image.transformed(transform)
&lt;/syntaxhighlight&gt;
这样可以得到一个新的QPixmap，就是旋转后的图片。

== 调节亮度==
亮度调节就比较麻烦了，图片的亮度调节需要在HSL的颜色空间下处理（我们比较熟悉的一帮是RGB三色表示）。

&lt;syntaxhighlight lang="python"&gt;
# 将QPixmap转为QImage，以便可以直接操作像素
image = self.rawImage.toImage()
for i in range(0, image.width()):
    for j in range(0, image.height()):
        # 取到(i, j)位置的像素点
        color = QColor(image.pixelColor(i, j))

        # 取到HSL空间下的像素值
        (h, s, l, a) = color.getHsl()

        # 计算调整后的亮度值并更新，更新的时候转成了rgb
        newBrightless = l * (value/100.0)
        color.setHsl(h, s, newBrightless, a)
        image.setPixel(i, j, color.rgb())
self.image = QPixmap.fromImage(image)

&lt;/syntaxhighlight&gt;

以上就是一个简单的例子，完整的程序[/images/ImageEditor.zip 下载]：</text>
      <sha1>668b0jf0j0eo8mija6wr6goxjt147uz</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:关于代码注释</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2018-01-16T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>在一个“敏捷”的团队，写注释被认作是一个不好的习惯，因为他们认为，

&gt; Good programming is self-explanatory. Bad Programming requires explanation

总结一下，认为程序中不需要写注释的原因主要有如下的几点：
* 需要写注释的程序说明代码不够清晰啊，可以可以通过重构的方式，让代码变得“可读”
* 维护注释是一件工作量很大的事情，改完代码之后，时常会忘记修改注释
* 注释如果解释的不清楚，那就需要“注释的注释”...
* ……</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5065" sha1="1a5ict32urr8cvom1wdh2hx8tyqzn52">在一个“敏捷”的团队，写注释被认作是一个不好的习惯，因为他们认为，

&gt; Good programming is self-explanatory. Bad Programming requires explanation

总结一下，认为程序中不需要写注释的原因主要有如下的几点：
* 需要写注释的程序说明代码不够清晰啊，可以可以通过重构的方式，让代码变得“可读”
* 维护注释是一件工作量很大的事情，改完代码之后，时常会忘记修改注释
* 注释如果解释的不清楚，那就需要“注释的注释”...
* ……
不能不说这些没有道理，实际上也都是很关心的问题，代码写的更好更可读，当然是值得推崇的。并且诚如所言，代码应该是“自解释”的，大部分情况下，我们可能的确不需要注释。代码的可读性，和注释，目的都是一样的，让别人看得懂，不会掉坑里面。这里的坑，可能是代码逻辑的，可能是业务逻辑的，可能是某个库的bug，可能是某种奇怪的设计或者历史原因。

所以说，有另外一个更重要的他们没有考虑到的就是：

&gt;  self-explanatory code only tell how it is working. It rarely tells how it should work.

正好最近又遇到一次坑。来描述一下这个故事：
起因是我们系统需要从一个第三方系统中查询数据。这个系统调用，我们代码里面是这么写的：
&lt;syntaxhighlight lang="java"&gt;
try {
    return client.getVehicleBaseData(finOrVin);
} catch (Exception e) {
    log.error("error loading vehicle basic data from eva for finOrVin:{}", finOrVin);
    throw new EvaAccessFailureException(evaLoadService.generateFallback(e.getMessage()));
}
&lt;/syntaxhighlight&gt;
这段代码的功能是，调用外部系统的api，然后返回一个结果；如果出错则抛出异常。同时，需要根据出错的“代码”来判断是对方系统的内部错误，还是资源找不到。
&lt;syntaxhighlight lang="java"&gt;
Fallback generateFallback(String message) {
    try {
        int startPos = message.indexOf("{\"error\":");
        if (startPos == -1) {
            return new Fallback(UNEXPECTED, message);
        }
        EvaErrorResponse response = JsonUtils.unmarshal(message.substring(startPos), EvaErrorResponse.class);
        return new Fallback(getByStringValue(response.getError().getErrorCode()));
    } catch (Exception e) {
        log.error("unexpected error message from EVA {}", message);
        log.error(e.getMessage(), e);
        return new Fallback(UNEXPECTED, message);
    }
}
&lt;/syntaxhighlight&gt;
这段代码尝试从message里面解析一串error，然后再反序列化为JSON，这里是这个EvaErrorResponse的定义：
&lt;syntaxhighlight lang="java"&gt;
@Data
@Builder
@Getter
@Setter
@NoArgsConstructor
@AllArgsConstructor
public class EvaErrorResponse {
    private Error error;
}

@Data
@Builder
@Getter
@Setter
@NoArgsConstructor
@AllArgsConstructor
class Error {
    private String errorCode;
    private String errorDesc;
}
&lt;/syntaxhighlight&gt;
姑且不说一个简单的Bean用这么多Lombok注解的问题~ 然后我需要做的是，模拟这个系统的出错返回，因为我们的开发环境无法连真实的三方系统测试。那么问题来了，三方系统出错的时候，应该返回什么呢？

首先问问写这个代码的人（也就是直接对接这个系统的人）吧。他给了我一个文档，文档里面是这么描述的：
[[File:about_comments_1.png|600px|the document]]

那么问题来了，这和代码定义完全不一样啊！然后告诉我以代码为准。从这个代码根本无法确定错误返回结构。然后又看看我们这个模拟的stub的代码，关于出错的地方是这么定义的：
&lt;syntaxhighlight lang="java"&gt;
public class OabResponseDto {

    private boolean success;

    private Object result;

    private String error;

&lt;/syntaxhighlight&gt;
后来才觉察到，这是另一个系统的接口返回了。但几个系统的模拟stub都写到了一起，让人完全无法确定真实的三方接口定义。最终，我找到了调用这个接口的测试环境，自己调用了一次，原来结果是这样的：
&lt;syntaxhighlight lang="java"&gt;
{"error":{"errorCode":"WDCS0003","errorDesc":"Resource not available!"}}
&lt;/syntaxhighlight&gt;
这耗费了我半天的时间。于是为了避免有人再踩这种坑，我加了个注释在这里：
&lt;syntaxhighlight lang="java"&gt;
Fallback generateFallback(String message) {
    try {
        /**
         * example actual response from eva:
         * {"error":{"errorCode":"WDCS0003","errorDesc":"Resource not available!"}}
         */
        int startPos = message.indexOf("{\"error\":");
&lt;/syntaxhighlight&gt;
然而这又被批判了，理由是这段注释不能解释代码。因为这里message并不是这样。那message到底是什么样？他们说，你可以调试打个断点看。难道让每一个看代码的人都打个断点来看么，这是什么逻辑！我就呵呵了。最终我还是妥协了，删了呗，对我而言无任何影响。</text>
      <sha1>1a5ict32urr8cvom1wdh2hx8tyqzn52</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:新的配置文件格式</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2018-02-24T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>程序中大都需要定义各种配置，诸如数据库连接之类的，最近也需要开发Web框架，于是也想找个比较好用的配置文件格式。搞来搞去，发现都不是很喜欢。先来看一下几种常见的配置文件格式吧：</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2874" sha1="4v4e1zcq1lbd6lvjusj7q6jpxsvqdce">程序中大都需要定义各种配置，诸如数据库连接之类的，最近也需要开发Web框架，于是也想找个比较好用的配置文件格式。搞来搞去，发现都不是很喜欢。先来看一下几种常见的配置文件格式吧：
=== Properties===
Java所带来的Properties文件可能是用的比较多的格式了吧，就是一个简单的key-value的文本文件，但是缺点也很明显：
* Unicode需要转码，看着不是很蛋疼么？
* 不支持数组类型，所以以前经常会用key.1,key.2...key.n这样的方式来遍历得到一个数组
* 扁平结构，如果碰到一些比较长的key就有点不好看了（比如SpringCloud的配置，spring.jpa.datasource.xxx)写起来比较麻烦
&lt;syntaxhighlight lang="properties"&gt;
spring.data.mongodb.host= localhost
spring.data.mongodb.port=27017 # the connection port (defaults to 27107)
spring.data.mongodb.uri=mongodb://localhost/test # connection URL
spring.data.mongo.repositories.enabled=true # if spring data repository support is enabled
&lt;/syntaxhighlight&gt;
=== Yaml/TOML===
Yaml好像很流行的样子，我们在springcloud的项目中大量使用，但是说实话这个格式我也不喜欢，为啥？
* 依赖于缩进，复制粘贴的时候麻烦了
* 语法有点复杂了
TOML感觉和YAML差不多，也挺复杂的样子。
&lt;syntaxhighlight lang="yaml"&gt;
# Zuul
zuul:
  host:
    connect-timeout-millis: 50000
    socket-timeout-millis: 10000


# Hystrix
hystrix:
  command:
    default:
      execution:
        isolation:
          thread:
            timeoutInMilliseconds: 10000
&lt;/syntaxhighlight&gt;

=== Ini===
Windows所带来的格式，优点是可以带分组，好像比Properties文件更舒服一点，但是对于上面提到的缺点也有。

&lt;syntaxhighlight lang="ini"&gt;
[curentUser]      ;  this is a Section
name=wisdo     ; this is Parameters
organization=cnblogs   ; this is Parameters
 
[database] 
server=127.0.0.0   ; use IP address in case network name resolution is not working 
port=143 
file = "user.dat" 
&lt;/syntaxhighlight&gt;

=== JSON/LUA===
Json的缺点在于你要用很多个引号，同时最大的问题在于不支持注释。Lua可能是我最想用的脚本了，但是在Java中使用也比较麻烦，尤其是我想手写一个配置文件解析器，这样就麻烦了（主要是不会）。

还有Ini + Json的方法，但是感觉也比较丑，于是想来想去，还不如按照自己的意愿发明一种配置文件格式好了，主要有以下的考虑：
* 语法应该简单，不需要依赖缩进
* 支持数组
* 支持使用变量（类似shell）
* 支持Unicode，中文直接写，所见即所得
* 支持某种形式的命名空间（类似ini中的section）来对配置进行分组
* 支持注释
* 支持多行字符串
* 格式好看...

目前正在计划中，准备利用Antlr实现解析。</text>
      <sha1>4v4e1zcq1lbd6lvjusj7q6jpxsvqdce</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:SQLite3 文件格式分析</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-09-25T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>最近一直在思考如何使用btree文件结构做一个单文件的加密存储格式，因此研究了一下sqlite3的文件格式以为参考。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13547" sha1="hgf42lm25o1foooy92hx3x7elhe6fuj">最近一直在思考如何使用btree文件结构做一个单文件的加密存储格式，因此研究了一下sqlite3的文件格式以为参考。


= 文件布局=
每一个sqlite的数据库文件由1个或者多个大小相同的页（page）构成，其文件布局如下：

[[File:Sqlite3-file-layout.png|600px|Sqlite3 file layout]]

其中，第一个page记为page 1（而不是从0开始算）。任一个page都属于以下的某一种：

* lock-byte page，设计给VFS使用，本身sqlite并不需要，任何大小在1073741824(1024m=1G)以内的数据库文件都不包含lock-byte page。
* freelist page，数据库的空闲page（譬如删除一些数据之后，页仍然保留）
    - freelist trunk page，存储一个4位的leaf page数组，因最小的page可用空间为480所以至少可以存储120个entry。其中第一个id为下一个freelist trunk page的id，如果没有则为空
    - freelist leaf page，不包含任何信息
* b-tree page
    - table b-tree interior page
    - table b-tree leaf page
    - index b-tree interior page
    - index b-tree leaf page 
* payload overflow page
* pointer map page   

= 页（page）=

== 页大小（page_size）==
页的大小必须为512~65536间的2的整数幂。从[https://www.sqlite.org/releaselog/3_12_0.html 3.12.0]开始，默认的page大小从1024调整到了4096。可以通过如下的命令来查看当前数据库的页大小：

&lt;syntaxhighlight lang="bash"&gt;
sqlite&gt; pragma page_size;
4096
sqlite&gt; pragma page_count;
3
&lt;/syntaxhighlight&gt;

可以通过命令来设置page_size，不过必须在创建库之前操作，否则不能生效。

&lt;syntaxhighlight lang="bash"&gt;
hfli@192:btree/sqlite $ sqlite3 p512.db
SQLite version 3.28.0 2019-04-16 19:49:53
Enter ".help" for usage hints.
sqlite&gt; pragma page_size;
4096
sqlite&gt; pragma main.page_size=512;
sqlite&gt; pragma page_size;
512
&lt;/syntaxhighlight&gt;

然后我们创建一个简单的表，来一探数据库文件的究竟：

&lt;syntaxhighlight lang="sql"&gt;
create table person(
    id integer not null primary key,
    name text,
    age number,
    remark text
);
&lt;/syntaxhighlight&gt;

创建完成后，不插入数据，则文件中共有三页，

&lt;syntaxhighlight lang="lua"&gt;
53514C69746520666F726D617420330002000101004020200000000100000003
...
0100000000000000000000000000000000000000000000000000000000000000 
...
0D00000000020000000000000000000000000000000000000000000000000000
...
&lt;/syntaxhighlight&gt;

== 文件头==

数据库文件的第一个页为一个特殊的页，其中包含的是数据库的文件头。上述的数据库文件头为：

&lt;syntaxhighlight lang="lua"&gt;
53514C69 74652066 6F726D61 74203300 02000101 00402020 00000001 00000003
00000000 00000000 00000001 00000004 00000000 00000003 00000001 00000000
00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001
002E3420 0D000000 01017A00 017A0000 00000000 00000000 00000000 00000000
00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
00000000 00000000 00000000 00000000 00000000 00000000 00008103 01071719
19018161 7461626C 65706572 736F6E70 6572736F 6E034352 45415445 20544142
4C452070 6572736F 6E280A20 20202069 6420696E 74656765 72206E6F 74206E75
6C6C2070 72696D61 7279206B 65792C0A 20202020 6E616D65 20746578 742C0A20
20202061 6765206E 756D6265 722C0A20 20202072 656D6172 6B207465 78740A29
&lt;/syntaxhighlight&gt;

其详细格式如下：

range  样例值      description
------ -------     ------------------------------------------
00..15 同→         "SQLite format 3\000" 
16..17 0x0200      page size（in bytes)，如果是1则表示为65536
18     0x01        file format write version,1=legacy,2=WAL，用于向后兼容。若新版本可被旧版本安全读取但不可写入，则可设置为比就版本的version更高。
19     0x01        file format read version, 1=legacy,2=WAL，用于向后兼容，若大于2则不允许读取或者写入。
20     0x00        每页的保留字节数（在页的末尾），通常为0，可用于存储一些额外的信息，例如让SQLite Encryption Extension存储checksum等。
21     0x40        Maximum embedded payload fraction. Must be 64. 暂时不支持更改
22     0x20        Minimum embedded payload fraction. Must be 32. 
23     0x20        Leaf payload fraction. Must be 32. 
24..27 0x00        文件修改次数
28..31 0x03        数据库页的数目
32..35 0x00        第一个freelist trunk page的page number
36..39 0x00        总共的freelist 数目
40..43 0x01        schema cookie，每当schema变化的时候这个值就增长（prepared statement对应到一个schema version，如果schema变化也必须重新prepare)。
44..47 0x04        schema format number(允许的格式为1,2,3或者4)，类似read/write version,用于向后兼容，以支持新的schema语法，当前最新为4（SQLite 3.3.0 on 2006-01-10）
48..51 0x00        默认的page缓存大小
52..55 0x03        The page number of the largest root b-tree page when in auto-vacuum or incremental-vacuum modes, or zero otherwise.
56..59 0x01        数据库文件编码，1=UTF-8, 2=UTF-16le, 3=UTF-16be
60..63 0x00        user version
64..67 0x00        True (non-zero) for incremental-vacuum mode. False (zero) otherwise. 
68..71 0x00        application id，为使用sqlite作为应用格式而设计
72..91 0x00        保留字段，填充为0
92..95 0x01        The version-valid-for number
96..99 0x2e3420    SQLITE_VERSION_NUMBER

== B-tree page==

=== table b-tree 和 index b-tree===
sqlite中通过page来持久化b-tree的节点，每一个b-tree的节点就对应到一个page。其中，又分为两种具体的用途：

* table b-tree: 使用64位有符号整数作为key（也即是rowid)，数据保存在叶子节点中（内部节点中只包含key和指向子节点的指针），因此来看这是属于b+-tree结构
* index b-tree: 只存储key而没有数据

对于一个b-tree的内部节点，存储有k个key和k+1个指向子节点的指针，在sqlite中即节点的page number。一个key以及其左边子节点的page number组合被称之为一个单元格（cell），而最右侧的指针没有对应的key，是单独储存的。每个数据库都有两个特殊的b-tree:

* 一个table b-tree用来存储所有的schema，包含系统的表sqlite_schema，这个b-tree的root page即第一个page，并存储了其他表和index的root page的序号
* 一个index b-tree用来存储schema中的index

除此之外，普通用户创建的表则对应到一个table b-tree（有一个例外就是如果建表没有指定primary key,则会使用index b-tree而不是table b-tree)。

=== b-tree page的文件布局===

b-tree page在文件中的格式如下：

* 如果是第一个page，则有100字节的文件头
* page header，占8个或者12个字节
* cell pointer 数组，假设page有K个cell，则存储K个2字节的，到cell content位置的偏移，按key升序排列
* 空闲空间
* cell content 区域
* 保留区域

其中，文件头的格式如下：

range   description
------- ----------------------
0       page type： 0x02=index b-tree 内部页 0x05=table b-tree内部页，0x0a=index b-tree叶子页，0x0d=table b-tree叶子页
1..2    第一个freeblock的offset
3..4    cell的个数
5..6    cell content区域起始偏移
7       cell content区域中的碎片大小
8..11   当前节点最右侧的子节点的page number，只存在于内部节点中

page中的空闲区域用freeblock链表来标记，每个freeblock的结构如下：

* 第一个freeblock的偏移存储在文件头中，下一个freeblock的offset存储在freeblock的前两个字节中，若已经是最后一个freeblock则为0。
* 接下来两个字节为freeblock的size（包含上述4个字节的头）

由此可见freeblock至少需要4个字节，如果空闲区域长度小于4，则被称之为一个碎片（fragment），这些碎片的总计大小存储在page的文件头中。在一个格式良好的page中，碎片的总大小不应该超过60字节。而sqlite也会通过重新组织文件来去掉碎片和freeblock，这称之为碎片整理（defragment）。

=== 变长整数variable-length integer===

为节省空间，sqlite中通过varint来存储霍夫曼编码的补码64位整数，占1-9个字节。设其从低到高分别为$A_0$, $A_1$, .., $A_8$, 则其解码如下：

* 若 $0\leq A_0\leq 240$，则$N=A_0$
* 若 $241\leq A_0\leq 248$，则$N=240 + 256 \times (A_0 - 241) + A_1$
* 若 $A_0= 249$，则$N=2288 + 256 \times A_1 + A2$
* 若 $A_0= 250$，则$N=big-ending(A_1--A_3)$
* 若 $A_0= 251$，则$N=big-ending(A_1--A_4)$
* 若 $A_0= 252$，则$N=big-ending(A_1--A_5)$
* 若 $A_0= 253$，则$N=big-ending(A_1--A_6)$
* 若 $A_0= 254$，则$N=big-ending(A_1--A_7)$
* 若 $A_0= 255$，则$N=big-ending(A_1--A_8)$

=== 单元格（cell）的格式===

根据page类型的不同，cell的格式也不相同：

table b-tree leaf cell：

* varint：总计的payload大小（包含overflow）
* varint：rowid
* payload（未包含overflow中的字节）
* 4字节的page number指向第一个overflow page，如果没有溢出则不计

table b-tree interior cell:

* 4字节的page number，指向左边的子节点
* varint: rowid

index b-tree leaf cell:

* varint：总计的payload大小（包含overflow）
* payload（未包含overflow中的字节）
* 4字节的page number指向第一个overflow page，如果没有溢出则不计

index b-tree interior cell:

* 4字节的page number，指向左边的子节点
* varint：总计的payload大小（包含overflow）
* payload（未包含overflow中的字节）
* 4字节的page number指向第一个overflow page，如果没有溢出则不计

=== overflow page===

对于table b-tree的叶子节点，其payload如果超过一个阈值，无法完整存储到单个page中，则会使用overflow page链表来存储余下的部分。

&lt;!-- todo --&gt;
设 $U$为没页的可用大小， $P$为payload的大小，$X$为页中最大直接存储的payload大小, $M$ 为最小必须存到page中的payload大小，则：

= 记录格式=

table b-tree中的payload（或者index b-tree中的key）都是存储为记录格式(record format)。每一个record包含文件头和body，依如下格式：

* varint: 文件头的长度，包含自身
* serial type数组，一个或者多个varint，记录每一列的数据类型

其中，serial type如下：

type   description
------ ------------------
0      NULL
1      8位整数（补码）
2      16位big-endian整数（补码）
3      24位big-endian整数（补码）
4      32位big-endian整数（补码）
5      48位big-endian整数（补码）
6      64位big-endian整数（补码）
7      IEE754 64位big-endian浮点数
8      0（schema format 4)
9      1（schema format 4)
10,11  保留
&gt;11    如果偶数则为BLOB，长度为$(N-12) \div 2$；如果为奇数则为字符串，长度为$(N-13) \div 2$（结尾符不存储）

在某些情况下，值的个数可能少于column，例如通过alter table来增加列，sqlite并未修改已有数据。这种情况下新增列的值为默认值。

= 样例分析=

== 内置表sqlite_schema==

系统的第一页是内置的表，这个表类似这样：

&lt;syntaxhighlight lang="sql"&gt;
CREATE TABLE sqlite_schema(
  type text,
  name text,
  tbl_name text,
  rootpage integer,
  sql text
);
&lt;/syntaxhighlight&gt;

创建一个新表并插入数据：

&lt;syntaxhighlight lang="sql"&gt;
create table person(
    id integer not null primary key,
    name text,
    age number,
    remark text
);
insert into person values(1, 'riguz', 20, 'a programmer');
&lt;/syntaxhighlight&gt;

[[File:Sqlite3-rowformat-e1.png|600px|Example]]

一直向表中插入数据，

&lt;syntaxhighlight lang="sql"&gt;
insert into person values(1, 'riguz1', 20, 'a programmer');
insert into person values(2, 'riguz2', 20, 'a programmer');
insert into person values(3, 'riguz3', 20, 'a programmer');
insert into person values(4, 'riguz4', 20, 'a programmer');
insert into person values(5, 'riguz5', 20, 'a programmer');
insert into person values(6, 'riguz6', 20, 'a programmer');
insert into person values(7, 'riguz7', 20, 'a programmer');
insert into person values(8, 'riguz8', 20, 'a programmer');
insert into person values(9, 'riguz9', 20, 'a programmer');
insert into person values(10, 'riguz10', 20, 'a programmer');
insert into person values(11, 'riguz11', 20, 'a programmer');
insert into person values(12, 'riguz12', 20, 'a programmer');
insert into person values(13, 'riguz13', 20, 'a programmer');
insert into person values(14, 'riguz14', 20, 'a programmer');
insert into person values(15, 'riguz15', 20, 'a programmer');
insert into person values(16, 'riguz16', 20, 'a programmer');
insert into person values(17, 'riguz17', 20, 'a programmer');
insert into person values(18, 'riguz18', 20, 'a programmer');

&lt;/syntaxhighlight&gt;
当插入到第18条数据的时候，split了节点，如图：

[[File:Sqlite-split-18.png|600px|Sqlite split]]

[https://www.sqlite.org/fileformat.html Database File Format ]
[http://barbra-coco.dyndns.org/sqlite/fileformat.html ]</text>
      <sha1>hgf42lm25o1foooy92hx3x7elhe6fuj</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:A brief introduction to MySQL binary log</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-06-15T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>Accroding to the mysql manual, the binary log( also known as binlog) is a very powerful feature that enables the recording of "events" that describe database changes. Those changes could be table modification or data change, and may also contain some statements which may potentially made changes such as `DELETE` which no matched items.

This kind of feature enables mysql in replication from master to slave servers by sending events contained in binlogs, and also data recovery. Except for that, it could also be used in CDC(change-data-capture) since it's event based, an example usage could be found in the cdc-component in [Eventuate™](https://eventuate.io/).</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6241" sha1="ahzvdydnk4kgcxyo9z3awjgh0b73284">Accroding to the mysql manual, the binary log( also known as binlog) is a very powerful feature that enables the recording of "events" that describe database changes. Those changes could be table modification or data change, and may also contain some statements which may potentially made changes such as `DELETE` which no matched items.

This kind of feature enables mysql in replication from master to slave servers by sending events contained in binlogs, and also data recovery. Except for that, it could also be used in CDC(change-data-capture) since it's event based, an example usage could be found in the cdc-component in [https://eventuate.io/ Eventuate™].


= binary log configuration=

== enable mysql binlog==

By default the binlog feature is not enabled by mysql, we can enable this by adding the following settings to `my.cnf` file:

&lt;syntaxhighlight lang="ini"&gt;
[mysqld]
log-bin=mysql-bin
server_id=1
&lt;/syntaxhighlight&gt;

Since MySQL5.7, the `server_id` must be specified if binlog is enabled, and this value should be unique among the cluster. Then we can find binlog like the following:

&lt;pre&gt;
mysql&gt; show binary logs;
+------------------+-----------+
| Log_name         | File_size |
+------------------+-----------+
| mysql-bin.000001 |       154 |
+------------------+-----------+
1 row in set (0.00 sec)
&lt;/pre&gt;

== binlog format==

There are 3 kinds of binlog format supported by mysql:

* STATEMENT: statement-based logging
* ROW: row-based logging
* MIXED: in mixed mode

By default, the statement-based binlog format is used, and that could be changed by settings:

&lt;syntaxhighlight lang="ini"&gt;
[mysqld]
log-bin=mysql-bin
server_id=1
binlog_format="ROW"
&lt;/syntaxhighlight&gt;

But be aware that the statement-based binlog may cause inconsistency in replication, accroding to the warning in mysql manual:

&gt; When using statement-based logging for replication, it is possible for the data on the master and slave to become different if a statement is designed in such a way that the data modification is nondeterministic; that is, it is left to the will of the query optimizer. In general, this is not a good practice even outside of replication. 

So in general maybe it's better to use row-based logging, however that could result in larger binlog files.

= events in binlog=

== View events in binlog==

We can show the events contained in binlog use `show binlog events` command:

&lt;pre&gt;
mysql&gt; show binlog events in 'mysql-bin.000001';
+------------------+-----+----------------+-----------+-------------+---------------------------------------+
| Log_name         | Pos | Event_type     | Server_id | End_log_pos | Info                                  |
+------------------+-----+----------------+-----------+-------------+---------------------------------------+
| mysql-bin.000001 |   4 | Format_desc    |         1 |         123 | Server ver: 5.7.30-log, Binlog ver: 4 |
| mysql-bin.000001 | 123 | Previous_gtids |         1 |         154 |                                       |
+------------------+-----+----------------+-----------+-------------+---------------------------------------+
2 rows in set (0.00 sec)
&lt;/pre&gt;

So let's try to find what happens when we do some operation in database. We'll create a new database and table, then insert a single record to the table by using the following commands:

&lt;syntaxhighlight lang="sql"&gt;
create database test;
use test;
create table foo(
    id int not null primary key,
    remark varchar(100)
);
insert into foo(id, remark) values(1, 'hello world!');
&lt;/syntaxhighlight&gt;

After this we'll find the following events has been appended to the binlog:

&lt;pre&gt;
| mysql-bin.000001 | 219 | Query          |         1 |         313 | create database test                                                             |
| mysql-bin.000001 | 313 | Anonymous_Gtid |         1 |         378 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                             |
| mysql-bin.000001 | 378 | Query          |         1 |         520 | use `test`; create table foo(
id int not null primary key,
remark varchar(100)
) |
| mysql-bin.000001 | 520 | Anonymous_Gtid |         1 |         585 | SET @@SESSION.GTID_NEXT= 'ANONYMOUS'                                             |
| mysql-bin.000001 | 585 | Query          |         1 |         657 | BEGIN                                                                            |
| mysql-bin.000001 | 657 | Table_map      |         1 |         706 | table_id: 116 (test.foo)                                                         |
| mysql-bin.000001 | 706 | Write_rows     |         1 |         760 | table_id: 116 flags: STMT_END_F                                                  |
| mysql-bin.000001 | 760 | Xid            |         1 |         791 | COMMIT /* xid=26 */                                                              |
+------------------+-----+----------------+-----------+-------------+----------------------------------------------------------------------------------+

&lt;/pre&gt;

== event types==

As we already see, there are different kinds of events shown in binlog such as `Query` and `Table_map`. Actually there are many more types than that, here are parts of them:

Event type         Description
------------------ -------------------------------------------------
UNKNOWN_EVENT      never occurs
START_EVENT_V3     a descriptor at each begging of binlog, and is replaced by  FORMAT_DESCRIPTION_EVENT since MySQL 5.0
QUERY_EVENT        occurs when an updating statement is done
STOP_EVENT         occurs when mysqld stops
INTVAR_EVENT       occurs each time when a AUTO_INCREMENT field is used
TABLE_MAP_EVENT    map the table defination to a number, happens before row operations 
WRITE_ROWS_EVENT   insert records
UPDATE_ROWS_EVENT  update rows
DELETE_ROWS_EVENT  delete rows

For more detailed explaination about event types, please refer to [https://dev.mysql.com/doc/internals/en/event-meanings.html Event Meanings].

References:

- [https://dev.mysql.com/doc/refman/5.7/en/replication-options.html MySQL 5.7 Reference Manual - Replication and Binary Logging Options and Variables]
- [https://dev.mysql.com/doc/refman/5.7/en/binary-log.html MySQL 5.7 Reference Manual - The Binary Log]</text>
      <sha1>ahzvdydnk4kgcxyo9z3awjgh0b73284</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:MySQL replication</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2019-01-21T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>MySQL设置Replication后，可以支持Master库上的修改自动同步到Slave库上。利用Docker可以在本机尝试这种特性。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4987" sha1="no47if7wivve269jnopjzuvq0uubwow">MySQL设置Replication后，可以支持Master库上的修改自动同步到Slave库上。利用Docker可以在本机尝试这种特性。

= 配置Master=
首先需要创建几个文件夹（略），用来挂载配置文件和数据。我们首先来配置Master库：

&lt;syntaxhighlight lang="ini"&gt;
# master/cnf/my.cnf
[mysqld]

server-id=1
log-bin=/var/lib/mysql/mysql-bin.log
binlog_format=MIXED
expire_logs_days=7
max_binlog_size=50m
max_binlog_cache_size=256m
&lt;/syntaxhighlight&gt;

启动Master：

&lt;syntaxhighlight lang="bash"&gt;
docker run --name mysql_master \
    --mount type=bind,src=/Users/hfli/mysql-replication/master/cnf/my.cnf,dst=/etc/my.cnf \
    --mount type=bind,src=/Users/hfli/mysql-replication/master/data/,dst=/var/lib/mysql \
    -e MYSQL_ROOT_PASSWORD=1125482715 \
    -d mysql:5.7.24
&lt;/syntaxhighlight&gt;

然后需要登录到MySQL创建一个用来复制的用户

&lt;syntaxhighlight lang="mysql"&gt;
create user 'replication' identified by '1153687060';
grant replication slave on *.* to 'replication'@'%' identified by '1153687060';
&lt;/syntaxhighlight&gt;

接下来需要看一下Master库的状态:

&lt;pre&gt;
mysql&gt; show master status;
+------------------+----------+--------------+------------------+-------------------+
| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |
+------------------+----------+--------------+------------------+-------------------+
| mysql-bin.000003 |      696 |              |                  |                   |
+------------------+----------+--------------+------------------+-------------------+
1 row in set (0.00 sec)
&lt;/pre&gt;

= 从库配置=

&lt;syntaxhighlight lang="ini"&gt;
# slave1/cnf/my.cnf
[mysqld]

server-id=2
&lt;/syntaxhighlight&gt;
启动docker：

&lt;syntaxhighlight lang="bash"&gt;
docker run --name mysql_slave1 \
    --mount type=bind,src=/Users/hfli/mysql-replication/slave1/cnf/my.cnf,dst=/etc/my.cnf \
    --mount type=bind,src=/Users/hfli/mysql-replication/slave1/data/,dst=/var/lib/mysql \
    --link mysql_master \
    -e MYSQL_ROOT_PASSWORD=1125482715 \
    -d mysql:5.7.24
&lt;/syntaxhighlight&gt;

然后即可启动Replication:

&lt;pre&gt;
mysql&gt; change master to master_host='mysql_master',master_user='replication',master_password='1153687060',master_log_file='mysql-bin.000003',master_log_pos=696;
Query OK, 0 rows affected, 2 warnings (0.03 sec)

mysql&gt; start slave;
Query OK, 0 rows affected (0.00 sec)

mysql&gt; show slave status\G;
*************************** 1. row ***************************
               Slave_IO_State: Waiting for master to send event
                  Master_Host: mysql_master
                  Master_User: replication
                  Master_Port: 3306
                Connect_Retry: 60
              Master_Log_File: mysql-bin.000003
          Read_Master_Log_Pos: 696
               Relay_Log_File: c17b953fb671-relay-bin.000002
                Relay_Log_Pos: 320
        Relay_Master_Log_File: mysql-bin.000003
             Slave_IO_Running: Yes
            Slave_SQL_Running: Yes
              Replicate_Do_DB:
          Replicate_Ignore_DB:
           Replicate_Do_Table:
       Replicate_Ignore_Table:
      Replicate_Wild_Do_Table:
  Replicate_Wild_Ignore_Table:
                   Last_Errno: 0
                   Last_Error:
                 Skip_Counter: 0
          Exec_Master_Log_Pos: 696
              Relay_Log_Space: 534
              Until_Condition: None
               Until_Log_File:
                Until_Log_Pos: 0
           Master_SSL_Allowed: No
           Master_SSL_CA_File:
           Master_SSL_CA_Path:
              Master_SSL_Cert:
            Master_SSL_Cipher:
               Master_SSL_Key:
        Seconds_Behind_Master: 0
Master_SSL_Verify_Server_Cert: No
                Last_IO_Errno: 0
                Last_IO_Error:
               Last_SQL_Errno: 0
               Last_SQL_Error:
  Replicate_Ignore_Server_Ids:
             Master_Server_Id: 1
                  Master_UUID: 83a0a667-1d50-11e9-b754-0242ac110002
             Master_Info_File: /var/lib/mysql/master.info
                    SQL_Delay: 0
          SQL_Remaining_Delay: NULL
      Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates
           Master_Retry_Count: 86400
                  Master_Bind:
      Last_IO_Error_Timestamp:
     Last_SQL_Error_Timestamp:
               Master_SSL_Crl:
           Master_SSL_Crlpath:
           Retrieved_Gtid_Set:
            Executed_Gtid_Set:
                Auto_Position: 0
         Replicate_Rewrite_DB:
                 Channel_Name:
           Master_TLS_Version:
1 row in set (0.00 sec)
&lt;/pre&gt;

一个有趣的问题：如果我修改了从库，会产生什么影响？例如已经有重复的数据，那么同步的时候就会报错，我们通过Last_Error可以看到错误。

&lt;pre&gt;
 Last_Errno: 1062
 Last_Error: Error 'Duplicate entry '2' for key 'PRIMARY'' on query. Default database: 'foo'. Query: 'INSERT INTO `foo`.`bar` (`id`, `remark`) VALUES ('2', 'existing')'
&lt;/pre&gt;</text>
      <sha1>no47if7wivve269jnopjzuvq0uubwow</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:使用Vagrant来管理Virtualbox</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2019-12-13T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>一直以来我用Virtualbox都是手动创建虚拟机，然后安装操作系统，虽然这个过程本身并不复杂但是也要重复操作和花费时间。通过Vagrant可以像使用Docker一样，编写脚本来管理虚拟机的配置，还可以通过公共的镜像仓库来获取一些别人已经构建好了的镜像。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1435" sha1="1zg15ui9bvndoxudhteftwfqp8zk856">一直以来我用Virtualbox都是手动创建虚拟机，然后安装操作系统，虽然这个过程本身并不复杂但是也要重复操作和花费时间。通过Vagrant可以像使用Docker一样，编写脚本来管理虚拟机的配置，还可以通过公共的镜像仓库来获取一些别人已经构建好了的镜像。


= 创建新的虚拟机=
通过Vagrant有两种方法来创建新的虚拟机：

* 使用vagrant命令生成一个Vagrantfile
* 手动编写Vagrantfile

例如，创建一个ubuntu的镜像，使用[https://app.vagrantup.com/ubuntu/boxes/trusty64 ubuntu/trusty64]这个镜像，可以首先通过如下的命令在当前文件夹下生成一个Vagrantfile：

&lt;syntaxhighlight lang="bash"&gt;
vagrant init ubuntu/trusty64
&lt;/syntaxhighlight&gt;
生成的Vagrantfile如下：

&lt;syntaxhighlight lang="bash"&gt;
Vagrant.configure("2") do |config|
  config.vm.box = "ubuntu/trusty64"
end
&lt;/syntaxhighlight&gt;

Vagrantfile就相当于Dockerfile，可以定义虚拟机的一些配置，除此之外还可以定义一些其他的参数。然后，要启动它可以这样：

&lt;syntaxhighlight lang="bash"&gt;
vagrant up
&lt;/syntaxhighlight&gt;
值得注意的是，截止目前，最新的Virtualbox6.1是不被Vagrant支持的，只能使用6.0.x版本。创建完成之后就可以在Virtualbox的控制页面看到这个虚拟机了。

[[File:Virtualbox.png|600px|Virtualbox]]

= 解决下载很慢的问题=
有时候</text>
      <sha1>1zg15ui9bvndoxudhteftwfqp8zk856</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:搭建GOCD Server</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2017-06-20T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>来部署一个[GOCD](https://hub.docker.com/r/gocd/gocd-server/)的容器。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2665" sha1="owiifs3jzoh73tzhd3br9myc1gxqimc">
来部署一个[https://hub.docker.com/r/gocd/gocd-server/ GOCD]的容器。

&lt;syntaxhighlight lang="bash"&gt;
sudo docker pull gocd/gocd-server:v17.5.0
docker run -d \
    --name gocd \
    -p 8153:8153 \
    -p 8154:8154 \
    -v /home/docker/go/data:/godata \
    -v /home/docker/go/home:/home/go \
    gocd/gocd-server:v17.5.0
&lt;/syntaxhighlight&gt;
启动起来后，访问8153端口，这时可以看到添加pipeline的界面了。

安装[https://github.com/gocd-contrib/script-executor-task/releases Script Executor]插件：
&lt;syntaxhighlight lang="bash"&gt;
cd /home/docker/go/data/plugins/external
wget https://github.com/gocd-contrib/script-executor-task/releases/download/0.3/script-executor-0.3.0.jar
chown 1000 script-executor-0.3.0.jar
sudo docker restart gocd
&lt;/syntaxhighlight&gt;
安装go-agent到Ubuntu宿主机上，参考[https://docs.gocd.org/current/installation/install/agent/linux.html GOCD文档]
&lt;syntaxhighlight lang="bash"&gt;
echo "deb https://download.gocd.io /" | sudo tee /etc/apt/sources.list.d/gocd.list
curl https://download.gocd.io/GOCD-GPG-KEY.asc | sudo apt-key add -
sudo apt-get update
sudo apt-get install go-agent
&lt;/syntaxhighlight&gt;
注意，go-agent只能运行在jdk8上，如果装了jdk9是运行不起来的
&lt;syntaxhighlight lang="bash"&gt;
dpkg -l | grep jdk
sudo apt-get autoremove openjdk-9-jre-headless
sudo apt-get install openjdk-8-jre-headless
sudo apt-get install go-agent
&lt;/syntaxhighlight&gt;
记得修改环境变量/etc/profile：
&lt;syntaxhighlight lang="bash"&gt;
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export CLASSPATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$CLASSPATH
export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH
&lt;/syntaxhighlight&gt;
配置gocd的工作目录：
&lt;syntaxhighlight lang="bash"&gt;
mkdir /home/gocd
chown -R go.go /home/gocd
chown go.go /usr/share/go-agent/*.jar

vim /etc/default/go-agent

GO_SERVER_URL=https://192.168.56.101:8154/go
AGENT_WORK_DIR=/home/gocd/${SERVICE_NAME:-go-agent}
DAEMON=Y
VNC=
&lt;/syntaxhighlight&gt;
下面我们就来建一个pipe line试试吧。

* 在Agents中启用我们的go-agent，并在Resource中添加一个LINUX的标签
* 新建一个Environment，把我们的这个agent加入进去
* 新建一个Pipeline，名称为hello，group为dev
* 选择Material的地方选择Git，填写http://root:****@192.168.56.101/springcloud/helloworld.git，其中root:****为GIT的账号密码，如果是public的库则无需这样设置，可以check connection查看是否可以访问
* 新建一个Stage，名称为build，然后Initial Job中设置为Script Executor，可以随便执行个bash命令，例如`echo "Hello World!"`
这样运行就可以看到pipeline绿了~~~
</text>
      <sha1>owiifs3jzoh73tzhd3br9myc1gxqimc</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:搭建Gitlab私服</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2017-06-18T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>我们利用Gitlab搭建一个内网的git私服，可以为团队提供git服务。首先是需要pull下来Gitlab的镜像了。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3012" sha1="6p156d8nb7wf8h76yn1jxlh2xgusklm">
我们利用Gitlab搭建一个内网的git私服，可以为团队提供git服务。首先是需要pull下来Gitlab的镜像了。
&lt;syntaxhighlight lang="bash"&gt;
sodu docker pull gitlab/gitlab-ce
&lt;/syntaxhighlight&gt;
这个镜像大概有300多M，如果下载太慢，请参照上一篇文章说的加速的配置。下载完成后，就需要来跑起来了，参照[https://docs.gitlab.com/omnibus/docker/ 官方文档]。
&lt;syntaxhighlight lang="bash"&gt;
sudo docker run --detach \
    --hostname 192.168.56.101
    --publish 1443:443 --publish 1080:80 --publish 1022:22 \
    --name gitlab \
    --restart always \
    --volume /home/docker/gitlab/etc:/etc/gitlab \
    --volume /home/docker/gitlab/logs:/var/log/gitlab \
    --volume /home/docker/gitlab/data:/var/opt/gitlab \
    gitlab/gitlab-ce:latest
&lt;/syntaxhighlight&gt;

注意到这里我们填的IP地址是192.168.56.101，这是宿主机的地址（宿主机现在我改成NAT和Host两个网卡了，因为桥接网卡在酒店IP不稳定....)，另外把端口映射出来了。这样在我的Mac上也可以通过192.168.56.101:1080来访问。

启动gitlab后就可以通过 http://192.168.56.101:1080 来访问了，默认的用户名是root，第一次进入会设置root密码。

*备注*
经过一番折腾，如果不使用默认端口（80，443）等配置的时候有些问题没有解决，于是为了简单起见，最终使用80端口。
---

*以下是可选操作，如果生成独立IP在Mac上访问虚拟机内的Docker也会存在麻烦，仅供参考*

我们把几个数据目录挂在到Ubuntu上，这样即便删除Docker后，数据也还存在。现在有一个很重要的问题了，按照上面的方式是把容器的80、22、443端口映射到了宿主机的端口上，如果能给容器一个独立的IP岂不是更好？根据网上的资料来看，目前有几种办法：

* Pipework
* Weave
* Flannel

就选[https://github.com/jpetazzo/pipework Pipework]吧，感觉会比较简单。
&lt;syntaxhighlight lang="bash"&gt;
git clone https://github.com/jpetazzo/pipework.git
sudo cp pipework/pipework /usr/local/bin/
sudo chmod +x /usr/local/bin/pipework
sudo apt install bridge-utils
&lt;/syntaxhighlight&gt;
配置宿主机为静态IP：
&lt;pre&gt;
# /etc/network/interfaces
auto enp0s3
iface enp0s3 inet static
        address 192.168.11.242
        netmask 255.255.248.0
        gateway 192.168.11.1

dns-nameservers 114.114.114.114
&lt;/pre&gt;
Ubuntu16.04貌似有BUG，通过重启networking服务不能改变IP地址，非要重启一下。
我们来看一下网络桥接的情况：
&lt;syntaxhighlight lang="bash"&gt;
riguz@docker-host:~$ brctl show
bridge name	bridge id		STP enabled	interfaces
docker0		8000.02425039a299	no
&lt;/syntaxhighlight&gt;
这个docker0就是docker自动生成的桥接网卡.我们来创建一个桥接网卡：
&lt;syntaxhighlight lang="bash"&gt;
sudo brctl addbr br0
sudo ip link set dev br0 up 
sudo ip addr add 192.168.10.1/24 dev br0
sudo pipework br0 gitlab 192.168.10.100/24@192.168.10.1
&lt;/syntaxhighlight&gt;</text>
      <sha1>6p156d8nb7wf8h76yn1jxlh2xgusklm</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:搭建Openshift本地环境</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-03-09T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>OpenShift是红帽基于Docker和Kubernetes的云开发平台即服务（PaaS）。而[OKD(The Origin Community Distribution of Kubernetes )](https://www.okd.io/)即Openshift的开源版本。在本机上搭建一套完整的Openshift环境较为麻烦，有以下几种方式：

* Running in a Container
* Run the All-In-One VM with Minishift
* 使用Virtualbox构建Openshift集群</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10915" sha1="bc5hy4zssscl041comglu05ppeivcy2">OpenShift是红帽基于Docker和Kubernetes的云开发平台即服务（PaaS）。而[https://www.okd.io/ OKD(The Origin Community Distribution of Kubernetes )]即Openshift的开源版本。在本机上搭建一套完整的Openshift环境较为麻烦，有以下几种方式：

* Running in a Container
* Run the All-In-One VM with Minishift
* 使用Virtualbox构建Openshift集群


= 使用VirtualBox构建Openshift集群=

按照[https://docs.openshift.com/container-platform/3.11/install/index.html 安装文档]应该可以在本地搭建一个集群，但是纯手动安装的话比较复杂，幸好有[https://github.com/eliu/openshift-vagrant Openshift Vagrant]这个项目可以帮助我们简单的构建出一个集群环境。

== 集群规划==

下面是计划搭建的最简单的单master、多node的一个集群配置：

Node                IP         Role               Instance
------------------- ---------- ------------------ ----------------
master.example.com  .100       node, master, etcd 4GMem, 2Core, 40GDisk
node1.example.com   .101       node               2GMem, 1Core, 40GDisk
node2.example.com   .102       node               2GMem, 1Core, 40GDisk

整个安装步骤可以分为这几步：

* 创建好master、node三个虚拟机
* 通过hosts文件设置好域名解析
* 在master、node上都安装docker依赖
* 配置在master上可以通过ssh访问到node01、node02
* 在master上安装ansible
* 在master上执行openshift-ansible部署openshift

== 定义虚拟机==

如果手动从virtualbox安装虚拟机、再安装系统的话，需要耗费不少时间，通过Vagrant我们可以快速自动化地创建出这样的一个机器集群，类似从docker拉取image一样。定义这些只需要创建一个Vagrantfile：

&lt;syntaxhighlight lang="lua"&gt;
Vagrant.configure("2") do |config|
    config.vm.box = "centos/7"
    config.vm.box_check_update = false

    config.vm.provider "virtualbox" do |vb|
        vb.memory = 2048
        vb.cpus = 1
    end

    config.vm.provision "shell", inline: &lt;&lt;-SHELL
        /vagrant/common.sh
    SHELL

    config.hostmanager.enabled = true
    config.hostmanager.manage_host = true
    config.hostmanager.ignore_private_ip = false
  
    (1..2).each do |i|
        config.vm.define "node0#{i}" do |node|
            node.vm.network "private_network", ip: "#{NETWORK_BASE}#{i}"
            node.vm.hostname = "node0#{i}.example.com"
        end
	end
end
&lt;/syntaxhighlight&gt;

以上的配置定义了操作系统、内存和cpu，以及网络和域名设置，然后创建node01、node02。这里用到了vagrant的hostmanager插件，他会去修改宿主机以及虚拟机的hosts文件，增加域名映射。同时，可以把一些公共的依赖项安装脚本进行provision，例如安装docker。然后，还需要创建master节点：

&lt;syntaxhighlight lang="lua"&gt;
config.vm.define "master", primary: true do |master|
        master.vm.network "private_network", ip: "#{NETWORK_BASE}0"
        # master.vm.hostname = "master.example.com"
        master.hostmanager.aliases = %w(master.example.com etcd.example.com nfs.example.com)
        master.vm.provider "virtualbox" do |vb|
            vb.memory = "4096"
            vb.cpus = 2
        end
end
&lt;/syntaxhighlight&gt;

创建了vagrantfile之后，就可以利用`vagrant up`命令来创建和启动这些虚拟机了。

[[File:Virtualbox-cluster.png|600px|Virtualbox]]

这里master的域名配置有个坑，那就是hostnamanger会会生成一个master.example.com的ip映射在hosts文件里面，但是这个文件开头还有127.0.0.1 指向 master.example.com，像这样：

&lt;syntaxhighlight lang="lua"&gt;
127.0.0.1       master.example.com      master
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

## vagrant-hostmanager-start
192.168.11.102  node02.example.com
192.168.11.100  master.example.com
192.168.11.101  node01.example.com
## vagrant-hostmanager-end
&lt;/syntaxhighlight&gt;

所以这里设置的`master.hostmanager.aliases`，同时要手动修改hostname：

&lt;syntaxhighlight lang="bash"&gt;
hostnamectl set-hostname master.example.com
&lt;/syntaxhighlight&gt;

== 安装依赖项==

各个节点上都需要安装docker环境，使用下面的命令安装：

&lt;syntaxhighlight lang="bash"&gt;
yum -y install docker-1.13.1

# http://softpanorama.org/VM/Docker/Installation/rhel7_docker_package_dockerroot_problem.shtml

usermod -aG dockerroot vagrant

cat &gt; /etc/docker/daemon.json &lt;&lt;EOF
{
    "group": "dockerroot"
}
EOF

systemctl enable docker
systemctl start docker
&lt;/syntaxhighlight&gt;

同时需要禁用掉SELinux：

&lt;syntaxhighlight lang="bash"&gt;
setenforce 0
sed -i 's/SELINUX=enforcing/SELINUX=permissive/g' /etc/selinux/config
&lt;/syntaxhighlight&gt;

而在master上需要装更多的依赖项：

&lt;syntaxhighlight lang="bash"&gt;
yum install wget git net-tools bind-utils yum-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct

yum install unzip

yum -y install https://releases.ansible.com/ansible/rpm/release/epel-7-x86_64/ansible-2.9.6-1.el7.ans.noarch.rpm
&lt;/syntaxhighlight&gt;

这里安装依赖项之前，可以考虑将Base源替换为163源，这样速度会稍微快一点。

== 配置ssh访问==

应为整个集群安装是在master上进行的，但实际上有一些东西是需要操作node的，因此要配置好在master上能直接无密码登录到其他的node上。这里通过ssh私钥的形式来设置，首先在Vagrantfile中:

&lt;syntaxhighlight lang="lua"&gt;
if File.exist?(".vagrant/machines/master/virtualbox/private_key")
    master.vm.provision "master-key", type: "file", source: ".vagrant/machines/master/virtualbox/private_key", destination: "/home/vagrant/.ssh/master.key"
end
if File.exist?(".vagrant/machines/node01/virtualbox/private_key")
    master.vm.provision "node01-key", type: "file", source: ".vagrant/machines/node01/virtualbox/private_key", destination: "/home/vagrant/.ssh/node01.key"
end
if File.exist?(".vagrant/machines/node02/virtualbox/private_key")
    master.vm.provision "node02-key", type: "file", source: ".vagrant/machines/node02/virtualbox/private_key", destination: "/home/vagrant/.ssh/node02.key"
end
&lt;/syntaxhighlight&gt;
然后通过下面的命令将文件拷贝过去：

&lt;syntaxhighlight lang="bash"&gt;
vagrant provision --provision-with master-key,node01-key,node02-key
&lt;/syntaxhighlight&gt;

这一步的目的是因为Vagrant在创建这些node的时候，这个key还没有生成，只能在创建完之后才能成功拷贝过去。然后设置master的ssh配置：

&lt;syntaxhighlight lang="bash"&gt;
# vagrant ssh master
#vim ~/.ssh/config
Host *
StrictHostKeyChecking no
&lt;/syntaxhighlight&gt;

到这一步，docker、ssh访问都应该是成功的，如果想检查是否配置成功，可以在master上测试：
&lt;syntaxhighlight lang="bash"&gt;
vagrant ssh master
docker -v
ssh -i node01.key vagrant@node01.example.com
&lt;/syntaxhighlight&gt;

== 创建Inventory==

通过ansible执行需要一个hosts文件，如下：

&lt;syntaxhighlight lang="ini"&gt;
[OSEv3:children]
masters
nodes
etcd

[OSEv3:vars]
ansible_ssh_user=vagrant
ansible_become=true
openshift_deployment_type=origin
openshift_disable_check=disk_availability,memory_availability,docker_storage,docker_image_availability

[masters]
master.example.com ansible_ssh_private_key_file="/home/vagrant/.ssh/master.key"

[etcd]
master.example.com ansible_ssh_private_key_file="/home/vagrant/.ssh/master.key"

[nodes]
master.example.com containerized=false etcd_ip=192.168.11.100 openshift_node_group_name='node-config-master-infra'  ansible_ssh_private_key_file="/home/vagrant/.ssh/master.key"
node01.example.com openshift_node_group_name='node-config-compute' ansible_ssh_private_key_file="/home/vagrant/.ssh/node01.key"
node02.example.com openshift_node_group_name='node-config-compute' ansible_ssh_private_key_file="/home/vagrant/.ssh/node02.key"

&lt;/syntaxhighlight&gt;

这里有几点坑：
* `containerized=false etcd_ip=192.168.11.100 `这个如果不加会导致[https://github.com/eliu/openshift-vagrant/issues/10 "Wait for control plane pods to appear" ]错误

这个文件保存到/etc/ansible/hosts。

== 安装==

在master上面安装ansible:

&lt;syntaxhighlight lang="bash"&gt;
yum -y install https://releases.ansible.com/ansible/rpm/release/epel-7-x86_64/ansible-2.9.6-1.el7.ans.noarch.rpm
wget https://github.com/openshift/openshift-ansible/archive/openshift-ansible-3.11.187-1.zip
&lt;/syntaxhighlight&gt;

然后，最好把openshift-ansible里面的mirror修改成国内的，否则很可能安装不成功或者要花很长时间：

&lt;syntaxhighlight lang="bash"&gt;
sed -i 's/mirror.centos.org/mirrors.163.com/g' openshift-ansible/roles/openshift_repos/templates/CentOS-OpenShift-Origin311.repo.j2
&lt;/syntaxhighlight&gt;

正是安装：

&lt;syntaxhighlight lang="bash"&gt;
ansible-playbook /home/vagrant/openshift-ansible/playbooks/prerequisites.yml &amp;&amp; ansible-playbook /home/vagrant/openshift-ansible/playbooks/deploy_cluster.yml
&lt;/syntaxhighlight&gt;

如果一切正常的话，就可以安装成功了。其中有几步比较耗时(大概十分钟左右），需要点耐心：

&lt;pre&gt;
TASK [openshift_node : Install node, clients, and conntrack packages]
TASK [openshift_node : Check status of node image pre-pull]
&lt;/pre&gt;

成功之后，可以看到log：

&lt;pre&gt;
PLAY RECAP ***********************************************************************************************************
localhost                  : ok=11   changed=0    unreachable=0    failed=0    skipped=5    rescued=0    ignored=0
master.example.com         : ok=622  changed=275  unreachable=0    failed=0    skipped=987  rescued=0    ignored=0
node01.example.com         : ok=130  changed=63   unreachable=0    failed=0    skipped=167  rescued=0    ignored=0
node02.example.com         : ok=130  changed=63   unreachable=0    failed=0    skipped=167  rescued=0    ignored=0


INSTALLER STATUS *****************************************************************************************************
Initialization               : Complete (0:00:18)
Health Check                 : Complete (0:00:04)
Node Bootstrap Preparation   : Complete (0:34:23)
etcd Install                 : Complete (0:00:32)
Master Install               : Complete (0:07:48)
Master Additional Install    : Complete (0:00:34)
Node Join                    : Complete (0:06:56)
Hosted Install               : Complete (0:00:56)
Cluster Monitoring Operator  : Complete (0:02:47)
Web Console Install          : Complete (0:01:45)
Console Install              : Complete (0:01:21)
Service Catalog Install      : Complete (0:07:53)
&lt;/pre&gt;

然后就可以访问` https://master.example.com:8443/`了：

[[File:Openshift-welcome.png|600px|Openshift home]]

Reference:
* [https://blog.csdn.net/sun_qiangwei/article/details/80443943 OpenShift 3.9 多节点集群（Ansible）安装]</text>
      <sha1>bc5hy4zssscl041comglu05ppeivcy2</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:在虚拟机中使用Docker</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2017-06-17T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>几年前在公司复杂维护机房的时候，就开始关注Docker这种基于容器的虚拟化技术了，当时并没有选择Docker，因为几年前docker刚起步还不是很成熟，不敢采用这样的技术（当然关键是自己不了解也能力不够）。当时采取的是KVM和Virtual Box，问题也很明显，因为一台物理机（Dell T320 32GRAM)开个四五台Virtual Box虚拟机就有点吃不消了，想做到专机专用，也是很困难的事情。当时的主要目的是想把Oracle、WebSphere等吃内存的东西隔离出来。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2620" sha1="hjmgl18px16hrzajqaxc2dyakoyagcy">
几年前在公司复杂维护机房的时候，就开始关注Docker这种基于容器的虚拟化技术了，当时并没有选择Docker，因为几年前docker刚起步还不是很成熟，不敢采用这样的技术（当然关键是自己不了解也能力不够）。当时采取的是KVM和Virtual Box，问题也很明显，因为一台物理机（Dell T320 32GRAM)开个四五台Virtual Box虚拟机就有点吃不消了，想做到专机专用，也是很困难的事情。当时的主要目的是想把Oracle、WebSphere等吃内存的东西隔离出来。
现在有机会接触到Docker了，有必要认真的学习下了。貌似在Mac上Docker实际上是运行在虚拟的Linux中的，因此决定使用虚拟机来运行Docker，以下是我的配置：

* Mac Book Pro, OSX
* Virtual Box, Ubuntu-server 16.04.2 X64 LTS, 4G Ram, 30G HDD，网卡桥接

好了，首先是安装Docker，参考[https://docs.docker.com/engine/installation/linux/ubuntu/#install-docker 官方文档]

&lt;syntaxhighlight lang="bash"&gt;
sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository \
   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"
sudo apt-get update
sudo apt-get install docker-ce
&lt;/syntaxhighlight&gt;
好了，这样就安装成功了。来下载一些[https://hub.docker.com/ docker镜像]吧，我们在后面可能慢慢会用到（先这样设想吧）：

* oracle/openjdk:8 选用Oracle的JDK8镜像来部署SpringBoot应用
* gitlab/gitlab-ce 用来搭建私有的GIT仓库
* sonatype/nexus3 用来搭建私有的Docker镜像库
* gocd/gocd-server 用来搭建gocd作CICD
* node 用来运行NodeJS的前端
* percona 用来提供MySQL服务
* mongo 用来提供MongoDB服务

&lt;syntaxhighlight lang="bash"&gt;
docker pull oracle/openjdk
docker pull gitlab/gitlab-ce
docker pull sonatype/nexus3
docker pull gocd/gocd-server
docker pull node
docker pull percona
docker pull mongo
&lt;/syntaxhighlight&gt;
你会发现太慢了，是不是?幸好可以有加速的方式，可以试用[https://www.daocloud.io/mirror#accelerator-doc DaoCloud]的加速器。
&lt;syntaxhighlight lang="bash"&gt;
sudo vim /etc/docker/daemon.json
&lt;/syntaxhighlight&gt;
输入以下内容:
&lt;syntaxhighlight lang="json"&gt;
{
    "registry-mirrors": [
        "http://1729****.m.daocloud.io"
    ],
    "insecure-registries": []
}
&lt;/syntaxhighlight&gt;
重启Docker：
&lt;syntaxhighlight lang="bash"&gt;
sudo /etc/init.d/docker restart
&lt;/syntaxhighlight&gt;</text>
      <sha1>hjmgl18px16hrzajqaxc2dyakoyagcy</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:使用GOCD部署一个SpringBoot应用</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2017-06-26T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>如何自动部署 springboot 的应用？</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2474" sha1="iacst1597kwmk5bzy25ctv5qe6ic97v">如何自动部署 springboot 的应用？
首先我们要做的是把GOCD的启动用户`go`加入到`docker`组中，这样就可以免`sudo`来执行docker的命令了
&lt;syntaxhighlight lang="bash"&gt;
sudo gpasswd -a go docker
sudo /etc/init.d/docker restart
newgrp - docker
&lt;/syntaxhighlight&gt;
我们现在来通过GOCD部署一个eureka的服务端。eureka用作服务发现，包含server端和client端，每个微服务是一个client注册到server上。看看需要哪些包：
&lt;syntaxhighlight lang="groovy"&gt;
dependencies {
    testCompile group: 'junit', name: 'junit', version: '4.12'
    compile group: 'org.springframework.boot', name: 'spring-boot-starter-web', version: '1.5.3.RELEASE'
    compile group: 'org.springframework.cloud', name: 'spring-cloud-starter-eureka-server', version: '1.3.1.RELEASE'
}
&lt;/syntaxhighlight&gt;
然后我们的程序写一句话就可以了：
&lt;syntaxhighlight lang="java"&gt;
@EnableEurekaServer
@SpringBootApplication
public class EurekaServerApplication {
    public static void main(String[] args){
        SpringApplication.run(EurekaServerApplication.class, args);
    }
}
&lt;/syntaxhighlight&gt;
这样启动后就可以访问到eureka的web页面了，我们可以在application.properties中配置一些属性：
&lt;syntaxhighlight lang="properties"&gt;
server.port=8761
eureka.client.register-with-eureka=false
eureka.client.fetch-registry=false
&lt;/syntaxhighlight&gt;
然后来想办法把应用打包到docker中。我们新建一个Dockerfile到src/main/docker中：
&lt;syntaxhighlight lang="lua"&gt;
FROM oracle/openjdk:8
VOLUME /tmp
ADD eureka-server-1.0-SNAPSHOT.jar app.jar
RUN sh -c 'touch /app.jar'
ENV JAVA_OPTS=""
ENTRYPOINT [ "sh", "-c", "java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /app.jar" ]
&lt;/syntaxhighlight&gt;
然后在build.gradle中添加一个构建任务：
&lt;syntaxhighlight lang="groovy"&gt;
buildscript {
    repositories {
        mavenCentral()
    }
    dependencies {
        classpath("org.springframework.boot:spring-boot-gradle-plugin:1.5.2.RELEASE")
        classpath('se.transmode.gradle:gradle-docker:1.2')
    }
}
task buildDocker(type: Docker, dependsOn: build) {
    push = false
    applicationName = jar.baseName
    dockerfile = file('src/main/docker/Dockerfile')
    doFirst {
        copy {
            from jar
            into stageDir
        }
    }
}
&lt;/syntaxhighlight&gt;
这样当我们执行`gradle build buildDocker`命令的时候就会把springboot应用打包成docker镜像了。</text>
      <sha1>iacst1597kwmk5bzy25ctv5qe6ic97v</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:搭建Nexus私服</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2017-06-25T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>我们安装[Nexus](https://store.docker.com/community/images/sonatype/nexus3)来作为我们的Docker镜像仓库。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="905" sha1="5z89vp21b6hok5azwgfdqz3sj6kwhu">
我们安装[https://store.docker.com/community/images/sonatype/nexus3 Nexus]来作为我们的Docker镜像仓库。
&lt;syntaxhighlight lang="bash"&gt;
sodu docker pull sonatype/nexus3
mkdir /home/docker/nexus
chown -R 200 /home/docker/nexus
sudo docker run -d \
    --name nexus \
    -v /home/docker/nexus \
    -p 8081:8081 \
    sonatype/nexus3
&lt;/syntaxhighlight&gt;
安装完成后，可以访问192.168.56.101:8081，登录进去后，添加一个Docker Hosted源，比如http://192.168.56.101:8081/repository/cloud-images/

我们可以把项目中需要用到的文件传到Nexus上。我们新建一个`raw`格式的repository
&lt;syntaxhighlight lang="bash"&gt;
curl --fail -u admin:admin123 --upload-file gradle-4.0-bin.zip 'http://192.168.56.101:8081/repository/files/'
&lt;/syntaxhighlight&gt;
这样就可以通过http://192.168.56.101:8081/repository/files/gradle-4.0-bin.zip 来访问我们这个文件了。</text>
      <sha1>5z89vp21b6hok5azwgfdqz3sj6kwhu</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Scheme语言的hello world</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2021-01-05T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>最近准备创建一个简单的基于JVM的新语言，但在语法设计上犹豫不决，研究了一些资料后，觉得可能Lisp正是我所追求的清晰、简单、优雅的语法参考，因此决定深入了解一下Scheme。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1104" sha1="ia3tjs26u43dvum5lg7olh4bnxd38q7">最近准备创建一个简单的基于JVM的新语言，但在语法设计上犹豫不决，研究了一些资料后，觉得可能Lisp正是我所追求的清晰、简单、优雅的语法参考，因此决定深入了解一下Scheme。

= 开发环境配置=
== 运行环境==
编译安装[https://github.com/cisco/ChezScheme ChezScheme]，据说是最好的scheme实现。

&lt;syntaxhighlight lang="bash"&gt;
git clone https://github.com/cisco/ChezScheme.git
cd ChezScheme
./configure
make
sudo make install
&lt;/syntaxhighlight&gt;

完成之后应该就可以来写一个hello world了：

&lt;pre&gt;
Chez Scheme Version 9.5.5
Copyright 1984-2020 Cisco Systems, Inc.

&gt; (+ 1 1)
2
&gt;
&gt; (display "Hello world")
Hello world
&lt;/pre&gt;

== Visual studio code==

需要安装两个插件

* vscode-scheme
* Code Runner

然后在settings中查找"Code-runner: Executor Map"，修改其中的scheme命令行：

&lt;syntaxhighlight lang="json"&gt;
"code-runner.executorMap": {
    "scheme": "scheme --script"
}
&lt;/syntaxhighlight&gt;


参考：

* [https://www.yinwang.org/blog-cn/2013/04/11/scheme-setup Scheme 编程环境的设置]</text>
      <sha1>ia3tjs26u43dvum5lg7olh4bnxd38q7</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:递归和迭代</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2021-02-26T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>这是读《计算机程序的构造和解释》的笔记。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1939" sha1="jk2v7qopo4yr8rp4ncqex5ojs6sig2s">这是读《计算机程序的构造和解释》的笔记。


= 递归和迭代=
== 计算裴波拉切数列==
裴波拉切数列是很简单的过程，其数学公式如下：

$$
Fib(n)=\left\{
\begin{array}{rcl}
0 &amp; {n = 0}\\
1 &amp; {n = 1}\\
Fib(n-1) + Fib(n-2) &amp; {n &gt; 1}\\
\end{array} \right.
$$

使用递归非常容易解决，就是直接将这个公式翻译成计算机语言即可：

&lt;syntaxhighlight lang="scheme"&gt;
(define (fib n)
    (cond 
        ((= n 0) 0)
        ((= n 1) 1)
        (else (+ (fib (- n 1))
                 (fib (- n 2)))
        )
    )
)
&lt;/syntaxhighlight&gt;
这个递归算法虽然实现很简单，但却有比较大的性能问题，出现了不必要的计算。例如计算Fib(5)，其计算过程如下：

[[File:Fib-tree.png|600px|Fib(5)]]

其中Fib(2)就计算了三次。那么，如何使用迭代来计算呢？迭代的思想在于给定若干变量的初始值，不断根据规则进行计算来改变这些变量，最后进行N次之后得到最终的结果。

$$
\begin{cases}
a = Fib(1) \\
b = Fib(0)
\end{cases}
\xrightarrow[\text{进行迭代}]{}
\begin{cases}
a = a + b \\
b = a
\end{cases}
\xrightarrow[\text{通过n次迭代变成}]{}
\begin{cases}
a = Fib(n+1)) \\
b = Fib(n)
\end{cases}
$$

这样实际上需要三个变量：

$$
\begin{cases}
F_{n} \\
F_{n+1} \\
Count
\end{cases}
\xrightarrow[\text{初始值}]{}
\begin{cases}
F_{n} &amp;= Fib(0)\\
F_{n+1} &amp;= Fib(1) \\
Count &amp;= n
\end{cases}
\xrightarrow[\text{第一次迭代}]{}
\begin{cases}
F_{n} &amp;= Fib(1)\\
F_{n+1} &amp;= Fib(0) + Fib(1) \\
Count &amp;= n - 1
\end{cases}\\
\xrightarrow[\text{第n次迭代}]{}
\begin{cases}
F_{n} &amp;= Fib(n)\\
F_{n+1} &amp;= Fib(n-1) + Fib(n) \\
Count &amp;= 0
\end{cases}
$$

那么，翻译成代码就是：

&lt;syntaxhighlight lang="scheme"&gt;
(define (fib n)
     (fib_iter 1 0 n)
)

(define (fib_iter a b i)
    (if (= i 0)
        b
        (fib_iter (+ a b) a (- i 1))
    )
)

&lt;/syntaxhighlight&gt;</text>
      <sha1>jk2v7qopo4yr8rp4ncqex5ojs6sig2s</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Redis实现分布式锁</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-07-17T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>Redis一个比较重要的应用场景就是分布式锁DLM (Distributed Lock Manager)。实际上已经有很多现成的redis库来完成这个功能了，但是可能实现途径有所差别，那么，正确的做法是什么呢？Redis官方建议了一个算法叫做`Redlock`，可以将其作为起点去实现更复杂的方案，来研究一下它的思路。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2630" sha1="s18kvbds1vb8sr87z9glslwoizfl31k">Redis一个比较重要的应用场景就是分布式锁DLM (Distributed Lock Manager)。实际上已经有很多现成的redis库来完成这个功能了，但是可能实现途径有所差别，那么，正确的做法是什么呢？Redis官方建议了一个算法叫做`Redlock`，可以将其作为起点去实现更复杂的方案，来研究一下它的思路。


= 原则=

要实现分布式锁得满足一些必要的条件：

* 互斥性：任意时刻只能有一个客户端能够成功获取锁
* 避免死锁：即使锁的持有者crash
* 容错性：只要redis集群大多数节点正常，客户端就能够获取或者释放锁

有一些常见的做法并不是安全的锁实现方式。

最简单的做法是，获取锁时，在redis创建一个带有过期时间的key。当需要释放锁时，删除这个key。这种做法无法避免单点故障，如果redis master宕机，则无法成功获取或者释放锁了。如果添加一个slave节点呢？很不幸也行不通，因为redis的主从复制是异步的，由此带来竞争：

* Client A 在master节点上获取了锁
* master节点在将数据同步到slave之前crash掉了
* slave提升为新的master
* Client B于是可以获取到相同的锁了（不满足互斥性）

= redis单机下的正确做法=

在不考虑redis集群的情况下，如何正确的实现一个分布式锁呢？其实也比较简单：

加锁：

&lt;syntaxhighlight lang="bash"&gt;
SET resource_name my_random_value NX PX 30000
&lt;/syntaxhighlight&gt;
其中，`NX`保证只有在key不存在的情况下才会被设置到redis中，`PX 30000` 设置了过期时间为30000毫秒。而key的值被设置为一个随机值，这个值必须在所有的客户端和加锁请求中唯一。之所以要这么做，是为了保证能够安全的释放锁，只有当key存在，且是由锁的持有者发起的解锁请求的时候，才删除这个key：

&lt;syntaxhighlight lang="lua"&gt;
if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
&lt;/syntaxhighlight&gt;

主要避免的一个问题就是，锁被其他的客户端给错误地释放了（有可能客户端释放锁的时候，因为某些原因锁已经过期了，但是其他的客户端已经获得了锁）。而锁的过期时间（或者说有效期），应该足够client完成操作，避免任务在进行的过程中其他客户端又获得了锁。

在单机的情况下以上就实现了一个较为完美的锁，如果要扩展到redis集群呢？

= Redlock 算法=
(TBD)


* [https://redis.io/topics/distlock Distributed locks with Redis]</text>
      <sha1>s18kvbds1vb8sr87z9glslwoizfl31k</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Redis Memory limit and eviction policy configuration</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-03-10T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>By default, the redis running inside docker has no limitation about memory however generally the container has. Thus if no further configuation of eviction or memory limit is set in redis, you may possibly get some error like Out-of-memory if you keep adding new keys to redis.</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8108" sha1="ja7f0ysgfst1r2gynrduodawea32xc5">By default, the redis running inside docker has no limitation about memory however generally the container has. Thus if no further configuation of eviction or memory limit is set in redis, you may possibly get some error like Out-of-memory if you keep adding new keys to redis.


= Testing with docker=

== Create a simple application==

To reproduce the problem, let's create a simple application by using SpringBoot. We'll have a post api for generating a random item and insert it into redis:

&lt;syntaxhighlight lang="java"&gt;
@PostMapping("/generate")
public Item generate() {
  Item item = Item.random();
  redisTemplate.opsForValue().set(item.getId(), item);
  return item;
}
&lt;/syntaxhighlight&gt;

== Keep inserting data via jmeter==

In order to keep inserting data into redis using the post api, we can use Apache jmeter to do this job.

[[File:Jmeter-report.png|600px|Jmeter]]

A few components should be added into the test plan:

* Thread group: to execute a lot of requests parallelly, eg. 20 threads, and using infinite loop
* Http Request sampler: to send post api
* Assertion results: to verify the request, eg. verify the response code should be 200
* Report: eg. Aggregate report to virtualize the result

== Round1: Redis with 128m memory==

Let's set the memory limit of redis container to 128m, by using this commands:

&lt;syntaxhighlight lang="bash"&gt;
docker run --name redis-128 \
		   -d \
		   -p 6381:6379 \
		   --memory="128m" \
		   -v /Users/hfli/Downloads/redis-test-data/redis-128:/data \
		   redis redis-server
&lt;/syntaxhighlight&gt;

Note that the option `--memory="128m"` sets the memory limitation for redis container, and also we perisit the data into a local volumum so that it will never lose after container restart.

To find the memory usage status of docker container, we can use the `docker stats` command:

&lt;pre&gt;
# docker stats
CONTAINER ID        NAME                CPU %               MEM USAGE / LIMIT   MEM %               NET I/O             BLOCK I/O           PIDS
a38d4615af6b        redis-128           0.12%               126.8MiB / 128MiB   99.06%              182MB / 18.6MB      3.08GB / 264MB      6
&lt;/pre&gt;

After successfully started redis, now we are able to use jmeter to test it. Keep running the job, and soon we'll get error in the application:

&lt;syntaxhighlight lang="log"&gt;
Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is org.springframework.data.redis.RedisSystemException: 
Error in execution; nested exception is io.lettuce.core.RedisCommandExecutionException: 
MISCONF Redis is configured to save RDB snapshots, but it is currently not able to persist on disk. Commands that may modify the data set are disabled, because this instance is configured to report errors during writes if RDB snapshotting fails (stop-writes-on-bgsave-error option). 
Please check the Redis logs for details about the RDB error.] with root cause
&lt;/syntaxhighlight&gt;

And we can also get some info about redis by using `info`（or use `info memory` to only get the info related to memory ) and `dbsize` command in redis-cli:

&lt;pre&gt;
# redis-cli
127.0.0.1:6379&gt; info
...
# Memory
used_memory:238777840
used_memory_human:227.72M
used_memory_rss:134189056
used_memory_rss_human:127.97M
used_memory_peak:238777840
used_memory_peak_human:227.72M
used_memory_peak_perc:100.00%
used_memory_overhead:42998602
used_memory_startup:791264
used_memory_dataset:195779238
used_memory_dataset_perc:82.26%
&lt;/pre&gt;
So it has used actually more that 128M! 

&lt;pre&gt;
127.0.0.1:6379&gt; dbsize
(integer) 843802
&lt;/pre&gt;

Now the redis server is able to start, but it will crash if you try to run `keys *` command.

== Round2: Redis with 128m memory and maxmemory==

&lt;syntaxhighlight lang="bash"&gt;
# redis.conf
maxmemory 100mb
&lt;/syntaxhighlight&gt;

Add the above maxmemory setting in redis.conf and then start redis by this:

&lt;syntaxhighlight lang="bash"&gt;
docker run -v /Users/hfli/Downloads/redis-test-data/redis-128.conf:/usr/local/etc/redis/redis.conf \
		   --name redis-128-max \
		   -d \
		   -p 6381:6379 \
		   --memory="128m" \
		   -v /Users/hfli/Downloads/redis-test-data/redis-128-max:/data \
		   redis redis-server /usr/local/etc/redis/redis.conf
&lt;/syntaxhighlight&gt;

This time we'll still get error when inserted lots of items, but the error message is a little bit different then:

&lt;pre&gt;
nested exception is org.springframework.data.redis.RedisSystemException: 
Error in execution; nested exception is io.lettuce.core.RedisCommandExecutionException:
 OOM command not allowed when used memory &gt; 'maxmemory'.] with root cause
&lt;/pre&gt;

That's because the default behavior of redis is that it will return error when memory limit is reached.

== Round3: Redis with 128m memory and maxmemory and lru==

Try to add one more line to set the lru policy for redis:

&lt;syntaxhighlight lang="bash"&gt;
# redis.conf
maxmemory 100mb
maxmemory-policy allkeys-lru
&lt;/syntaxhighlight&gt;

Then even if we keep inserting new data into redis, we'll never get some error, because redis will automatically evict the existing items to save memory, that means the `dbsize` of redis would not be always increcing anymore.

= Eviction policy=

According to the offical document of redis, LRU and LFU(Least Frequently Used) are supported by redis(~4.0). In order to use them first we need to set the memory limit so that redis will use a specified amout of memory for the data set. As shown in the above sections, we can set it in config file or set it thorugh redis-cli at runtime:

&lt;syntaxhighlight lang="bash"&gt;
config set maxmemory 100mb
&lt;/syntaxhighlight&gt;

If it's successfully configured we should be able to find it via `info` command:

&lt;pre&gt;
127.0.0.1:6379&gt; info memory
# Memory
...
maxmemory:104857600
maxmemory_human:100.00M
&lt;/pre&gt;
By default is the value `0` which means no limit at all, and for those running at a 32-bit system there is an implicit memory limit of 3GB.

The evicition policies are listed as bellow:

* **noeviction**: Return errors
* **allkeys-lru**: Remove less recently used(LRU) keys first
* **volatile-lru**: Only remove LRU keys that has expire set
* **allkeys-random**: Randomly remove keys
* **volatile-random**: Randomly remove keys that has expire set
* **volatile-ttl**: Evict keys with expire set and keys has shorter time to live(TTL) will be removed firstly
* **volatile-lfu**: Remove less frequently used(LFU) keys first
* **allkeys-lfu**: Remove LFU keys that has expire set

For volatile-lru, volatile-lfu, volatile-random and volatile-ttl policies, it's possible that no keys are available to be removed, then redis will behave like noeviction(ie. throw error).

== How to choose policy==

Generally LFU will be better than LRU, try to think that some items recently accessed but is actually almost never accessed in the future, if you use LRU then there is a risk that items get higher chance to be requested in the future will be evicted, while LFU does not have such a problem.

In short, follow these rules to choose the evict policy:

* **allkeys-lru**: If you expect that some elements will be accessed far more often than the rest, choose it. And if you're not sure, this is also the suggested option for you.
* **allkeys-random**: If you'd prefer that all elements have the same chance to be accessed, use it.
* **volatile-ttl**: If you wish to evict keys according to ttl
* **volatile-lru/volatile-random**: If you use the same redis for both persistent keys and caching, *BUT* usually in this case it's suggested to run two seperate instance of redis.

== Controll LRU precision==

The redis LRU algorithm is an approxmimated LRU algorithm, the reason behind is that use true LRU algorithm will cost more memory, while the approximation is virtually equivalent (good enough) for the application using redis. 

And you're able to tune the precision of LRU by:

&lt;syntaxhighlight lang="bash"&gt;
maxmemory-samples 5
&lt;/syntaxhighlight&gt;

The larger value you use, the more approximated you will get, but also with more CPU usage.

&lt;pre&gt;
maxmemory_human:0B
maxmemory_policy:noeviction
&lt;/pre&gt;</text>
      <sha1>ja7f0ysgfst1r2gynrduodawea32xc5</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:KDBX4 文件格式解析</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2019-03-07T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>最近因为开始开发我自己的密码管理软件，因此对一些开源的密码管理软件做了一下研究，这其中一个比较著名的就是[KeePass](https://keepass.info/)。KeePass将密码存在一个文本文件中，最新的格式是[KDBX4](https://keepass.info/help/kb/kdbx_4.html)，官方的KeePass是在.Net平台上开发的，也有不少其他平台的移植版本，当然KDBX解析的库也比较多，可惜即便是官方文档也没有详细的描述。几经折腾找到了一个比较好的实现[Keepassxc](https://keepassxc.org/)，这是一个基于c++和QT开发的跨平台版本，兼容Keepass的文件格式，我把代码做了精简就得到[一个KDBX的操作库](https://github.com/soleverlee/keepass-client)，顺便调试了一下KDBX的文件格式，看看它是怎么存密码的。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10697" sha1="ddeer1yki6gg6vjd67x5qntz4xr1671">最近因为开始开发我自己的密码管理软件，因此对一些开源的密码管理软件做了一下研究，这其中一个比较著名的就是[https://keepass.info/ KeePass]。KeePass将密码存在一个文本文件中，最新的格式是[https://keepass.info/help/kb/kdbx_4.html KDBX4]，官方的KeePass是在.Net平台上开发的，也有不少其他平台的移植版本，当然KDBX解析的库也比较多，可惜即便是官方文档也没有详细的描述。几经折腾找到了一个比较好的实现[https://keepassxc.org/ Keepassxc]，这是一个基于c++和QT开发的跨平台版本，兼容Keepass的文件格式，我把代码做了精简就得到[https://github.com/soleverlee/keepass-client 一个KDBX的操作库]，顺便调试了一下KDBX的文件格式，看看它是怎么存密码的。

[/images/ka.kdbx 这里]有一个使用keepass创建的简单数据库，master密码是1125482715。

[[File:keepass_ka.png|600px|ka.kdbx]]

= 文件头（明文）=
我们以十六进制形式打开文件可以看到这样的结构：

[[File:kdbx_hex_ka_header.png|600px|Hex of ka.kdbx]]

== 文件头格式==
其中，文件头的结构可以用以下的形式来表述：

$$
Item_{i} = Id_{i} + Length + Data \\
Header = MagicNumber + Version + Item_{0} + ... + Item_{n} + Hash + Hmac
$$

首先，MagicNumber=```0x9AA2D903 0xB54BFB67```，代表这是KDBX文件格式

然后可以看到Version=```0x00040000```，目前有这样几种版本：
    - 0x00040000=4
    - 0x00030001=3.1
    - 0x00030000=3
    - 0x00020000=2

然后是多个Header Item, 结构为[ID][Length][Data]，譬如```02100000 0031C1F2 E6BF7143 50BE5805 216AFC5A FF```即代表id=0x02, length=0x00000010=16, data=0x31~0xFF。其中，这些ID中有一些特殊含义的ID：

&lt;pre&gt;
EndOfHeader = 0,
Comment = 1,
CipherID = 2,
CompressionFlags = 3,
MasterSeed = 4,
TransformSeed = 5,
TransformRounds = 6,
EncryptionIV = 7,
ProtectedStreamKey = 8,
StreamStartBytes = 9,
InnerRandomStreamID = 10,
KdfParameters = 11,
PublicCustomData = 12
&lt;/pre&gt;
所以这个Header就是表明加密算法，这些算法用UUID来标记：

* ```0x31c1f2e6bf714350be5805216afc5aff``` AES
* ```0xad68f29f576f4bb9a36ad47af965346c``` TWOFISH
* ```0xD6038A2B8B6F4CB5A524339A31DBB59A``` CHACHA20

== 文件头校验==

在文件头的后面有两个比较特殊的段，存储了两个用来验证文件头正确性的字段：

* Header Hash(SHA-256)，即文件头的哈希值
* Hmac(HMAC-SHA-256 )值，为文件头和密码一起加密后得出的值

通过计算哈希值能够判断文件头是否被人篡改，或者更准确的说是不是出现了损坏，因为如果真的被人篡改了，我相信他会连这个hash一起改掉，验证没有太大意义。因为KDB中数据采取了对称加密算法，而文件中也不会存储主密码，所以我们如何知道用户输入的密码是不是正确呢？

在Kdb以前的版本中，是尝试通过使用用户输入的密码去进行解密，如果出现问题或者解密出来的内容哈希值对不上，那么密码不对了。而在kdbx4中，采取了HMAC的方式，Hmac在哈希的基础上，加入了一个Key，意味着同一段数据，用不同的Key哈希之后的结果是不一样的。那么就可以根据用户输入的密码来计算Hmac值，如果和文件中记录的对不上，认为密码错误。

其实这个问题我也想过，我之前的想法是，把一段已知的明文加密后存储起来，然后再解密的时候，尝试用用户的密码加密后，来解密这个密文，看是否匹配。。当然如果这样做，需要考虑一下[https://zh.wikipedia.org/wiki/%E5%B7%B2%E7%9F%A5%E6%98%8E%E6%96%87%E6%94%BB%E5%87%BB 已知明文攻击]。

[https://stackoverflow.com/questions/14493029/reliable-way-to-tell-if-wrong-key-is-used-in-aes256-decryption 这里]还有有一个讨论可以参考。

== Key transform==

虽然不同的用户设置的密码都不一样，但通常我们在进行加密的时候，不会直接拿这个作为Key，而是会通过KDF ^[key derivation function] 将原始密码进行转换。keepass也不例外，我们这个文件设置的是使用Argon2来进行KDF，之前的版本采取的是AES-KDF。Keepass中转换的步骤如下:

=. 将原始密码进行SHA-256转换，即 $$ sha256（1125482715）= d31d31dd2d99b5d35ce232896d0b3f1fe41daf6ba47b5c24d52e8890a0307da6 $$=
=. 再进行一次SHA-256 $$ sha256(d31d31dd2d99b5d35ce232896d0b3f1fe41daf6ba47b5c24d52e8890a0307da6) = \\ bfa11b4e4376cf1b17088a3de375f1df6a9c4cb3eb36f3ce2416b10481eb619f $$=
=. 将上次得到的哈希值，同header中配置的Transform seed进行KDF，得到最终的transformedMasterKey, 这里我们用的是argon2。$$ argon2d(2, 1024, pwd, salt) = \\=
104e9ba7b6b4479eec1a8fe3f9ca285fd10e0f33435fcabd8edf3e16380a98c7 $$这一步计算参见下面的代码：

其中：$$ KdfSeed=3f09ea13ceffb8e867a4af3ab17854f9f5f152591653c737a8962b94356e2c0f $$

&lt;syntaxhighlight lang="c"&gt;
#include "argon2.h"
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

int main(int argc, char*argv[])
{
    uint8_t pwd[32] = {0xbf, 0xa1, 0x1b, 0x4e, 0x43, 0x76, 0xcf, 0x1b, 0x17, 0x08, 0x8a, 0x3d, 0xe3, 0x75, 0xf1, 0xdf, 0x6a, 0x9c, 0x4c, 0xb3, 0xeb, 0x36, 0xf3, 0xce, 0x24, 0x16, 0xb1, 0x04, 0x81, 0xeb, 0x61, 0x9f};
    uint8_t salt[32] = {0x3f, 0x09, 0xea, 0x13, 0xce, 0xff, 0xb8, 0xe8, 0x67, 0xa4, 0xaf, 0x3a, 0xb1, 0x78, 0x54, 0xf9, 0xf5, 0xf1, 0x52, 0x59, 0x16, 0x53, 0xc7, 0x37, 0xa8, 0x96, 0x2b, 0x94, 0x35, 0x6e, 0x2c, 0x0f};
    uint8_t result[32];
    // argon2: seed=kef.seed, version=19, rounds=2, memory=1024, parallelism=2
    argon2_hash(2, 1024, 2, pwd, 32, salt, 32, result, 32, nullptr, 0, Argon2_d, 19);
    for(int i = 0; i &lt; 32; i ++)
        printf("%02x", result[i]);
}
&lt;/syntaxhighlight&gt;

== Hmac计算==

另一个就是HMac值的计算了，首先需要算出一个Key，在keepass中是这样去算的:

$$
Key1 = sha512(MasterSeed + TransformedMasterKey + 0x01)
$$

&lt;pre&gt;
sha512(17e4aa736440b2c6f963184b9baf07a3c2b7ac652a95d4b375baf938cd5dbe4b104e9ba7b6b4479eec1a8fe3f9ca285fd10e0f33435fcabd8edf3e16380a98c701)
= 9340685dcea0fbee49a68417708cbffb24958fc6fb20de6cb158196b6291f0719f46669bbc8f7254bcbc0da0650d795fe9c782e443d3f32b7a957f73c8f58128
&lt;/pre&gt;

然后需要把这个key再计算一下:

$$
Key = sha512(BlockIndex + Key1)
$$

&lt;pre&gt;
sha512(ffffffffffffffff9340685dcea0fbee49a68417708cbffb24958fc6fb20de6cb158196b6291f0719f46669bbc8f7254bcbc0da0650d795fe9c782e443d3f32b7a957f73c8f58128)
=1062ee78cf505ac4af4e53f343b04782178a3c6d6b8e64ecb23ca6ce9489ab30660b92cf1f88dbf0333769e9f362ae2d7dff82554d864a4c2d1d3b751b5698f7
&lt;/pre&gt;

这个Key才是最终用来计算Hmac的Key:

$$
HmacValue = Hmac-sha256(header, Key)
$$

&lt;pre&gt;
Hmac-sha256(03D9A29A67FB4BB500000400021000000031C1F2E6BF714350BE5805216AFC5AFF030400000000000000042000000017E4AA736440B2C6F963184B9BAF07A3C2B7AC652A95D4B375BAF938CD5DBE4B0B8B00000000014205000000245555494410000000EF636DDF8C29444B91F7A9A403E30A0C040100000056040000001300000005010000004908000000020000000000000005010000004D0800000000001000000000000401000000500400000002000000420100000053200000003F09EA13CEFFB8E867A4AF3AB17854F9F5F152591653C737A8962B94356E2C0F000710000000C1F6FD873E14050697C168B3E9DA5DB200040000000D0A0D0A, 1062ee78cf505ac4af4e53f343b04782178a3c6d6b8e64ecb23ca6ce9489ab30660b92cf1f88dbf0333769e9f362ae2d7dff82554d864a4c2d1d3b751b5698f7)
=376123254b1aef5db7cb13e73807fc74341b8baa7e182a50f4cfdf14d5fdd532
&lt;/pre&gt;

= 文件内容（Encrypted)=

== 秘钥计算==
在文件头后面，跟着的是文件的数据内容了，这部分数据是加密过的。因此首先需要知道是根据什么样的秘钥进行加密的。其实很简单:

$$
Key = sha256 (MasterSeed + TransformedMasterKey)
$$

&lt;pre&gt;
sha256(17E4AA736440B2C6F963184B9BAF07A3C2B7AC652A95D4B375BAF938CD5DBE4B104e9ba7b6b4479eec1a8fe3f9ca285fd10e0f33435fcabd8edf3e16380a98c7)
=dce60234d641f71f377ecafb5a566ce954d26c03fd3b5b23e9ed092ef42b5290
&lt;/pre&gt;

所以这个文件中，解密是这样的：

&lt;pre&gt;
Key=dce60234d641f71f377ecafb5a566ce954d26c03fd3b5b23e9ed092ef42b5290
Iv=c1f6fd873e14050697c168b3e9da5db2

9a0106470245744f9121bbafa5dd10df =&gt; 01040000000300000002400000008B2E
&lt;/pre&gt;

这里需要指出的是，这里应该使用AES-CBC-NoPadding算法，这样加密后的密文和原文是一样的长度，否则会变长。而且解密的时候，是一段一段的解的，16byte一截。

&lt;syntaxhighlight lang="java"&gt;
public static byte[] decrypt(byte[] key, byte[] initVector, byte[] encrypted) {
        try {
            IvParameterSpec iv = new IvParameterSpec(initVector);
            SecretKeySpec skeySpec = new SecretKeySpec(key, "AES");

            Cipher cipher = Cipher.getInstance("AES/CBC/NoPadding");
            cipher.init(Cipher.DECRYPT_MODE, skeySpec, iv);

            return cipher.doFinal(encrypted);
        } catch (Exception ex) {
            ex.printStackTrace();
        }

        return null;
    }
&lt;/syntaxhighlight&gt;

== Inner Header==

Inner Header跟Header结构一样，对应了如下的类型：
&lt;pre&gt;
0x00: End of header.
0x01: Inner random stream ID (this supersedes the inner random stream ID stored in the outer header of a KDBX 3.1 file).
0x02: Inner random stream key (this supersedes the inner random stream key stored in the outer header of a KDBX 3.1 file).
0x03: Binary (entry attachment). D = F ‖ M, where F is one byte and M is the binary content (i.e. the actual entry attachment data). F stores flags for the binary; supported flags are:
    0x01: The user has turned on process memory protection for this binary.
&lt;/pre&gt;

== XML Database==

Inner Header之后一大段就是XML加密后的内容了，直接解密就可以了。解密出来其实就是个XML。这里就不过多解释了。
最终结构就是如图所示了：

[[File:keepass_hex_sturcture.png|600px|Hex of ka.kdbx]]，

目前还有两个地方没大搞懂的就是，标红的地方，就是加密的部分开头和结尾的，不知道有何用，代码嵌套的挺深的，看了下没找到地方，各个文档中也没说清楚，不过可以肯定的是，这两个地方用到了。有时间再看吧。


References:

* [https://cryptii.com/pipes/aes-encryption Cryptii]
* [https://www.devglan.com/online-tools/aes-encryption-decryption AES Encryption and Decryption Online Tool(Calculator)]
* [http://extranet.cryptomathic.com/hashcalc/index Hash]
* [https://github.com/Evidlo/keepassxc-specs/blob/master/kdbx-binary/kdbx4_overview.md overview for KDBX4]
* [https://gist.github.com/msmuenchen/9318327 KeePass v2.x (KDBX v3.x) file format]
* [https://gist.github.com/lgg/e6ccc6e212d18dd2ecd8a8c116fb1e45 Keepass file format explained]</text>
      <sha1>ddeer1yki6gg6vjd67x5qntz4xr1671</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:破解Db Schema序列号</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2016-07-22T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>我始终认为数据库设计在系统设计中是一个很重要的工作，然而一直没有比较好的ER建模工具。使用过MySQL Workbench和Power Designer两种工具，但都存在很多不喜欢的地方，直到遇到DbSchema后眼前一亮，这才是一个Nice的工具嘛。
很可惜对于我们这种屌丝来说，是不舍得花钱去购买一个license的，试用期15天到了怎么办呢？当时也没发现有可用的破解版，因为它是基于Java的，这对破解来说减小了难度，于是趁着辞职后在家没事的空档来研究了一下破解。其实也就上午花了一会时间就搞定了。记录下破解的过程。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4589" sha1="oja7ygoq8afxg4r1l9xc0xv1o89nnr0">我始终认为数据库设计在系统设计中是一个很重要的工作，然而一直没有比较好的ER建模工具。使用过MySQL Workbench和Power Designer两种工具，但都存在很多不喜欢的地方，直到遇到DbSchema后眼前一亮，这才是一个Nice的工具嘛。
很可惜对于我们这种屌丝来说，是不舍得花钱去购买一个license的，试用期15天到了怎么办呢？当时也没发现有可用的破解版，因为它是基于Java的，这对破解来说减小了难度，于是趁着辞职后在家没事的空档来研究了一下破解。其实也就上午花了一会时间就搞定了。记录下破解的过程。
首先是找到dbschema.jar，这是程序的主要jar包，其他是一些第三方的jar包和jdbc驱动等，于是它就是破解的关键。利用jd-gui反编译这个jar包，首先把源码都保存下来。

顺藤摸瓜，首先打开dbschema的注册窗口，根据里面的关键字搜索，比如Registration，然后一个个去找，这时，发现一个对话框：
&lt;pre&gt;
&lt;code class="java"&gt;
public class RegistrationDialog
...
JButton localJButton1 = new JButton(getAction("register"));
&lt;code&gt;
&lt;/pre&gt;
这不就是注册的按钮么？然后就看它的action:
&lt;syntaxhighlight lang="java"&gt;
/*     */   public void register() {
/*  96 */     String str1 = this.b.getText();
/*     */     
/*  98 */     if ((str1 == null) || (str1.length() == 0)) {
/*  99 */       JOptionPane.showMessageDialog(this, d.a(11), "Error", 0);
/* 100 */       return;
/*     */     }
/* 102 */     String str2 = this.c.getText();
/* 103 */     if (str2 == null) {
/* 104 */       JOptionPane.showMessageDialog(this, d.a(19), "Error", 0);
/* 105 */       return;
/*     */     }
/* 107 */     str1 = str1.trim();
/* 108 */     str2 = str2.trim();
/* 109 */     e.b(d.a(31), str1);
/* 110 */     e.b(d.a(21), str2);
/*     */     
/* 112 */     int i = g.b();
/* 113 */     if (i == Integer.MAX_VALUE) {
/* 114 */       dispose();
/* 115 */       JOptionPane.showMessageDialog(this.a.c(), d.a(23), "Info", 1, null);
/* 116 */       this.a.c().c();
/* 117 */     } else if ((i &gt; 0) &amp;&amp; (str1.toLowerCase().startsWith("extend"))) {
/* 118 */       dispose();
/* 119 */       JOptionPane.showMessageDialog(this.a.c(), d.a(24).replaceAll("\\{days\\}", "" + i), "Info", 1, null);
/* 120 */     } else if (i == -2) {
/* 121 */       String str3 = d.a(77).replace("{0}", new SimpleDateFormat("dd.MMMMM.yyyy").format(new Date(g.c())));
/* 122 */       JOptionPane.showMessageDialog(this.a.c(), str3, "Error", 0);
/*     */     } else {
/* 124 */       JOptionPane.showMessageDialog(this.a.c(), d.a(8), "Error", 0);
/*     */     }
/*     */   }
&lt;/syntaxhighlight&gt;
112行开始有点意思，其实大概能猜到是干什么，反正是算剩余天数的，那么这个int i = g.b();就是最核心的东西了：
&lt;syntaxhighlight lang="java"&gt;
public static int b()
  {
    String str1 = e.d(d.a(31), null);
    String str2 = e.d(d.a(21), null);
    int m = -1;
    if ((str1 != null) &amp;&amp; (str2 != null) &amp;&amp; (str2.length() &gt; 3))
    {
      if ((str1.toLowerCase().startsWith("extend")) &amp;&amp; (c(str1, str2)))
      {
        m = Math.max(15 - f("mmax"), -1);
      }
      else if (str2.length() &gt; 15)
      {
        String str3 = str2.substring(4, 9);
        String str4 = str2.substring(0, 4) + str2.substring(9);
        if (c("ax5" + str1 + "b52w" + str3 + "vb3", str4))
        {
          try
          {
            k = Integer.parseInt(str3) * 86400000L + 1356994800000L;
          }
          catch (NumberFormatException localNumberFormatException)
          {
            c.b(localNumberFormatException);
          }
          m = Integer.MAX_VALUE;
        }
      }
    }
    else {
      m = Math.max(15 - f("mma"), -1);
    }
    return m;
  }
&lt;/syntaxhighlight&gt;
看到这，我们其实已经拿到了计算key的方法，只不过这是一个验证的函数，如果我们要计算出key，需要反向推倒出来，这里就不具体解释了，最终反向出来的代码其实很简单，我做了一个C++版本的：
&lt;syntaxhighlight lang="java"&gt;
inline const string generateKey(string name)
{
    string salt = getSalt();
    cout &lt;&lt; "salt:" &lt;&lt; salt &lt;&lt; endl;
    string encryptSource = "ax5" + name + "b52w" + salt + "vb3";
    cout &lt;&lt; "encrypt:" &lt;&lt; encryptSource &lt;&lt; endl;
    string hash = MD5(encryptSource).toStr();
    cout &lt;&lt; "md5:" &lt;&lt; hash &lt;&lt; endl;
    return hash.substr(0, 4) + salt + hash.substr(4);
}
&lt;/syntaxhighlight&gt;
于是我们就有了一个key生成器了，完整的key生成器源码在Github。</text>
      <sha1>oja7ygoq8afxg4r1l9xc0xv1o89nnr0</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:使用OpenID Connect进行用户认证</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-11-30T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>OAuth2作为一个广泛使用的授权标准，已经基本上普及了，但是其协议本身是比较复杂的，如果不仔细研究还是会一知半解。一个常见的错误用法就是用OAuth2来进行认证（Authentication）。OAuth2不是为解决认证的问题的协议，也没有定义认证的流程；但是，在OAuth2的基础上，加以扩展得到的OpenID Connect确是为解决这个问题而生的。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8438" sha1="bow8audrc3oyl2tfmvmbbijkgrsds50">
OAuth2作为一个广泛使用的授权标准，已经基本上普及了，但是其协议本身是比较复杂的，如果不仔细研究还是会一知半解。一个常见的错误用法就是用OAuth2来进行认证（Authentication）。OAuth2不是为解决认证的问题的协议，也没有定义认证的流程；但是，在OAuth2的基础上，加以扩展得到的OpenID Connect确是为解决这个问题而生的。

= OAuth2为什么不能用来进行认证=
== 认证（Authentication）和 授权（Authorization）的区别==

Authentication与Authorization是有区别的。

* Authentication：the process of verifying an identity (who they say they are)
* Authorization：the process of verifying what someone is allowed to do (permissions)

== OAuth Login的问题==

在OAuth的使用场景中，一种比较常见的攻击场景是“Threat: Code Substitution (OAuth Login)”，即使用OAuth得到的code来登陆到应用中，通常在“social login"中会存在这样的威胁。

[[File:OAuth_login.png|600px|OAuth Login]]

如上所示，客户端认为可以拿到用户的信息（通过authorization code flow)，那么操作的主体就是用户本人，于是很简单就实现了”微信登陆“的功能。这里的问题在于，攻击者可以自己申请一个”合法“的client，然后诱导用户请求同样的API的access_token，在这个例子中，也就是”获取微信用户信息“，然后按照OAuth2的流程，授权服务器会调用一个url来返回给其一个”code“，这个code是绑定到了登陆的用户的。

攻击者这时可以利用这个”code“，然后去调用client的回调接口，模拟从OAuth2的认证服务器回调。这时候client会拿着这个code去换取access_token，然后获取用户信息，然后就登陆成功了。当然这个其实很好解决：

* code一定要绑定到client_id上，也就是说一个code只能由申请它的client去换取access_token，这个检测是由Authorization server完成的

那么，既然OAuth2已经解决了这个问题，是不是就用来做认证了呢？答案还是否定的，原因很简单，因为即便你可以解决这些问题，OAuth2中并没有定义一个”标准“的做法来实现认证。

&gt; Clients should use an appropriate protocol, such as OpenID (cf.
      [OPENID]) or SAML (cf. [OASIS.sstc-saml-bindings-1.1]) to
      implement user login.  Both support audience restrictions on
      clients.

OAuth2的`access_token`的实质是将用户的一些访问权限（scopes）代理给客户端。通过OAuth2客户端可以获取一个`access_token`，代表一个用户授权进行某些操作的凭证。客户端可以通过OAuth2的introspection endpoint来获取元数据，例如用户名等。虽然在OAuth2协议中，这个endpoint本身是给resource server验证token使用的，但实际上并没有限制说只有resource server可以访问。那么是否可以用OAuth来做认证呢？

但OAuth2设计access_token的本意是给资源服务器使用的。资源服务器根据其判断是否有权限访问资源，而并不关心client程序是谁。因此实际上access_token代表的是代理的用户权限，而不是用户本身。另外如果是client credential的授权流程的话。就根本没有用户存在了。在OpenID Connect的规范中也提到了：

&gt; They define mechanisms to obtain and use Access Tokens to access resources but do not define standard methods to provide identity information. Notably, without profiling OAuth 2.0, it is incapable of providing information about the authentication of an End-User.

== OpenID Connect与OAuth2的区别在哪==

实际上OIDC在OAuth2上的核心区别在于，OpenID flow最终会生成一个"ID Token"而不是access token，借此来对用户进行认证。

&gt; OpenID Connect implements authentication as an extension to the OAuth 2.0 authorization process. Use of this extension is requested by Clients by including the openid scope value in the Authorization Request. Information about the authentication performed is returned in a JSON Web Token (JWT) [JWT] called an ID Token (see Section 2). 

= OpenID Connect授权流程=

OpenID Connect中定义了几种授权流程：

* Authorization Code Flow
* Implicit Flow
* Hybrid Flow

实际上，尽管有多种授权流程可用，但推荐的做法是使用Authorization code flow（PKCE)，以保证最佳的安全性。

== Authorization Code Flow==
OpenID Connect的流程跟OAuth2的code流程差别不大，流程为：

* client发起认证请求到授权服务器上
* 授权服务器认证终端用户
* 授权服务器提示用户并得到用户的授权
* 授权服务器重定向到client，并带一个Authorization code
* client通过Authorization code换取token（id_token和access_token)
* client验证id_token，认证完成

首先是客户端生成授权的URL：

&lt;syntaxhighlight lang="lua"&gt;
https://authorization-server.com/authorize?
  response_type=code
  &amp;client_id=egHuu4oJxgOLeBzPAQ9sXg4i
  &amp;redirect_uri=https://www.oauth.com/playground/oidc.html
  &amp;scope=openid+profile+email+photos
  &amp;state=sRROJ_iPTam39Dc7
  &amp;nonce=eFRvo_n5ecyYU_Sv
&lt;/syntaxhighlight&gt;

这里比OAuth的流程多了一个`nonce`的随机字符串。这是用来防止replay攻击的，相当于对token的一个额外的验证，而state设计师为了防止CSRF的。然后跳转到授权服务器登陆成功后，会redirect并附带一些参数：

&lt;pre&gt;
?state=sRROJ_iPTam39Dc7
  &amp;code=MsxVU0nqVYeg0BdPMV59atYOUSCZKzpbcDbCrBXwVVNt2Xw7
&lt;/pre&gt;

然后拿这个code去换取token:

&lt;syntaxhighlight lang="lua"&gt;
POST https://authorization-server.com/token

grant_type=authorization_code
&amp;client_id=egHuu4oJxgOLeBzPAQ9sXg4i
&amp;client_secret=p4NlH7i7o2JQJ9xpGdhG95eXWgX1I8teWYZo8pH5-vILSZXv
&amp;redirect_uri=https://www.oauth.com/playground/oidc.html
&amp;code=MsxVU0nqVYeg0BdPMV59atYOUSCZKzpbcDbCrBXwVVNt2Xw7
&lt;/syntaxhighlight&gt;

最终可以拿到access_token以及id_token:

&lt;syntaxhighlight lang="json"&gt;
{
  "token_type": "Bearer",
  "expires_in": 86400,
  "access_token": "B1dETMtgNOPBHD8CfgkcM4PEhZxOt748pUeejk_6gfUVMpfIhObdfhLigQKLQ7MVjNj4zDmb",
  "scope": "openid profile email photo",
  "id_token": "eyJraWQiOiJzMTZ0cVNtODhwREo4VGZCXzdrSEtQUkFQRjg1d1VEVGxteW85SUxUZTdzIiwiYWxnIjoiUlMyNTYifQ.eyJzdWIiOiJjb25jZXJuZWQtY2FyYWNhbEBleGFtcGxlLmNvbSIsIm5hbWUiOiJDb25jZXJuZWQgQ2FyYWNhbCIsImVtYWlsIjoiY29uY2VybmVkLWNhcmFjYWxAZXhhbXBsZS5jb20iLCJpc3MiOiJodHRwczovL3BrLWRlbW8ub2t0YS5jb20vb2F1dGgyL2RlZmF1bHQiLCJhdWQiOiJlZ0h1dTRvSnhnT0xlQnpQQVE5c1hnNGkiLCJpYXQiOjE2MDA2NzQ1MTQsImV4cCI6MTYwMzI2NjUxNCwiYW1yIjpbInB3ZCJdfQ.ZoPvZPaomdOnnz2GFRGbgaW7PPWIMFDqSBp0gbN4An4a9F-Bc-4_T9EBGV8aGetyjZYAON0gjNV0p0NGFiwettePWKuxBzusuGCEd9iXWWUO9-WTF5e2AGr3_jkg34dbxfiFXy3KgH7m0czm809cMaiZ_ofLYgJHVD8lqMQoWifhoNhpjPqa19Svc3nCHzSYHUgTXQWvA56NmQvyVPh_OM7GMpc6zHopmihJqt3eREof8N-bOd7FL39jeam2-k1TFSDogyJE513aC0OssRADr_TWvtL8xoaPkXM_7bXYs9_7erXmzF9la0hvmOuasieetpLhOvFeoiOJWCU9xhxj4Q"
}
&lt;/syntaxhighlight&gt;

简而言之，

* id_token是给client做认证用的，可能包含一些用户敏感的信息
* access_token是给resource server用的

== Implicit flow==

这种授权流程的步骤如下：

* client发起认证请求到授权服务器上
* 授权服务器认证用户
* 授权服务器得到用户的授权
* 授权服务器直接将id_token以及access_token(如果请求了的话)到client上
* client校验id token，完成认证

跟code flow的区别在于，授权服务器认证用户完成之后直接将token发给了client而不是发送一个code。这种流程设计是本身是针对运行在浏览器上的client的，已经不被建议使用。

= 其他=
== id_token结构==
如上生成的token解析出来如下：
&lt;syntaxhighlight lang="json"&gt;
{
  "sub": "concerned-caracal@example.com",
  "name": "Concerned Caracal",
  "email": "concerned-caracal@example.com",
  "iss": "https://pk-demo.okta.com/oauth2/default",
  "aud": "egHuu4oJxgOLeBzPAQ9sXg4i",
  "iat": 1600674514,
  "exp": 1603266514,
  "amr": [
    "pwd"
  ]
}
&lt;/syntaxhighlight&gt;

id_token中包含了一些必须的信息：

* iss: 证书的签发者
* sub: 对应的主体（也就是到底认证的是谁了）的标识，通常就是用户名
* aud: 证书的受众，必须包含client_id。
* exp: 过期时间
* iat: 签发时间

Reference：

* [https://tools.ietf.org/html/rfc6819 RFC6819 - OAuth 2.0 Threat Model and Security Considerations]</text>
      <sha1>bow8audrc3oyl2tfmvmbbijkgrsds50</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:OAuth 2.0的一些问题</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-12-01T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>OAuth2或者OIDC是一个比较复杂的问题，但是很多人用的时候都是一知半解，所以出现一些不正确或者不建议的做法。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5274" sha1="34onfoxyuzlgmq10vxi12subs68yoqq">OAuth2或者OIDC是一个比较复杂的问题，但是很多人用的时候都是一知半解，所以出现一些不正确或者不建议的做法。

= 应用场景=

一个是在什么情况下应该使用OAuth/OIDC的问题。

[[File:OAuth_OIDC_cases.png|600px|Cases]]

* 第一种是应用自己完成认证授权的流程，通常的做法是通过用户名和密码登录认证；然后通过规则来判断用户是否有权限进行某些操作；这种跟OAuth没有半毛线关系
* 第二种是，在第一种的基础上，应用也希望能够通过第三方登录（但同时自己也有一套用户机制），也就是典型的“social login”，那么应该考虑使用OIDC
* 第三种是应用自己没有用户体系，需要借助第三方来进行认证，但应用自己管理资源。这种情况下也需要用OIDC来进行登陆认证，跟第二种没有实质的区别
* 第四种是，在第一种的基础上，假设应用还需要访问一些其他的受保护的资源，比如你写博客的时候想用QQ空间的照片。那么应该通过OAuth2来获取对这个资源的访问权限即可
* 第五种情况，应用通过第三方进行认证，有自己的资源；但同时也需要访问其他受保护的资源。这里认证肯定还是需要通过OIDC来实现；访问受保护的资源使用OAuth2即可。对于用户自己的资源，可以根据自己的需要进行管理，而不需要通过什么access_token。

这里有一个很大的误区就是，认为凡是授权都需要OAuth2，譬如第五种情况，的确我们通过OIDC完成认证体系；那么问题是，假设我访问自己的resource，是否需要走一遍OAuth2的流程？试想我们这么做，那么可能会变成这样：

* 用户通过QQ登陆到你开发的一个博客网站，很开心，想自己去写博客什么的
* 用户现在想去看自己的博客，而你需要得到一个access_token，所以会弹出一个框告诉用户，你要访问你的博客，是否允许... 这不是很扯淡么。如果我认证之后每次都还需要这样做一次，那有什么意义存在呢？
* 好吧，假设说你用一个client credential的流程，这样应用自己可以去请求一个access_token了，不需要用户参与。这样是否可行呢？

如果client app跟resource server是同一个应用，那么这样相当于我自己去拿一个token，然后我自己验证这个token，然后知道我是否有权限。且不论效率问题，唯一可能的场景就是必须要通过OAuth Server来获取用户的权限，但通常这些规则也必须要进行设置（那为什么要到OAuth中设置？这样通常代价更高）。

如果client app跟resource server不是一个应用，这样做倒是可以实现一个统一的权限管理机制，不过，唯一的问题就是性能问题。

= Token相关=
== Token的存储==

=== Web APP===
* 如果应用有服务端，那么token存储在服务端。浏览器通过session跟服务器交互
* 如果没有服务端，那么id_token和access_token应该存储在浏览器的内存中
* 如果不得不存储在浏览器，那么可以通过加密之后存储在session cookie中

[[File:https://images.ctfassets.net/cdy7uua7fh8z/6a4aA0TH8PJQpvhkLaGSIp/e38aae00318515f2a0efa0dfce24dca2/in-memory-token-storage.png|600px|Next.js]]

=== Native/Mobile app===
可以存储在OS提供的安全存储中，例如：

* 安卓中使用KeyStore
* iOS中使用KeyChain

=== SPA单页应用===
跟Web APP一样，如果SPA有对应的后端支持，token应该存储在SPA的后端，但是SPA需要通过某种机制去获取这个token；如果没有对应的后端，那么只能存储在内存中。

=== id_token可以当access_token使用么===

id_token是由授权服务器颁发给client的，比如请求一个id_token的：
&lt;pre&gt;
https://authorization-server.com/authorize?
  response_type=code
  &amp;client_id=egHuu4oJxgOLeBzPAQ9sXg4i
  &amp;redirect_uri=https://www.oauth.com/playground/oidc.html
  &amp;scope=openid+profile+email+photos
  &amp;state=sRROJ_iPTam39Dc7
  &amp;nonce=eFRvo_n5ecyYU_Sv
&lt;/pre&gt;
最后得到的id_token是这样的：

&lt;syntaxhighlight lang="json"&gt;
{
  "sub": "concerned-caracal@example.com",
  "name": "Concerned Caracal",
  "email": "concerned-caracal@example.com",
  "iss": "https://pk-demo.okta.com/oauth2/default",
  "aud": "egHuu4oJxgOLeBzPAQ9sXg4i",
  "iat": 1600674514,
  "exp": 1603266514,
  "amr": [
    "pwd"
  ]
}
&lt;/syntaxhighlight&gt;

这里`aud`即client的id，是可以被client所信任的。而id_token不是设计给resource server使用的，所以显然不能使用id_token代替access_token。


References:

* [https://auth0.com/docs/applications Applications in Auth0]
* [https://alexbilbie.com/guide-to-oauth-2-grants/ A Guide To OAuth 2.0 Grants]
* [https://openid.net/connect/faq/ OpenID Connect FAQ and Q&amp;As]
* [https://auth0.com/docs/authorization/authentication-and-authorization Authentication and Authorization]
* [https://stackoverflow.com/questions/48544500/oauth-and-authentication OAuth and authentication]
* [https://www.scottbrady91.com/OAuth/OAuth-is-Not-Authentication OAuth is Not Authentication]
* [https://openid.net/specs/openid-connect-core-1_0.html OpenID Connect Core 1.0 incorporating errata set 1]</text>
      <sha1>34onfoxyuzlgmq10vxi12subs68yoqq</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:OAuth2授权流程</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-09-17T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>最近有一个问题一直比较困惑，起因是我们有一个React的应用使用OAuth进行权限验证，而我之前的实践通常是基于Token的权限验证（即通过用户名和密码获取JWT Token），那么OAuth是否适合这样的场景呢？</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15098" sha1="k5v7rxvx28gig3q9ukm7wjzpeev3hj0">最近有一个问题一直比较困惑，起因是我们有一个React的应用使用OAuth进行权限验证，而我之前的实践通常是基于Token的权限验证（即通过用户名和密码获取JWT Token），那么OAuth是否适合这样的场景呢？

= OAuth 2.0简介=
== 什么是OAuth==

RFC6749对OAuth 2.0进行了介绍：

&gt; The OAuth 2.0 authorization framework enables a third-party
   application to obtain limited access to an HTTP service, either on
   behalf of a resource owner by orchestrating an approval interaction
   between the resource owner and the HTTP service, or by allowing the
   third-party application to obtain access on its own behalf.  This
   specification replaces and obsoletes the OAuth 1.0 protocol described
   in RFC 5849.

从这个描述我们可以得出一些结论：

* OAuth 2.0 是一个授权（而不是“认证”）的框架
* OAuth 2.0 设计是为了授权一个第三方的应用访问受限的HTTP服务资源
* OAuth 2.0 出现取代了OAuth 1.0

那么，OAuth的出现时为了解决什么问题呢？对于一个典型的客户端请求服务器（受限）资源的场景，譬如一个网络相册服务，客户端需要将用户的认证信息（通常是用户名和密码）发送给服务端，这样才能保证有且仅有这个相册的owner能够访问该相册。现在假设有一个第三方的应用，比如一个什么打印机应用来帮用户打印照片，它也需要调用相册的服务来获取照片，那么显而易见的做法就是将用户的认证信息共享给第三方。但是这样做带来了一些问题：

* 第三方应用需要存储用户的认证信息，这样通常是不安全的
* 服务器需要能够支持密码认证
* 第三方获取到的权限可能比需要要大（比如也许打印机只需要访问一张照片，但是有了用户的认证信息实际上也可以访问到其他的照片，而无法进行限制），而且无法限制其使用时长
* 如果期望取消某一个第三方应用的授权，唯一的办法是修改密码，但假设有多个第三方应用那么都会收到影响

在OAuth中，客户端通过请求一个单独的access token来访问受限的资源，而token中包含了一些关于权限的描述信息（譬如范围、时效等）。这样在上面的打印照片的例子中，打印服务不需要知道用户的用户名和密码就可以获取到用户想打印的照片。

== OAuth 中的角色==

OAuth中定义了四个角色：

* resource owner：资源所有者（通常是用户），可以授权应用访问其所有的受保护的资源
* resource server：资源服务器，可以根据access token获取受保护的资源
* client：代表resource owner及其授权、访问受保护资源的应用
* authorization server：通过对resource owner认证并获得其授权后，颁发access token的service

值得注意的是，这些角色并不要求是分开的实体，同一个server也可以拥有多个角色，例如resource server和authorization server可以是同一个服务。

对于client，又可以分为两种：

* confidential clients: 可以认证的客户端，能够（安全的）保存自身的认证信息
* public clients: 无法存储自身认证信息，例如运行在浏览器或者移动端的应用


= 授权流程=

OAuth中最初定义了4种授权方式：

* Authorization Code
* Implicit
* Resource Owner Password Credentials
* Client Credentials

后面又发布了一些新的流程和增强，有些已经过时了。

=== Authorization Code Grant===

客户端需要生成一个授权链接，包含如下参数：

* response_type: 必须为`code`
* client_id: 客户端标识
* redirect_uri(Optional): 重定向链接
* scope(Optional): 授权的scope
* state(Recommended):  用来防止跨站请求伪造

&lt;syntaxhighlight lang="lua"&gt;
https://authorization-server.com/authorize?
  response_type=code
  &amp;client_id=egHuu4oJxgOLeBzPAQ9sXg4i
  &amp;redirect_uri=https://www.oauth.com/playground/authorization-code.html
  &amp;scope=photo+offline_access
  &amp;state=hCi3i1u67XgxqbO-
&lt;/syntaxhighlight&gt;

授权服务器收到请求后，对请求参数进行检查，如果无误则对用户进行认证，并取得用户授权；授权完成后，授权服务器重定向到请求中的redirect_uri上，并附加一些参数：

* code: 授权码，必须是在短期内失效（以降低泄漏后带来的风险），建议最长不超过10分钟；客户端对其应该只使用一次，否则授权服务器将拒绝请求并应该尽可能revoke之前通过该code颁发的token。
* state: 即请求中的state值

在上面的例子中，当用户授权之后，会跳转到：

&lt;syntaxhighlight lang="lua"&gt;
https://www.oauth.com/playground/authorization-code.html?
  state=hCi3i1u67XgxqbO-
  &amp;code=7RfqR_w09Ak75fZRlFCVL1ZtKUM3RR67Wd18I9tNZQwSANx9
&lt;/syntaxhighlight&gt;

客户端必须首先验证`state`是否与用户会话中的值一致（这个值可以保存在cookie、session或者通过其他方式保存），从而防止CSRF攻击。验证无误后，客户端需要使用这个code来换取token：

&lt;syntaxhighlight lang="lua"&gt;
POST https://authorization-server.com/token

grant_type=authorization_code
&amp;client_id=egHuu4oJxgOLeBzPAQ9sXg4i
&amp;client_secret=p4NlH7i7o2JQJ9xpGdhG95eXWgX1I8teWYZo8pH5-vILSZXv
&amp;redirect_uri=https://www.oauth.com/playground/authorization-code.html
&amp;code=7RfqR_w09Ak75fZRlFCVL1ZtKUM3RR67Wd18I9tNZQwSANx9
&lt;/syntaxhighlight&gt;

其中：

* grant_type: 必须为`authorization_code`
* code: 上一步从认证服务器拿到的code
* redirect_uri: 如果拿code这一步有的话这里也必须一致
* client_id: 客户端标识，如果客户端不是public的话，需要对client进行认证(上面的例子中通过client_secret进行认证)

认证无误后，就可以换取到access_token了：

&lt;syntaxhighlight lang="json"&gt;
{
  "token_type": "Bearer",
  "expires_in": 86400,
  "access_token": "Y80stMYZlsL6p6YSwwR16UiUueaV_BtuGVVtbmAj-b2Y_5u-yKtGqq2gWL2NY6ftKNMo6hin",
  "scope": "photo offline_access",
  "refresh_token": "eA-3mBXx8G9MLDzoKbJZNyV6"
}
&lt;/syntaxhighlight&gt;

[[File:OAuth-authorization-code-flow.png|600px|Authrozation code flow]]


=== Authorization Code Grant with PKCE===

上面一种流程通常推荐跟PKCE（Proof Key for Code Exchange）一起使用来增强安全，区别如下：

* 在跳转到authorization server之前，生成一个secret code verifier（43-128位，包含[a-zA-Z0-9-._~]的随机字符串 ）和challenge（通过 $Base64UrlEncode(SHA256(CodeVerifier))$ 生成）。如果不支持SHA256的话，则跟secret code verifier一致
* challenge在第一次获取code的时候回发送给服务端，服务端会保存challenge；而后在获取access token的时候，客户端需要发送code verifier，从而服务器可以重新进行一次hash来对比

例如，

&lt;syntaxhighlight lang="lua"&gt;
code verifier = sz3-THfasVfv882QlbHeLsmBOdkEvgQXAYlce7MTeqzHG7Dk
code challenge = base64url(sha256(code_verifier)) 
               = pVx7RqTYem8RYTImvRC1M4EsoaOkeqYB6I4l5tnrPWg
&lt;/syntaxhighlight&gt;

客户端需要存储code verifier。然后在授权的URL中带上challenge参数：

&lt;syntaxhighlight lang="lua"&gt;
https://authorization-server.com/authorize?
  response_type=code
  &amp;client_id=egHuu4oJxgOLeBzPAQ9sXg4i
  &amp;redirect_uri=https://www.oauth.com/playground/authorization-code-with-pkce.html
  &amp;scope=photo+offline_access
  &amp;state=G_SbnGGJEopEPN9A
  &amp;code_challenge=pVx7RqTYem8RYTImvRC1M4EsoaOkeqYB6I4l5tnrPWg
  &amp;code_challenge_method=S256
&lt;/syntaxhighlight&gt;

同样，授权服务器会重定向到请求的redirect_uri上并带上state和code

&lt;syntaxhighlight lang="lua"&gt;
?state=G_SbnGGJEopEPN9A
&amp;code=dS6-4QKtIsX6fNBPzxo4DffXTtgufG_MLbZntG6kQwoEKXUP
&lt;/syntaxhighlight&gt;

当客户端拿这个code换取token的时候，需要带上code_verifier。

&lt;syntaxhighlight lang="lua"&gt;
POST https://authorization-server.com/token

grant_type=authorization_code
&amp;client_id=egHuu4oJxgOLeBzPAQ9sXg4i
&amp;client_secret=p4NlH7i7o2JQJ9xpGdhG95eXWgX1I8teWYZo8pH5-vILSZXv
&amp;redirect_uri=https://www.oauth.com/playground/authorization-code-with-pkce.html
&amp;code=dS6-4QKtIsX6fNBPzxo4DffXTtgufG_MLbZntG6kQwoEKXUP
&amp;code_verifier=sz3-THfasVfv882QlbHeLsmBOdkEvgQXAYlce7MTeqzHG7Dk
&lt;/syntaxhighlight&gt;

授权服务器会依照这个code_verifier与之前的challenge进行比较，从而防止有人通过某些途径拿到autorization code之后使用它（因为hash是不可逆的，除非很难通过challenge得到原始的code_verifier）。

[[File:OAuth-authorization-code-PKCE-flow.png|600px|OAuth PKCE]]

=== Legacy: Implicit Grant===

首先客户端需要生成一个授权的URL,例如：

&lt;syntaxhighlight lang="lua"&gt;
https://authorization-server.com/authorize?
  response_type=token
  &amp;client_id=egHuu4oJxgOLeBzPAQ9sXg4i
  &amp;redirect_uri=https://www.oauth.com/playground/implicit.html
  &amp;scope=photo
  &amp;state=wjtEAa38CxUJbhKE
&lt;/syntaxhighlight&gt;

其中，response_type: 必须为token，其他参数与前面的授权流程一样。不同的是，服务端重定向的时候，带的参数为access token而不是code：

&lt;syntaxhighlight lang="lua"&gt;
#access_token=cXoSzbih9UYXAZEQlN7gag4sWhvpP9J941OHOhrbXzGqlA_mzC-os3u3X4_g25I1x5epxRM_
  &amp;token_type=Bearer
  &amp;expires_in=86400
  &amp;scope=photos
  &amp;state=wjtEAa38CxUJbhKE
&lt;/syntaxhighlight&gt;

这种方式虽然简单，但是安全性是比较缺乏的，已经不被推荐使用：

&gt; It is not recommended to use the implicit flow (and some servers prohibit this flow entirely) due to the inherent risks of returning access tokens in an HTTP redirect without any confirmation that it has been received by the client.

&gt; Public clients such as native apps and JavaScript apps should now use the authorization code flow with the PKCE extension instead.

=== Legacy: Resource Owner Password Credentials （Password Grant）===

这种方式即通过用户名和密码来直接获取access token，应用需要将用户的用户名和密码发送给授权服务器来获取token，已经不推荐使用。

=== Client Credentials===

将客户端的认证信息作为获取access token的凭证，通常用于访问一些客户端自身的一些资源（而不是用户的资源）。

请求的参数为：

* grant_type: 为`client_credentials`
* scope（Optional)：请求授权的scope

例如，一个授权请求：

&lt;syntaxhighlight lang="lua"&gt;
POST /token HTTP/1.1
     Host: server.example.com
     Authorization: Basic czZCaGRSa3F0MzpnWDFmQmF0M2JW
     Content-Type: application/x-www-form-urlencoded

     grant_type=client_credentials
&lt;/syntaxhighlight&gt;

这里授权服务器必须要对客户端进行认证（上面的请求中带了客户端的认证信息），如果没有问题则返回token信息：

&lt;syntaxhighlight lang="json"&gt;
{
  "access_token":"2YotnFZFEjr1zCsicMWpAA",
  "token_type":"example",
  "expires_in":3600,
  "example_parameter":"example_value"
}
&lt;/syntaxhighlight&gt;
注意，在这个flow中是不允许包含`refresh token`在返回结果中的。

=== Refresh Token===
在前面的流程中，获取access token的同时也会拿到一个refresh token，客户端可以通过这个refresh token来重新拿到一个token。

请求参数：

* grant_type: 必须为`refresh_token`
* refresh_token: token返回中的refresh token
* scope(Optional): 请求的授权scope，必须是包含在最初拿token时请求的scope中

例子：

&lt;syntaxhighlight lang="lua"&gt;
POST /token HTTP/1.1
     Host: server.example.com
     Authorization: Basic czZCaGRSa3F0MzpnWDFmQmF0M2JW
     Content-Type: application/x-www-form-urlencoded

     grant_type=refresh_token&amp;refresh_token=tGzv3JOkF0XG5Qx2TlKWIA
&lt;/syntaxhighlight&gt;

服务端必须要对客户端进行认证，并对refresh token进行校验（确保这个refresh_token有效并且是之前签发给这个client的）。

=== Device Code===
对于无法使用浏览器或者说有输入限制的互联网设备，OAuth 提供了一个Device code的flow。在这个流程中，客户端首先请求一个device code：

&lt;syntaxhighlight lang="lua"&gt;
POST https://example.okta.com/device

client_id=https://www.oauth.com/playground/
&lt;/syntaxhighlight&gt;

然后拿到一个device code:

&lt;syntaxhighlight lang="json"&gt;
{
  "device_code": "NGU5OWFiNjQ5YmQwNGY3YTdmZTEyNzQ3YzQ1YSA",
  "user_code": "BDWD-HQPK",
  "verification_uri": "https://example.okta.com/device",
  "interval": 5,
  "expires_in": 1800
}
&lt;/syntaxhighlight&gt;

其中的device code和user code是需要展示给用户的。

然后客户端需要一直去poll,

&lt;syntaxhighlight lang="lua"&gt;
POST https://example.okta.com/token

grant_type=urn:ietf:params:oauth:grant-type:device_code
&amp;client_id=https://www.oauth.com/playground/
&amp;device_code=NGU5OWFiNjQ5YmQwNGY3YTdmZTEyNzQ3YzQ1YSA
&lt;/syntaxhighlight&gt;

当用户完成授权之后，可以得到一个token:

&lt;syntaxhighlight lang="json"&gt;
{
  "token_type": "Bearer",
  "access_token": "RsT5OjbzRn430zqMLgV3Ia",
  "expires_in": 3600,
  "refresh_token": "b7a3fac6b10e13bb3a276c2aab35e97298a060e0ede5b43ed1f720a8"
}
&lt;/syntaxhighlight&gt;


= 其他=
== access_token的验证==

当resource server拿到一个access_token的时候，是需要对其进行验证的，这再RFC7662中已经标准化，可以通过api来进行：

&lt;syntaxhighlight lang="lua"&gt;
POST /introspect HTTP/1.1
Host: server.example.com
Accept: application/json
Content-Type: application/x-www-form-urlencoded
Authorization: Bearer 23410913-abewfq.123483

token=2YotnFZFEjr1zCsicMWpAA
&lt;/syntaxhighlight&gt;

返回如下：

&lt;syntaxhighlight lang="json"&gt;
{
  "active": true,
  "client_id": "l238j323ds-23ij4",
  "username": "jdoe",
  "scope": "read write dolphin",
  "sub": "Z5O3upPC88QrAjx00dis",
  "aud": "https://protected.example.net/resource",
  "iss": "https://server.example.com/",
  "exp": 1419356238,
  "iat": 1419350238,
  "extension_field": "twenty-seven"
}
&lt;/syntaxhighlight&gt;


Ref:

* [https://tools.ietf.org/html/rfc6749 RFC6749 - The OAuth 2.0 Authorization Framework]
* [https://tools.ietf.org/html/rfc7636 RFC7636 - PKCE extension]
* [https://tools.ietf.org/html/rfc8628 RFC8628 - OAuth 2.0 Device Authorization Grant]
* [https://tools.ietf.org/html/rfc6819 RFC6819 - OAuth 2.0 Threat Model and Security Considerations]
* [https://tools.ietf.org/html/draft-ietf-oauth-jwt-bcp-07 RFC7519 - JSON Web Token Best Current Practices]
* [RFC7662 - OAuth 2.0 Token Introspection
](https://tools.ietf.org/html/rfc7662)
* [https://www.oauth.com/playground/ OAuth 2.0 Playground ]
* [https://tools.ietf.org/html/draft-ietf-oauth-security-topics-15 OAuth 2.0 Security Best Current Practice]
* [https://oauth.net/articles/authentication/#common-pitfalls Common pitfalls for authentication using OAuth]
* [https://stackoverflow.com/questions/40956418/is-oauth2-only-used-when-there-is-a-third-party-authorization is oauth2 only used when there is a third party authorization?]
* [https://medium.com/securing/what-is-going-on-with-oauth-2-0-and-why-you-should-not-use-it-for-authentication-5f47597b2611 What is going on with OAuth 2.0? And why you should not use it for authentication.]</text>
      <sha1>k5v7rxvx28gig3q9ukm7wjzpeev3hj0</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Rust(2) Ownership</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2019-11-18T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>传统的C语言需要开发人员手动管理内存，而像Java、Go这样的语言是通过垃圾回收机制自动进行内存管理。但通常垃圾回收机制本身较为复杂且需要不定期的进行（也就是说实际当内存不在需要的时候并不一定能得到及时的释放）。而rust语言采取的所有权机制（Ownership）是它区别于其他语言的一个重要特征，它被用来进行高效安全的内存管理。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9010" sha1="iswvgzblp0y7xjbybq472dr4cqqdix0">传统的C语言需要开发人员手动管理内存，而像Java、Go这样的语言是通过垃圾回收机制自动进行内存管理。但通常垃圾回收机制本身较为复杂且需要不定期的进行（也就是说实际当内存不在需要的时候并不一定能得到及时的释放）。而rust语言采取的所有权机制（Ownership）是它区别于其他语言的一个重要特征，它被用来进行高效安全的内存管理。


= Ownership概念=

在rust中，内存是由一个所有权管理系统进行管理的，它会使用一些由编译器在编译时生成的规则。这个内存管理系统的好处在于，不会像JVM stop-the-world一样暂停应用程序或者使得应用程序在运行的时候变慢。那么到底怎么样去定义ownership呢？有如下的一套规则：

* rust中每一个值都有一个变量作为与其对应的owner
* 一个值在同一时间有且仅有一个owner
* 当owner离开作用域的时候，值所在的空间会被释放(调用对象的`drop`方法)

这和C++的析构函数如出一辙，称之为*Resource Acquisition Is Initialization (RAII)*

&lt;syntaxhighlight lang="rust"&gt;

{
    let s = String::from("hello"); // s is valid from this point forward

    // do stuff with s
}                                  // 程序走到这里会调用 drop 方法释放掉内存
&lt;/syntaxhighlight&gt;

= 作用域转移(Move)=
上面说到，当对象离开作用域的时候，会调用`drop`函数来释放掉占用的内存，那么，如果遇到下面的情况呢？

&lt;syntaxhighlight lang="rust"&gt;
let s1 = String::from("hello");
let s2 = s1;

&lt;/syntaxhighlight&gt;
字符串在内存中实际上是分为了两部分，

[[File:rust_string_mm.svg|600px|Rust String copy]]

其中，左边的值存放在栈上，是固定的长度的，另外存了一个内存地址指向实际的内容，而这部分内容也就是右边的部分，是存放在堆上面的。当我们将s2赋值给s1的时候，实际上并没有进行深拷贝，也就是说堆上的数据仍旧是那个，只是将s2的指针指向了这一部分内存。那么现在存在一个问题就是，s1和s2离开作用域的时候，都会去调用`drop`释放这部分内存，这部分内存会被释放两次，显然这是不对的。为了解决这一个问题，rust中当将一个变量赋值给另一个变量的时候，会发生作用域转移，旧的对象不再有效，而释放内存这个操作，会由转移后的对象来承担这个职责。所以，一旦作用域转移后，就不能再使用这个对象了：

&lt;syntaxhighlight lang="rust"&gt;
 --&gt; test.rs:5:9
  |
5 |     let s1 = s;
  |         ^^ help: consider prefixing with an underscore: `_s1`
  |
  = note: `#[warn(unused_variables)]` on by default

error[E0382]: borrow of moved value: `s`
 --&gt; test.rs:6:20
  |
4 |     let s = String::from("hello");
  |         - move occurs because `s` has type `std::string::String`, which does not implement the `Copy` trait
5 |     let s1 = s;
  |              - value moved here
6 |     println!("{}", s);
  |                    ^ value borrowed here after move

error: aborting due to previous error
&lt;/syntaxhighlight&gt;

== 使用`clone`进行深拷贝==
因为默认就是浅拷贝，所以拷贝操作可以认为是很轻量级的，对性能没有什么影响。但如果的确需要深拷贝呢？那么应该使用`clone`方法，这跟其他语言差不多。

&lt;syntaxhighlight lang="rust"&gt;
let s = String::from("hello");
let s1 = s.clone();            // 进行深拷贝操作
println!("{}\n{}", s, s1);     // 这样s作用域并没有被转移，仍然可用
&lt;/syntaxhighlight&gt;
== 简单对象的深拷贝==

对于简单的基本类型而言，实际上拷贝之后，也并没有发生作用域转移，这点值得注意。
&lt;syntaxhighlight lang="rust"&gt;
let x = 3.14;
let y = x;                   // 没问题，拷贝之后x即失效
println!("x={} y={}", x, y);

let x = String::from("hello");
let y = x;
println!("x={} y={}", x, y); // 不可以，因为x已经invalid了
&lt;/syntaxhighlight&gt;
原因是对于这些对象的拷贝完全发生在栈上，rust认为采取上面的作用域转移的策略对它们没有任何价值，所以这样设计。

== 自定义对象的深拷贝==
实质上刚才所说的简单对象复制后没有发生作用域转移的深层原因是因为它们实现了一个特殊的接口`Copy`，rust中有这些对象实现了这个接口：

* 基本类型，包括数值类型、布尔、浮点数、字符类型
* 只包含实现了`Copy`接口的元组

对于我们自己的对象，也可以实现`Copy`接口，从而使得拷贝之后，作用域不会转移。如下：

&lt;syntaxhighlight lang="rust"&gt;
#[derive(Debug, Copy, Clone)]
struct Point {
    x: i32,
    y: i32,
}

fn main() {
    let p1 = Point { x: 10, y: 10 };
    let _p2 = p1;
    println!("p1:{:?}", p1); // 没问题，因为Point继承了Copy接口
}

&lt;/syntaxhighlight&gt;

== 方法传参和返回值也会发生作用域转移==
如同拷贝一样，将变量传递给函数同样会发生作用域转移，例如：

&lt;syntaxhighlight lang="rust"&gt;
fn output(str: String) {
    println!("=&gt;{}", str);
}

fn main() {
    let x = String::from("hello");
    output(x);
    println!("x={}", x);     // 不可以，因为x作用域已经转移了
}
&lt;/syntaxhighlight&gt;
上述的Copy规则同样适用于通过方法调用发生的作用域转移。同样，如果一个函数有返回值，那么通过返回值会更改Ownership。现在可以注意到，一旦一个变量传给了某个函数调用之后，那么这个变量就被转移了，如果我们希望多次使用这个变量，岂不是很麻烦？唯一的办法就是再将它从返回值返回回来，像这样：

&lt;syntaxhighlight lang="rust"&gt;
fn output(str: String) -&gt; String {
    println!("=&gt;{}", str);
    str
}

fn generate() -&gt; String {
    String::from("hello world!")
}

fn main() {
    let x = generate();
    let x = output(x);         // 通过返回值再把x传出来，重新获得所有权
    println!("x={}", x);       // 不可以，因为x作用域已经转移了
}
&lt;/syntaxhighlight&gt;
那么如果我们函数本身也有一个返回值怎么办？虽然我们理论上也可以通过元组的方式来实现，但是代码会变得很奇怪，所以并不是真正的解决方法。

= 引用(Reference)=
解决上述问题的一个办法就是，变量引用。如果是一个变量引用的话，那么就不会夺取该变量的所有权，如下：

&lt;syntaxhighlight lang="rust"&gt;
fn output(str: &amp;String) {
    println!("=&gt;{}", str);
}

fn main() {
    let x = generate();
    output(&amp;x);                // 传递x的引用，这样就不会夺取所有权了
    println!("x={}", x);       // 可以，因为x作用域未发生转移
}
&lt;/syntaxhighlight&gt;
是不是很像c++? &#x1f605;要创建引用也很简单，加一个`&amp;`就可以了。

&lt;syntaxhighlight lang="rust"&gt;
let x = String::from("hello world");
let y: &amp;String = &amp;x;
println!("x={} y={}", x, y);   // 没问题，y是一个引用，并不会夺取所有权
&lt;/syntaxhighlight&gt;

= 借用(Borrowing)=
引用变量作为函数的参数，称之为借用(borrowing)。所谓有借有还，再借不难，借的东西迟早都是要还回去的。如果你对借用的东西做了改变，怎么办呢？比如这样：

&lt;syntaxhighlight lang="rust"&gt;
12 | fn output(str: &amp;String) {
   |                ------- help: consider changing this to be a mutable reference: `&amp;mut std::string::String`
13 |     str.push_str("world!");
   |     ^^^ `str` is a `&amp;` reference, so the data it refers to cannot be borrowed as mutable

&lt;/syntaxhighlight&gt;
不用担心，借用的对象默认就是不可变的，所以编译器会检测出来，不允许这样操作。如果的确需要改变怎么办呢？对于这种情况，可以使用`&amp;mut`创建可变的引用，当然前提是这个变量本身也要是可变的才行，否则编译器也会报错。

&lt;syntaxhighlight lang="rust"&gt;
fn output(str: &amp;mut String) {
    str.push_str("world!");
    println!("=&gt;{}", str);
}

fn main() {
    let mut x = String::from("hello world");
    output(&amp;mut x);
}
&lt;/syntaxhighlight&gt;
可变引用有一个限制就是，在同样的作用域里面至多可以有一个变量的可变引用，这样做的好处是在编译时就避免了数据竞争。rust中有以下的限制：

* 在同一个scope中，最多有一个变量的可变引用
* 可变引用和不可变引用不能同时存在。这里决定是否同时存在的条件是，在可变引用之后的语句是否有不可变引用被使用。

&lt;syntaxhighlight lang="rust"&gt;
let mut s = String::from("hello");

let r1 = &amp;s; // no problem
let r2 = &amp;s; // no problem
println!("{} and {}", r1, r2);
// r1 and r2 are no longer used after this point

let r3 = &amp;mut s; // no problem
// 如果在这个地方之后还有使用r1和r2的地方，那么编译会报错
// println!("{} and {}", r1, r2);
println!("{}", r3);
&lt;/syntaxhighlight&gt;</text>
      <sha1>iswvgzblp0y7xjbybq472dr4cqqdix0</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Rust(1) 基本语法</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2019-10-22T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>使用rust语言编写hello world再容易不过了：
```rust
fn main() {
    println!("Hello world!");
}
```
然后利用rustc编译器编译即可:
```bash
rustc hell.rs -o hello.out &amp;&amp; ./hello.out
```</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6668" sha1="dutv84e604xml7vz3n3yovai41bda2i">使用rust语言编写hello world再容易不过了：
&lt;syntaxhighlight lang="rust"&gt;
fn main() {
    println!("Hello world!");
}
&lt;/syntaxhighlight&gt;
然后利用rustc编译器编译即可:
&lt;syntaxhighlight lang="bash"&gt;
rustc hell.rs -o hello.out &amp;&amp; ./hello.out
&lt;/syntaxhighlight&gt;
= 可变(mutable)与不可变(immutable)=
rust程序默认的变量是不可变的，类似Scala这种函数式编程的语言，鼓励用户使用immutable的变量。当然如果你非想要使用可变的对象也是支持的：

&lt;syntaxhighlight lang="rust"&gt;
let i = 32; // immutable
let mut i = 32;
&lt;/syntaxhighlight&gt;
编译器会检查是否对不可变对象重新赋值:

&lt;pre&gt;
  |
4 |     let i = 10;
  |         -
  |         |
  |         first assignment to `i`
  |         help: make this binding mutable: `mut i`
...
7 |     i = 99;
  |     ^^^^^^ cannot assign twice to immutable variable
&lt;/pre&gt;

那么，对于简单类型直接赋值会有问题，如果是复杂类型，如何呢？比如我们用一个不可变的字符串，然后去调用它的函数改变值，会发生生么情况呢？

&lt;pre&gt;
--&gt; test.rs:5:5
  |
4 |     let s = String::from("hello");
  |         - help: consider changing this to be mutable: `mut s`
5 |     s.push_str(" world!!!");
  |     ^ cannot borrow as mutable

&lt;/pre&gt;
结果表明，rust依然保持对象是不可变的。看了一下这个方法的定义，有些蹊跷：
&lt;syntaxhighlight lang="rust"&gt;
pub fn push_str(&amp;mut self, string: &amp;str) {
    self.vec.extend_from_slice(string.as_bytes())
}
&lt;/syntaxhighlight&gt;
具体怎么做到的，我们后面再来研究。

= 基本类型=
rust跟大多数编译型语言一样是静态类型(statically typed)的语言，即所有的变量的类型在程序编译的时候就是已知的。在rust语言中，有着如下的基本类型：

== 标量类型(Scalar types)==

类型          长度
------------ -------- ----------------------------------------
bool         1        true/ false
char         4        并不等同于Unicode
i8/u8        8
i16/u16      16
i32/u32      32       i32是默认类型，通常拥有最快的速度
i64/u64      64
i128/u128    128
isize/usize  arch     取决于机器架构，在32位机器上位32位，64位上位64位
f32          32       浮点数使用IEEE-754标准
f64          64

&lt;syntaxhighlight lang="rust"&gt;
let f = true;
let sum:i32 = 100;
let heart_eyed_cat = '&#x1f63b;';
&lt;/syntaxhighlight&gt;
== 复合类型(Compound types)==

复合类型分为元组（Tuple）和数组。元组可以用来将不同类型的解构组合到一起：

&lt;syntaxhighlight lang="rust"&gt;
let t: (i32, bool) = (100, false);

let (x, y) = t; // 解构元组
let x = t.0;    // 或者通过序号访问
&lt;/syntaxhighlight&gt;

数组的与元组的区别在于数组中包含的都是同一种数据类型的值。

&lt;syntaxhighlight lang="rust"&gt;
let a = [1, 2, 3];
let a: [i32; 5] = [1, 2, 3, 4, 5]; // 显示声明一个数组
let b = [10; 5];                   // 声明初始值为10、长度为5的数组
&lt;/syntaxhighlight&gt;
值得注意的是，在rust中元组和数组都是固定长度的，一旦声明以后就不可以更改。如果非要可变长度的集合，那么可以考虑使用标准库中的`vector`。并且数组中的元素也是不可以更改的，如果尝试去更改一个不可变的对象编译时会出错：

&lt;pre&gt;
6 |     let b = [100; 5];
  |         - help: consider changing this to be mutable: `mut b`
7 |     b[1] = 1024;
  |     ^^^^^^^^^^^ cannot assign
&lt;/pre&gt;
这和一些其他的语言(例如Java中的final)是有区别的。

数组中如果如果声明的长度和和实际值的长度不一样会怎样呢？rust在编译时就会出错：

&lt;pre&gt;
 --&gt; hell.rs:11:23
   |
11 |     let a: [i32; 3] = [1];
   |                       ^^^ expected an array with a fixed size of 3 elements, found one with 1 element
&lt;/pre&gt;

另外，rust程序会在运行时对数组的边界进行检查，如果越界访问数组将抛出错误而结束程序，而不是返回一个错误的内存。

= 方法=
在rust中定义一个方法使用`fn`关键字定义：

&lt;syntaxhighlight lang="rust"&gt;
fn foo(i: i32, j: i32) {
    let sum = i + j
}

// 带有返回值的方法
fn sum(i: i32, j: i32) -&gt; i32 {
    i + j
}
&lt;/syntaxhighlight&gt;

在rust中方法是第一类值，意味着你可以这样操作：
&lt;syntaxhighlight lang="rust"&gt;
let fn_s  = sum;
let s = fn_s(i, j);
&lt;/syntaxhighlight&gt;

另外，方法中包含在大括号中的语句块，被称作是表达式(expression)，可以这样用：

&lt;syntaxhighlight lang="rust"&gt;
let a = {
   e + 10
};
println!("{}", a);
&lt;/syntaxhighlight&gt;

= 流程控制=

== if语句==

rust的if语句和其他语言基本类似，稍微有一点区别：

&lt;syntaxhighlight lang="rust"&gt;
if e % 2 == 0 {        // if条件后面不用写小括号
    println!("{}", e);
} else if e % 3 == 0 { // 但是后面的语句块必须包含在大括号之中，哪怕只有一行
    println!("{} % 3 ==0", e);
} else {
    println!(":p");
}
&lt;/syntaxhighlight&gt;

== 条件赋值==

因为if语句本身是一个表达式，所以可以把if和let联合在一起来使用，也就是条件赋值：
&lt;syntaxhighlight lang="rust"&gt;
let a = if condition {
    5
} else {
    6
};
&lt;/syntaxhighlight&gt;
当然前提是不同的分支下的语句要是一样的类型，否则编译器会检测出错误。

== 循环==
rust的`loop`关键字支持创建一个循环:
&lt;syntaxhighlight lang="rust"&gt;
let mut i = 0;
loop {
    i += 1;
    println!("-&gt;{}", i);
}
&lt;/syntaxhighlight&gt;
基本上这就是一个死循环了。不知道为啥要定义这样一个奇葩的关键字。索性我们可以像其他编程语言一样`break`。值得注意的是，跟条件赋值一样，loop语句也是可以和let一起来赋值的，像下面这样：

&lt;syntaxhighlight lang="rust"&gt;
let s = loop {
    i += 1;
    println!("-&gt;{}", i);
    if(i &gt; 100) {
        break i;
    }
};
println!("s = {}", s); // s = 101
&lt;/syntaxhighlight&gt;

除了这个`loop`外，也可以“正常的”像其他语言一样，使用`while`和`for`进行条件循环：

&lt;syntaxhighlight lang="rust"&gt;
while i &lt; 1000 {    // 不用写小括号
    i += 1;
}

for e in a.iter() { // 使用for循环遍历数组
    println!("{}", e); 
}

// for i in (1..10).rev()
// 使用rev()反转顺序
for i in (1..10) {
    println!("{}", i);
}

&lt;/syntaxhighlight&gt;
= rust语言的一些惯例=

== 命名方式==

rust中推荐使用蛇形命名(snake case)来作为方法和变量的命名方式，所有的标识符都是小写且使用下划线分隔，例如：

&lt;syntaxhighlight lang="rust"&gt;
let foo_bar = 1;

fn print_info() {

}
&lt;/syntaxhighlight&gt;</text>
      <sha1>dutv84e604xml7vz3n3yovai41bda2i</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Jer语言(1)：语法设计</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2021-01-05T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>最近准备实现一个基于JVM的新语言"Jer"，一开始想先实现一个Hello world，然后逐步再朝上面添加新的功能；后来觉得还是需要先把这个语言的语法层面大致设计好再动手才行。本身是出于好玩的一个目的，但是也的确希望这个语言有一些特点，而不是单纯换一个语法而已。在这个期间思考了很多，但一直没有想到自己满意的方法，姑且先按照现在的想法设计一版出来吧。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7046" sha1="cu0lheh0dvttz5uv885ub3ujka5zxwy">最近准备实现一个基于JVM的新语言"Jer"，一开始想先实现一个Hello world，然后逐步再朝上面添加新的功能；后来觉得还是需要先把这个语言的语法层面大致设计好再动手才行。本身是出于好玩的一个目的，但是也的确希望这个语言有一些特点，而不是单纯换一个语法而已。在这个期间思考了很多，但一直没有想到自己满意的方法，姑且先按照现在的想法设计一版出来吧。


= 设计目标=
我对这个语言有这些期望：

* 基于JVM平台，即最终通过代码编译生成class字节码
* 跟Java能够兼容（可以互相调用）
* 尽量简单，应该基本的数据类型、流程控制等，支持OO，但对于一些高级特性例如泛型、lambda等就不考虑了
* 依然是强类型的语言

== hello world==
类似Java这样一个java文件只能对应一个类也许不是也个好的办法，jer源文件中可以申明任意数量的类。对于一个hello world来说，(大概）应该长这样子：

&lt;syntaxhighlight lang="rust"&gt;
// 导入其他类或者其中的静态方法
use jer/lang/System

// 没有定义在类中的方法对应到java中的静态方法
main(args: [String) = {
    msg: String = "hello world!"
    println(msg)
}

&lt;/syntaxhighlight&gt;

其中，不需要使用分号作为行的分隔符，直接换行就行了。一个Jer源文件即对应到一个Java的类，类的名称即是文件名。



= Jer 语法=

&lt;syntaxhighlight lang="antlr"&gt;
compilationUint
    : importedType* declaration* EOF
    ;
declaration
    : constantDeclaration
    | methodDeclaration
    | abstractDeclaration
    | typeDeclaration
    ;

&lt;/syntaxhighlight&gt;
每一个文件中，可以包含任意：

* 导入申明
* 常量变量申明
* 方法申明（即为静态函数）
* type或者类的申明

== 导入申明==

通过导入申明来引入其他包或者文件中定义的类。

&lt;syntaxhighlight lang="antlr"&gt;
importedType
    : USE fullPath
    ;
fullPath
    : (IDENTIFIER '/')* TYPE_NAME
    ;
&lt;/syntaxhighlight&gt;

&lt;syntaxhighlight lang="rust"&gt;
use java/lang/String
use java/util/DateTime
&lt;/syntaxhighlight&gt;

这里需要考虑一个场景就是，如果是其他Jer文件中定义了常量或者静态方法，在其他文件中如何使用？

&lt;syntaxhighlight lang="rust"&gt;
// com/riguz/jer/Util.jer
sum(a: Integer, b: Integer) -&gt; Integer = {
    // ...
}

// com/riguz/jer/Foo.jer
use com/riguz/jer/Util

main(args: [String) = {
    sum(1, 20)
}
&lt;/syntaxhighlight&gt;
== 常量定义==
常量定义跟普通的局部变量唯一的区别就是多了一个const的关键字。具体的语法在后面介绍。
&lt;syntaxhighlight lang="antlr"&gt;
constantDeclaration
    : CONST variableDeclaration
    ;
&lt;/syntaxhighlight&gt;
== 方法==

&lt;syntaxhighlight lang="antlr"&gt;
methodDeclaration
    : methodSignature methodImplementation?
    ;
methodSignature
    : IDENTIFIER '(' formalParameters? ')' functionReturnType?
    ;
formalParameters
    : formalParameter (',' formalParameter)*
    ;
functionReturnType
    : TO type
    ;
methodImplementation
    : '=' block
    ;
formalParameter
    : IDENTIFIER ':' type
    ;
&lt;/syntaxhighlight&gt;

方法分为两种，一种是有返回值的方法，另一种是没有返回值的方法（void），在定义的时候稍微有些区别：

&lt;syntaxhighlight lang="rust"&gt;
// 没有返回值的方法依靠方法的副作用
main(args: [String) = {
    msg: String = "hello world!"
    println(msg)
}

// 返回值通过箭头表示
sum(a: Integer, b: Integer) -&gt; Integer = {
    return a + b
}

&lt;/syntaxhighlight&gt;
== 抽象类和自定义类型==

&lt;syntaxhighlight lang="antlr"&gt;
abstractDeclaration
    : ABSTRACT TYPE_NAME '{' propertyDeclaration* methodSignature*'}'
    ;
typeDeclaration
    : TYPE TYPE_NAME typeAbstractions? '{' propertyDeclaration* constructorDeclaration* methodDeclaration*'}'
    ;
typeAbstractions
    : IS TYPE_NAME (',' TYPE_NAME)*
    ;
propertyDeclaration
    : IDENTIFIER ':' type
    ;
constructorDeclaration
    : '(' constructorFormalArguments? ')' methodImplementation
    ;
constructorFormalArguments
    : constructorFormalArgument (',' constructorFormalArgument)*
    ;
constructorFormalArgument
    : IDENTIFIER (':' TYPE_NAME)?
    ;
&lt;/syntaxhighlight&gt;
== 数据类型==

&lt;syntaxhighlight lang="antlr"&gt;
type
    : TYPE_NAME
    | arrayType
    ;
arrayType
    : '[' type
    ;
&lt;/syntaxhighlight&gt;
=== 基本数据类型===
数据类型与Java基本一致，对应到JVM的各个数据类型：

* Bool : JVM boolean
* Byte : JVM byte
* Short: JVM short
* Integer: JVM int
* Long: JVM long
* Float: JVM float
* Double: JVM double
* Char: JVM char
* String: java/lang/String

取消java中的primitive 类型，即所有一切都是引用类型。

=== 数组类型===

数组类型用`[&lt;Type&gt;`表示，例如`[Integer`即表示一个整数数组。


== 表达式==

&lt;syntaxhighlight lang="antlr"&gt;
expression
    : primary
    | expression bop='.'
        ( methodCall
        | IDENTIFIER
        )
    | methodCall
    | objectCreation
    ;
primary
    : '(' expression ')'
    | literal
    | IDENTIFIER
    ;
literal
    : DECIMAL_LITERAL
    | FLOAT_LITERAL
    | CHAR_LITERAL
    | STRING_LITERAL
    | BOOL_LITERAL
    | NULL_LITERAL
    ;
methodCall
    : instance=IDENTIFIER? '('methodName=IDENTIFIER methodArguments? ')'
    ;
methodArguments
    : expression (',' expression)*
    ;
objectCreation
    : NEW '(' methodArguments? ')'
    ;
&lt;/syntaxhighlight&gt;

== statement==

&lt;syntaxhighlight lang="antlr"&gt;
block
    : '{' statement* '}'
    ;
statement
    : variableDeclaration
    | embeddedStatement
    ;

embeddedStatement
    : block
    | assignment
    | expressionStatement
    | selectionStatement
    | loopStatement
    | returnStatement
    ;
assignment
    : IDENTIFIER '=' expression
    ;
selectionStatement
    : IF '(' expression ')' statement (ELSE statement)?
    ;
loopStatement
    : WHILE '(' expression ')' statement
    ;
returnStatement
    : RETURN expression
    ;
expressionStatement
    : methodCall
    ;
variableDeclaration
    : IDENTIFIER ':' type ('=' variableInitializer)?
    ;
variableInitializer
    : arrayInitializer
    | expression
    ;
arrayInitializer
    : '{' variableInitializer (',' variableInitializer)* '}'

&lt;/syntaxhighlight&gt;

= 代码示例=

&lt;syntaxhighlight lang="rust"&gt;
use java/lang/String
use java/util/DateTime

const pi: Float = 3.1415926f
const msg: String = "hello world"
const kb: Integer = 1024
const success: Boolean = true
const id: Long = 12345678

main(args: [String) = {
    (println "Hello world")
}

circleArea(radius: Float) -&gt; Float = {
    return pi(multiply 2, radius)
}

abstract Movable {
    x: Integer
    y: Integer
    move(x1: Integer, y1: Integer)
}

abstract Animal {
    name: String
    sayHelloTo(person: Person)
    address() -&gt; String
}

type Dog is Animal, Movable {
    (name) = {
        x = 0
        y = 0
    }

    sayHelloTo(person: Person) = {
        (println "Hello")
    }
}
&lt;/syntaxhighlight&gt;</text>
      <sha1>cu0lheh0dvttz5uv885ub3ujka5zxwy</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Compile OpenJDK8 on MacOSX</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2018-04-09T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>纯手工编译[OpenJDK8](http://openjdk.java.net/projects/jdk8u/)。在Mac上尝试了一下，因为编译这玩意需要XCode4*，而现在XCode都升级到9了，虽然可以下载旧版的XCode，但试了一下貌似不太兼容。于是在Virtualbox中装lubuntu来编译一下玩玩。在Virtualbox使用NAT网络做端口转发的时候，竟然发现不支持主机的22端口（貌似是[权限的问题](https://apple.stackexchange.com/questions/235518/ssh-to-virtualbox-mac-host-linux-guest-using-nat)），改为10240则Ok。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6145" sha1="29c7nlq1pg76eg4zrhfphf8zr235zjv">纯手工编译[http://openjdk.java.net/projects/jdk8u/ OpenJDK8]。在Mac上尝试了一下，因为编译这玩意需要XCode4*，而现在XCode都升级到9了，虽然可以下载旧版的XCode，但试了一下貌似不太兼容。于是在Virtualbox中装lubuntu来编译一下玩玩。在Virtualbox使用NAT网络做端口转发的时候，竟然发现不支持主机的22端口（貌似是[https://apple.stackexchange.com/questions/235518/ssh-to-virtualbox-mac-host-linux-guest-using-nat 权限的问题]），改为10240则Ok。
&lt;syntaxhighlight lang="bash"&gt;
sudo apt-get install mercurial
sudo apt-get install lrzsz
hg clone http://hg.openjdk.java.net/jdk8u/jdk8u
cd jdk8u
sh get_source.sh
&lt;/syntaxhighlight&gt;
可能遇到的问题：
&lt;pre&gt;
...
jdk:   abort: stream ended unexpectedly (got 8159 bytes, expected 29096)
...
WARNING: hotspot exited abnormally (255)
WARNING: jdk exited abnormally (255)
WARNING: nashorn exited abnormally (255)
&lt;/pre&gt;
解决办法是重试N次get_source.sh就可以了。

切换到最新的release tag：
&lt;pre&gt;
hg tags
hg up jdk8u162-b12
&lt;/pre&gt;

安装依赖项：
&lt;pre&gt;
sudo apt-get install build-essential
sudo apt-get install libasound2-dev libcups2-dev libfreetype6-dev
sudo apt-get install libx11-dev libxext-dev libxrender-dev libxtst-dev libxt-dev
sudo apt-get update
sudo apt-get upgrade

sudo add-apt-repository ppa:openjdk-r/ppa  
sudo apt-get update   
sudo apt-get install openjdk-7-jdk  
# refer https://github.com/hgomez/obuildfactory/wiki/How-to-build-and-package-OpenJDK-8-on-Linux
&lt;/pre&gt;
然后开始编译吧：
&lt;pre&gt;
bash ./configure --with-target-bits=64 --with-freetype-include=/usr/include/freetype2/ --with-freetype-lib=/usr/lib/x86_64-linux-gnu
&lt;/pre&gt;

Ubuntu16下面必须指定freetype的路径，按照OpenJDK Build README里面所说，期望的路径是```Expecting the freetype libraries under lib/ and the headers under include/. ```.而且特别指出:
&lt;pre&gt;
*The build is now a "configure &amp;&amp; make" style build
*Any GNU make 3.81 or newer should work
*The build should scale, i.e. more processors should cause the build to be done in less wall-clock time
*Nested or recursive make invocations have been significantly reduced, as has the total fork/exec or spawning of sub processes during the build
*Windows MKS usage is no longer supported
*Windows Visual Studio vsvars*.bat and vcvars*.bat files are run automatically
*Ant is no longer used when building the OpenJDK
*Use of ALT_* environment variables for configuring the build is no longer supported
&lt;/pre&gt;
因此有些文章上面设置ant, ALT_BOOTDIR等步骤不适用编译jdk8.为了提高编译速度，在虚拟机中设置了使用4个cpu核心。我们需要指定编译使用的cpu数来提高编译速度。
&lt;pre&gt;
make clean
rm -rf build
bash ./configure --with-target-bits=64 --with-freetype-include=/usr/include/freetype2/ --with-freetype-lib=/usr/lib/x86_64-linux-gnu --with-jobs=4
&lt;/pre&gt;
这样配置完后的输出如下:
&lt;pre&gt;
A new configuration has been successfully created in
/home/riguz/jdk/jdk8u/build/linux-x86_64-normal-server-release
using configure arguments '--with-target-bits=64 --with-freetype-include=/usr/include/freetype2/ --with-freetype-lib=/usr/lib/x86_64-linux-gnu --with-jobs=4'.

Configuration summary:
* Debug level:    release
* JDK variant:    normal
* JVM variants:   server
* OpenJDK target: OS: linux, CPU architecture: x86, address length: 64

Tools summary:
* Boot JDK:       java version "1.7.0_95" OpenJDK Runtime Environment (IcedTea 2.6.4) (7u95-2.6.4-3) OpenJDK 64-Bit Server VM (build 24.95-b01, mixed mode)  (at /usr/lib/jvm/java-7-openjdk-amd64)
* C Compiler:     gcc-5 (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 version 5.4.0 (at /usr/bin/gcc-5)
* C++ Compiler:   g++-5 (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 version 5.4.0 (at /usr/bin/g++-5)

Build performance summary:
* Cores to use:   4
* Memory limit:   1997 MB
* ccache status:  not installed (consider installing)
&lt;/pre&gt;
这里有个performance的提示，就是ccache。但是这玩意安装之后貌似[https://bugs.openjdk.java.net/browse/JDK-8067132 识别错误]，索性不要了。然后就可以开始编译了
&lt;pre&gt;
make images
&lt;/pre&gt;
尴尬的是虚拟机磁盘空间(10G)不够，幸亏可以动态调整一下
&lt;pre&gt;
VBoxManage modifyhd ~/VirtualBox\ VMs/lubuntu/lubuntu.vdi --resize 20480
# 完了之后需要进入到系统，用分区工具调整分区大小，可能需要删除swap分区，扩展/后再重建swap分区
sudo apt-get install gparted
gparted
&lt;/pre&gt;
images目标会```create complete j2sdk and j2re images```，花费了大约10分钟时间:
&lt;pre&gt;
----- Build times -------
Start 2018-04-11 10:03:36
End   2018-04-11 10:13:47
00:00:23 corba
00:00:15 demos
00:04:49 hotspot
00:01:02 images
00:00:15 jaxp
00:00:20 jaxws
00:02:26 jdk
00:00:28 langtools
00:00:13 nashorn
00:10:11 TOTAL
-------------------------
&lt;/pre&gt;
生成的文件在build/*/images中
&lt;pre&gt;
riguz@riguz-VirtualBox:~/jdk/jdk8u/build/linux-x86_64-normal-server-release/images/j2sdk-image$ cd bin/
riguz@riguz-VirtualBox:~/jdk/jdk8u/build/linux-x86_64-normal-server-release/images/j2sdk-image/bin$ ls
appletviewer  javadoc       jdeps       jsadebugd     pack200      servertool
extcheck      javah         jhat        jstack        policytool   tnameserv
idlj          javap         jinfo       jstat         rmic         unpack200
jar           java-rmi.cgi  jjs         jstatd        rmid         wsgen
jarsigner     jcmd          jmap        keytool       rmiregistry  wsimport
java          jconsole      jps         native2ascii  schemagen    xjc
javac         jdb           jrunscript  orbd          serialver
riguz@riguz-VirtualBox:~/jdk/jdk8u/build/linux-x86_64-normal-server-release/images/j2sdk-image/bin$ ./java -version
openjdk version "1.8.0-internal"
OpenJDK Runtime Environment (build 1.8.0-internal-riguz_2018_04_11_10_03-b00)
OpenJDK 64-Bit Server VM (build 25.71-b00, mixed mode)
&lt;/pre&gt;
最后就是测试了。测试需要安装jtreg，注意一定要指定JT_HOME.

&lt;pre&gt;
sudo apt-get install jtreg
cd test &amp;&amp; make PRODUCT_HOME=`pwd`/../build/*/images/j2sdk-image JT_HOME=/usr/bin/jtreg all
&lt;/pre&gt;</text>
      <sha1>29c7nlq1pg76eg4zrhfphf8zr235zjv</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:理解Java并发(2)：ThreadPoolExecutor解析</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2019-10-31T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>使用多线程技术可以有效的利用CPU时间，在同一个时间内完成更多的任务，但同时值得注意的是，线程创建本身也是有开销的，线程池使得我们可以重复的利用已经存在的线程，从而节省这一部分的开销，提高程序的效率。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6710" sha1="6hhz3lqg3hpvsgignpy6zx4z49reewu">使用多线程技术可以有效的利用CPU时间，在同一个时间内完成更多的任务，但同时值得注意的是，线程创建本身也是有开销的，线程池使得我们可以重复的利用已经存在的线程，从而节省这一部分的开销，提高程序的效率。


= 线程数的限制=

首先一个问题是，我们在创建新的线程的时候，是不是线程越多就越好呢？实际上是不可能无限的创建新的线程的，总会有个限制，那么问题是这个限制是多大，或者说取决于什么呢？

== 操作系统的最大线程数==
首先希望搞清楚的一个问题就是，到底我们可以创建多少个线程呢？在linux上，可以通过以下的方式查看系统的最大线程数限制：
&lt;pre&gt;
# cat /proc/sys/kernel/threads-max
15734
# ulimit -v
unlimited
&lt;/pre&gt;
据说是按照这个公式计算出来的:
&lt;syntaxhighlight lang="c"&gt;
max_threads = mempages / (8 * THREAD_SIZE / PAGE_SIZE);
&lt;/syntaxhighlight&gt;
在windows上也比较类似，总结来说就是，每个系统的最大线程数都不尽相同，不仅与系统有关还与内存大小以及用户的设置有关系。

== JVM限制==

JVM本身貌似没有对线程数进行限制，但同样不能无限制的创建线程否则会出现`java.lang.OutOfMemoryError: unable to create new native thread`。在JVM中有以下的一些参数可能会影响能创建的线程数：

* -Xms 设置堆的最小值
* -Xmx 设置堆的最大值
* -Xss 设置每个线程的stack大小

因为一个机器上的内存是一定的，所以如果`-Xss`设置的越大，单个线程所占用的栈空间越大，那么能创建的线程数就越少。一个比较有趣的事实是，能创建的最大线程数是跟`-Xmx`的值负相关的，即你设置的堆越大，反而能创建的最大线程数越少！这是别人的测试结果：

&lt;pre&gt;
2 mb --&gt; 5744 threads
4 mb --&gt; 5743 threads
...
768 mb --&gt; 3388 threads
1024 mb --&gt; 2583 threads
&lt;/pre&gt;
原因就是堆空间越大，那么机器上剩下的内存就越少，即可以用来分配给线程栈上的内存就越少，所以会出现这样的结果。

= 线程池=
在Java中线程的启动和停止是有开销的。这个开销主要包括：

* 为线程开辟栈空间（例如OpenJDK6在Linux上会使用`pthread_create`来创建线程，内部使用`mmap`分配内存)
* 通过操作系统的调用来创建和注册本地线程
* 保存线程的相关信息（JVM/native thread descriptors)到JVM中

根据网上的测试来看，通常使用线程池可以获得大幅的性能提升（亲测至少15倍）。而使用线程池相当于重用了已有的线程，避免了这部分开销。当任务越多越频繁的情况下，这部分开销越不可小觑。

== ThreadPoolExecutor==

ThreadPoolExecutor 是一个利用线程池技术实现的多任务处理器，它的申明如下：

&lt;syntaxhighlight lang="java"&gt;
public ThreadPoolExecutor(int corePoolSize,
                            int maximumPoolSize,
                            long keepAliveTime,
                            TimeUnit unit,
                            BlockingQueue&lt;Runnable&gt; workQueue) {
    this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,
            Executors.defaultThreadFactory(), defaultHandler);
}
&lt;/syntaxhighlight&gt;
乍一看有很多个参数，那么该如何去配置呢？

=== CorePoolSize / MaximumPoolSize===

线程池会根据这两个参数去管理池中的线程。当一个新的任务提交的时候，会遵循如下的规则：

* 如果池中线程数小于corePoolSize，哪怕有空闲的线程也会创建一个新的线程来
* 当吃中线程数超过corePoolSize但是小于MaximumPoolSize的时候，只有当workQueue满的时候才会创建新的线程

所以当corePoolSize和MaximumPoolSize一样的时候，实际上就是一个固定大小的线程池，相当于使用`Executors.newFixedThreadPool`:

&lt;syntaxhighlight lang="java"&gt;
public static ExecutorService newFixedThreadPool(int nThreads) {
    return new ThreadPoolExecutor(nThreads, nThreads,
                                  0L, TimeUnit.MILLISECONDS,
                                  new LinkedBlockingQueue&lt;Runnable&gt;());
}
&lt;/syntaxhighlight&gt;

池中的线程默认只要当提交了新任务的时候才会创建，如果希望提前创建线程可以使用`prestartCoreThread`或者`prestartAllCoreThreads`。

=== Keep-alive 时间===

当池中的线程数多余corePoolSize的时候，超出部分的线程会在空闲一段时间之后被终止掉，这个时间就是keepAliveTime。如果设置为0那么一旦超出部分运行结束之后就会被终止掉，反之如果设置为`Long.MAX_VALUE`那么空闲线程就会一直存活。

默认情况下，只有超出corePoolSize的线程才会受到这个存活时间的影响，如果希望对于核心线程也能超时终止，那么可以使用`allowCoreThreadTimeOut`来控制。

=== workQueue===
工作队列用来持有提交的任务。规则如下：

* 如果当前池中的线程少于corePoolSize，则创建新的线程
* 如果大于corePoolSize，则倾向于将任务加入到workQueue中
* 如果无法将任务加入到队列中，则会创建新的线程，直到池中的线程数达到maximumPoolSize
* 如果超过maximumPoolSize，那么将会拒绝提交的任务

对于队列的选择也可以使用不同的策略：

* 使用`SynchronousQueue`可以直接将任务从队列转手到线程池，这个参数要配合将maximumPoolSize设置为无限大来配合使用。因为这个朝这个队列中插入一条数据将会阻塞一直到它被消费，也就是说读写操作要配套，实际上就是进行了一个数据交换，根本没有在队列中实际存储任务。如果maximumPoolSize太小可能会导致任务被拒绝。
* 使用无界的队列例如`LinkedBlockingQueue`，那么一旦线程超过corePoolSize的时候新线程都会被加入到队列中，也就是说maximumPoolSize根本不会生效了。
* 使用有界队列例如`ArrayBlockingQueue`，超过队列数的新任务将创建新的线程。那么这时候队列大小和线程数上限需要权衡配合。

References:

* [https://stackoverflow.com/questions/344203/maximum-number-of-threads-per-process-in-linux Maximum number of threads per process in Linux?]
* [https://eknowledger.wordpress.com/2012/05/01/max-number-of-threads-per-windows-process/ Max Number of Threads Per Windows Process]
* [http://baddotrobot.com/blog/2009/02/26/less-is-more/ Less is More]
* [https://stackoverflow.com/questions/5483047/why-is-creating-a-thread-said-to-be-expensive Why is creating a Thread said to be expensive?]
       </text>
      <sha1>6hhz3lqg3hpvsgignpy6zx4z49reewu</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:阅读笔记：ConcurrentHashMap</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-03-19T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>我们知道HashMap不是Thread-safe的，而HashTable内部采取了同步操作，是线程安全的。然而有趣的是你去看HashTable的文档，它会建议你：如果不要Thread-Safe你就用HashMap吧，否则你用ConcurrentHashMap好了。

一般如果对线程安全有要求，我们有如下的一些选择：

* ConcurrentHashMap
* Hashtable
* Collections.synchronizedMap</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8433" sha1="pk0zmzuensjr0czehi5idnfhwsxi7pt">我们知道HashMap不是Thread-safe的，而HashTable内部采取了同步操作，是线程安全的。然而有趣的是你去看HashTable的文档，它会建议你：如果不要Thread-Safe你就用HashMap吧，否则你用ConcurrentHashMap好了。

一般如果对线程安全有要求，我们有如下的一些选择：

* ConcurrentHashMap
* Hashtable
* Collections.synchronizedMap


= Collections.synchronizedMap=

这个实现很粗暴，实际上就是将Map的各个操作都进行了包装和同步：

&lt;syntaxhighlight lang="java"&gt;
private static class SynchronizedMap&lt;K,V&gt;
        implements Map&lt;K,V&gt;, Serializable {
    private final Map&lt;K,V&gt; m;     // Backing Map
    final Object      mutex;  

    SynchronizedMap(Map&lt;K,V&gt; m) {
        this.m = Objects.requireNonNull(m);
        mutex = this;
    }
&lt;/syntaxhighlight&gt;

在构造函数中传入了原来的Map，以及一个对象锁（如果不传那就默认是this了）。然后，所有的操作都进行了同步处理：

&lt;syntaxhighlight lang="java"&gt;
public boolean containsValue(Object value) {
    synchronized (mutex) {return m.containsValue(value);}
}
public V get(Object key) {
    synchronized (mutex) {return m.get(key);}
}
// ...
&lt;/syntaxhighlight&gt;

= HashTable=

HashTable实现线程安全的方式与上面有些类似，对所有需要同步的地方直接进行了同步：

&lt;syntaxhighlight lang="java"&gt;
public synchronized int size() {
    return count;
}
&lt;/syntaxhighlight&gt;

那么HashMap和HashTable有什么区别呢？除了同步之外，总结下来有以下几点：

* HashMap允许一个null的key，value也可以为null；但是HashTable不允许null作为key或者value
* HashMap是JDK1.2才引入的
* HashTable是基于Dictionary接口实现的

HashTable（以及ConcurrentHashMap）都是不允许null值作为Key和Vaule的，主要的原因是因为要支持并发，假设调用`get(key)`得到了null，你是不能确认是key不存在，还是说存在但是值为null。在非并发场景下可以通过`contains(key)`来判断是否真的存在，但是在并发场景下，很可能会被其他线程修改。在JDK注释中有这样的解释：

&gt; The main reason that nulls aren't allowed in ConcurrentMaps (ConcurrentHashMaps, ConcurrentSkipListMaps) is that ambiguities that may be just barely tolerable in non-concurrent maps can't be accommodated. The main one is that if map.get(key) returns null, you can't detect whether the key explicitly maps to null vs the key isn't mapped. In a non-concurrent map, you can check this via map.contains(key), but in a concurrent one, the map might have changed between calls.

= ConcurrentHashMap=
在Java1.7和1.8中ConcurrentHashMap实现差别较大，在1.7中采用分段锁的方式实现，将Map分为许多个Segment（Segment继承自ReentrantLock）,操作的时候，只会去占用某一个Segment，而其他的Segment不会受到影响。

而在1.8中直接使用CAS+ synchronized来实现。其在内存中的结构与HashMap几乎相同了。

== get操作==

&lt;syntaxhighlight lang="java"&gt;
public V get(Object key) {
    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek;

    // 得到最终的hash值（将高位混合到低位去避免哈希冲突）
    int h = spread(key.hashCode());
    if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp;
        // (n-1) &amp; h 计算出所在的index值（与HashMap相同）
        (e = tabAt(tab, (n - 1) &amp; h)) != null) { 
        // 如果哈希值相同，则直接定位到节点，再判断是否equal即可
        if ((eh = e.hash) == h) {                
            // 这里比较key是否equal，单纯只凭hashCode是不够的。
            // 首先比较内存地址是否一致；然后再调用equals方法，是一种优化手段。
            if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))
                return e.val;
        }
        /*
            Hash值的首位被用作标记位，为负数的hash值是特殊的节点（也就是红黑树化了）
        */
        // 如果根据哈希值没有匹配到，那证明可能有哈希冲突，为负数是红黑树则在树中查找
        else if (eh &lt; 0)                         
            return (p = e.find(h, key)) != null ? p.val : null;
        // 否则是普通的链表，在链表中一直朝下找即可
        while ((e = e.next) != null) {           
            if (e.hash == h &amp;&amp;
                ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))))
                return e.val;
        }
    }
    return null;
}
&lt;/syntaxhighlight&gt;

可见get操作没有加任何的锁，而是通过将`transient volatile Node&lt;K,V&gt;[] table;`将table设置为volatile来保证可见性的。

&lt;syntaxhighlight lang="java"&gt;
transient volatile Node&lt;K,V&gt;[] table;

static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; {
    final int hash;
    final K key;
    volatile V val;
    volatile Node&lt;K,V&gt; next;
    //...
}
&lt;/syntaxhighlight&gt;

== put操作==

&lt;syntaxhighlight lang="java"&gt;
final V putVal(K key, V value, boolean onlyIfAbsent) {
    if (key == null || value == null) throw new NullPointerException();
    int hash = spread(key.hashCode());
    int binCount = 0;
    for (Node&lt;K,V&gt;[] tab = table;;) {
        Node&lt;K,V&gt; f; int n, i, fh;
        // 因为是懒加载，第一次插入的时候可能需要初始化
        if (tab == null || (n = tab.length) == 0)
            tab = initTable();
        // 没有找到（节点之前不存在）
        else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) {
            // 尝试CAS插入节点到空桶中，如果失败，则会重新走上面的流程进来
            if (casTabAt(tab, i, null,
                         new Node&lt;K,V&gt;(hash, key, value, null)))
                break;                   // no lock when adding to empty bin
        }
        // 如果（后面的流程）插入到了红黑树中，会导致首节点改变，所以这个地方需要帮忙更改过来
        else if ((fh = f.hash) == MOVED)
            tab = helpTransfer(tab, f);
        else {
            V oldVal = null;
            // f为当前定位到的桶中的第一个节点，将其同步进行后续操作
            synchronized (f) {
                // 看看同步之前当前节点是否已经被更改了；如果是则需要重新开始轮回
                if (tabAt(tab, i) == f) {
                    // 普通链表
                    if (fh &gt;= 0) {
                        binCount = 1;
                        for (Node&lt;K,V&gt; e = f;; ++binCount) {
                            K ek;
                            // 如果已经存在值
                            if (e.hash == hash &amp;&amp;
                                ((ek = e.key) == key ||
                                 (ek != null &amp;&amp; key.equals(ek)))) {
                                oldVal = e.val;
                                if (!onlyIfAbsent)
                                    e.val = value;
                                break;
                            }
                            // 不存在则新增一个节点，插入到链表尾部
                            Node&lt;K,V&gt; pred = e;
                            if ((e = e.next) == null) {
                                pred.next = new Node&lt;K,V&gt;(hash, key,
                                                          value, null);
                                break;
                            }
                        }
                    }
                    // 按红黑树处理
                    else if (f instanceof TreeBin) {
                        Node&lt;K,V&gt; p;
                        binCount = 2;
                        if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key,
                                                       value)) != null) {
                            oldVal = p.val;
                            if (!onlyIfAbsent)
                                p.val = value;
                        }
                    }
                }
            }
            if (binCount != 0) {
                if (binCount &gt;= TREEIFY_THRESHOLD)
                    treeifyBin(tab, i);
                if (oldVal != null)
                    return oldVal;
                break;
            }
        }
    }
    addCount(1L, binCount);
    return null;
}
&lt;/syntaxhighlight&gt;

* [https://www.javatpoint.com/difference-between-hashmap-and-hashtable Difference between HashMap and Hashtable]</text>
      <sha1>pk0zmzuensjr0czehi5idnfhwsxi7pt</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Linux I/O模型与Java NIO</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-08-26T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>I/O 即Input与Output，包含了文件的读写或者是网络的I/O。在Linux/Unix中有五种I/O模型：

* blocking I/O
* nonblocking I/O
* I/O multiplexing (select and poll)
* signal driven I/O (SIGIO)
* asynchronous I/O (the POSIX aio_functions)

Java 从Java SE 1.4开始引入NIO，在Java 7推出了NIO 2。那么，不同的IO模型之间具体有什么差异，又该如何使用呢？</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9249" sha1="m5z1fy7mq7ywaabnnsr566wfb3l0h03">I/O 即Input与Output，包含了文件的读写或者是网络的I/O。在Linux/Unix中有五种I/O模型：

* blocking I/O
* nonblocking I/O
* I/O multiplexing (select and poll)
* signal driven I/O (SIGIO)
* asynchronous I/O (the POSIX aio_functions)

Java 从Java SE 1.4开始引入NIO，在Java 7推出了NIO 2。那么，不同的IO模型之间具体有什么差异，又该如何使用呢？

= I/O 模型=

在I/O操作中，通常是分为两个阶段的：

* 首先是等待数据就绪。
* 然后将数据从kernel复制到process

例如在socket中，第一个阶段就是等待从网络发送数据过来，然后存入内核的缓冲区。然后第二个阶段，将接收到的数据从内核缓冲区拷贝到用户程序中。

== 五种I/O模型==
=== Blocking I/O===

Blocking I/O即阻塞式IO，在Linux中默认所有的socket都是blocking的。在这种模式下，两个阶段都是阻塞的。这个过程类似这样：

&lt;syntaxhighlight lang="c++"&gt;
data = recvfrom(socket)
&lt;/syntaxhighlight&gt;

=== Non-blocking I/O===

将socket设置为non-blocking之后，如果数据没有就绪的时候不会阻塞住请求进程而是立即返回一个错误(`EWOULDBLOCK`)，这样请求进程可以不断尝试去获取是否有数据就绪（这个过程称之为***polling***)。然而，在数据的拷贝阶段，这个过程还是blocking的。

&lt;syntaxhighlight lang="c++"&gt;
do {
    data = recvfrom(socket)
} while(data == EWOULDBLOCK)
&lt;/syntaxhighlight&gt;

=== I/O multiplexing===

I/O multiplexing(多路复用)是通过单个进程管理多个网络连接的一种方式，通常有`select`，`pool`和`epoll`等几种方式。在这种模式下，socket会被设置为non-blocking，通过不断轮询所有的socket，直到某个socket有数据则返回。

&lt;syntaxhighlight lang="c++"&gt;
while(true) {
    socket = select(sockets) // 这里如果没有一个socket是就绪的就会一直阻塞
    data = recvfrom(socket)  // 同样从内核拷贝数据到process的时候也是block的
}
&lt;/syntaxhighlight&gt;

一个更具体的例子：

&lt;syntaxhighlight lang="c++"&gt;
while(1){
    FD_ZERO(&amp;rset);
    for (i = 0; i&lt; 5; i++ ) {
        FD_SET(fds[i],&amp;rset);
    }

    puts("round again");
    select(max+1, &amp;rset, NULL, NULL, NULL);

    for(i=0;i&lt;5;i++) {
        if (FD_ISSET(fds[i], &amp;rset)){
            memset(buffer,0,MAXBUF);
            read(fds[i], buffer, MAXBUF);
            puts(buffer);
        }
    }	
}
&lt;/syntaxhighlight&gt;

除了使用`select`之外，还可以使用`pool`和`epoll`，但是本质上两个阶段都会block。看起来除了可以处理多个socket连接之外没啥好处，但是如果考虑到使用多线程的话，那么`recvfrom`可以在线程中处理，理论上可以提高吞吐。

=== Signal driven I/O===
这种模式下首先将socket设置为singal-driven，然后通过`sigaction`注册一个回调。这个过程不是block的，一旦数据ready之后，一个`SIGIO`的信号会发送到process中，然后拷贝数据阶段依然是blocking的。

&lt;syntaxhighlight lang="c++"&gt;
handler = () -&gt; {
    recvfrom(socket)
}
sigaction(socket, handler)
&lt;/syntaxhighlight&gt;

=== Asynchronous I/O===

在AIO模式下，两个阶段都是nonblocking的，跟signal-driven I/O模式的区别在于，前者是当数据ready之后通知应用去读取；而AIO是内核直接将数据拷贝到process完成之后通知process。

== 同步于异步、阻塞与非阻塞==

同步异步、阻塞和非阻塞比较confusing, POSIX中是这样定义的：

* 同步I/O是指请求的进程被阻塞一直到操作结束
* 异步I/O不导致请求进程阻塞

根据这个定义，除了AIO之外，其他四种都是synchronous的，因为数据复制阶段（recvfrom)是阻塞的。

[[File:http://www.masterraghu.com/subjects/np/introduction/unix_network_programming_v1.3/files/06fig06.gif|600px|I/O Models]]

= Java中的I/O=
== blocking I/O==
在Java中构建一个简单的Socket服务器，为每一个连接新建一个线程处理：

&lt;syntaxhighlight lang="java"&gt;
public class EchoServer {
    public static void main(String[] args) throws IOException {
        ServerSocket server = new ServerSocket();
        server.bind(new InetSocketAddress(9000));

        while (true) {
            Socket socket = server.accept();
            new Thread(clientHandler(socket)).start();
        }
    }

    private static Runnable clientHandler(Socket socket) {
        return () -&gt; {
            try {
                BufferedReader reader = new BufferedReader(new InputStreamReader(socket.getInputStream()));
                PrintWriter writer = new PrintWriter(new OutputStreamWriter(socket.getOutputStream()));

                String line = "";
                while (!"/quit".equals(line)) {
                    line = reader.readLine();
                    writer.write(line + "\n");
                    writer.flush();
                }
            } catch (IOException ex) {
                ex.printStackTrace();
            }
        };
    }
}
&lt;/syntaxhighlight&gt;

== non-blocking I/O==

Java NIO中主要有以下的一些类：

* [https://docs.oracle.com/javase/8/docs/api/java/nio/package-summary.html#buffers Buffers]: 数据缓冲容器
* [https://docs.oracle.com/javase/8/docs/api/java/nio/charset/package-summary.html Charsets]: 字符集编码和解码
* [https://docs.oracle.com/javase/8/docs/api/java/nio/channels/package-summary.html Channels]: 可以进行I/O操作的连接
* Selectors, selection keys: 用来实现multiplexed, non-blocking I/O机制

其中，`Buffer`中可以存储固定大小的容器，而其中的`ByteBuffer`类比较特殊：

* 可以作为I/O操作的目标
* 可以分配为direct buffer，JVM会尝试进行原生的I/O操作以提高性能
* 可以直接map文件的一部分到buffer中(`MappedByteBuffer`)，支持一些额外的文件操作
* 可以自定义字节序

使用Nio实现一个EchoServer:

&lt;syntaxhighlight lang="java"&gt;
public class NioEchoServer {
    public static void main(String[] args) {
        try (Selector selector = Selector.open();
             ServerSocketChannel serverSocket = ServerSocketChannel.open()) {
            serverSocket.bind(new InetSocketAddress(9999));
            serverSocket.configureBlocking(false);
            serverSocket.register(selector, SelectionKey.OP_ACCEPT);
            ByteBuffer buffer = ByteBuffer.allocate(4);

            while (true) {
                selector.select();
                final Set&lt;SelectionKey&gt; selectedKeys = selector.selectedKeys();
                Iterator&lt;SelectionKey&gt; iterator = selectedKeys.iterator();
                while (iterator.hasNext()) {
                    final SelectionKey key = iterator.next();
                    if (key.isAcceptable()) {
                        accept(selector, serverSocket);
                    } else if (key.isReadable()) {
                        readAndAnswer(buffer, key);
                    } else {
                        throw new RuntimeException("Unsupported operation");
                    }
                    iterator.remove();
                }
            }

        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    private static void accept(final Selector selector,
                               final ServerSocketChannel serverSocket)
            throws IOException {
        SocketChannel client = serverSocket.accept();
        client.configureBlocking(false);
        client.register(selector, SelectionKey.OP_READ);
    }

    private static void readAndAnswer(final ByteBuffer buffer,
                                      final SelectionKey key)
            throws IOException {
        final SocketChannel client = (SocketChannel) key.channel();
        client.read(buffer);
        buffer.flip();
        String s = StandardCharsets.UTF_8.decode(buffer).toString();
        System.out.println("-&gt; " + s);
        buffer.clear();
    }
}
&lt;/syntaxhighlight&gt;

== Asynchronous I/O==

Java支持AIO，具体有这些类：

* `AsynchronousFileChannel`: 用于文件异步读写；
* `AsynchronousSocketChannel`: 客户端异步socket；
* `AsynchronousServerSocketChannel`: 服务器异步socket。

但性能上可能并没有太大的提升（Linux平台），以致于Netty中移除了对NIO.2的支持：

&gt; I don't think NIO.2 will have better performance than NIO, because NIO.2 still make use of select/poll system calls and thread pools to simulate asynchronous IO. One example is that Netty removed NIO.2 support in 4.0.0, because the author think that NIO.2 doesn't bring better performance than NIO in Linux platform.


See also:

* [http://www.masterraghu.com/subjects/np/introduction/unix_network_programming_v1.3/ch06lev1sec2.html 6.2 I/O Models]
* [https://devarea.com/linux-io-multiplexing-select-vs-poll-vs-epoll/#.X0W520lS-L8 Linux – IO Multiplexing – Select vs Poll vs Epoll]
* [https://medium.com/@clu1022/%E6%B7%BA%E8%AB%87i-o-model-32da09c619e6 淺談I/O Model]
* [https://www.baeldung.com/java-nio-selector Introduction to the Java NIO Selector]
* [https://stackoverflow.com/questions/27541283/io-performance-selector-nio-vs-asynchronouschannelnio-2 IO performance: Selector (NIO) vs AsynchronousChannel(NIO.2)]</text>
      <sha1>m5z1fy7mq7ywaabnnsr566wfb3l0h03</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Java中的时间</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2019-11-07T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>你如果以为，Java中谈到时间仅仅就意味着`java.util.Date`那就大错特错了，Java中的时间其实可以说五花八门，Java8发布后又增加了一些新的用来表示日期和时间的类，那么我们在构建应用程序的时候到底应该用哪个类来呢？彼此之间又有什么区别？</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3803" sha1="fmg2zend98ak2v9mu6q0xksrgd12lci">你如果以为，Java中谈到时间仅仅就意味着`java.util.Date`那就大错特错了，Java中的时间其实可以说五花八门，Java8发布后又增加了一些新的用来表示日期和时间的类，那么我们在构建应用程序的时候到底应该用哪个类来呢？彼此之间又有什么区别？


= 关于时间的表示=
通常由于文化和地区的不同，世界上各个地方的人们对于时间的表达方式都不尽相同，比如在中国以前用农历和十二时辰来表示，而在西方是二十四小时制（巧合的是正好一个时辰能对应上两个小时，据称与”高合成数“有关）。那么在计算机领域，在表示时间的时候也有不同的表示方法，比较常见的有：

* 通过计算当前时间到Jan 01 1970(Unix Epoch)这一天（准确的说是00：00）经过的秒数来表示，例如 `1573090869`。
* 比较直接的方法就是直接以时分秒的形式表示当地时间，同时将当地的时区加进去，例如 `2001-07-04T12:08:56.235-07:00`

= Java中的时间类=
抛开时间戳不谈，在Java中专门用来表示时间的其他类有：

Class                     Since       Description
------------------------- ----------- -----------------------
java.util.Date            JDK1.0      日期+时间
java.util.Calendar        JDK1.1
java.sql.Date                         只包含日期
java.sql.Time                         只包含时间
java.sql.Timestamp
java.time.Instant         JDK1.8     
java.time.LocalTime       JDK1.8
java.time.LocalDate       JDK1.8
java.time.OffsetTime      JDK1.8
java.time.OffsetDateTime  JDK1.8
java.time.ZonedDateTime   JDK1.8

在Java8之前，我们通常用`java.util.Date`来表示时间，虽然没啥需求实现不了的，但有着以下的问题：

* 时间类不统一，`java.util`和`java.sql`包中都有关于时间的类，而时间格式化的又在`java.text`包中，有点乱的很
* 所有时间的类都是mutable的，非线程安全

所以Java8开始对时间进行了修改，使用起来将更加方便。通常来讲，对于需要处理时区问题的系统，`OffsetDateTime`是一个较好的选择，即包含了时间信息，又包含了时区的信息，可以得到准确的时间表述。官方文档中也建议使用：

&gt;It is intended that ZonedDateTime or Instant is used to model data in simpler applications. This class may be used when modeling date-time concepts in more detail, or when communicating to a database or in a network protocol.

在处理类似时间转换的时候，可以借助`ZonedDateTime`来实现。例如有一个`yyyyMMddHHmmss`格式的时间，但该时间是`CST`时间，这个时间有如下的特点：

* 在夏季时间相当于utc+5
* 在冬季相当于utc+6

而每年会有一个时间点去切换这个夏季时间和冬季时间（称之为day-saving)，而且不是一个固定的日期（大致相当于按照第几个星期几来算的），要想把这个时间转换为标准的时间描述，可以采取这样的方式:

&lt;syntaxhighlight lang="java"&gt;
LocalDateTime localDateTime = LocalDateTime.parse(
    "20191010095425", DateTimeFormatter.ofPattern("yyyyMMddHHmmss"));
ZonedDateTime zonedDateTime = ZonedDateTime.of(
    localDateTime, TimeZone.getTimeZone("CST"));
OffsetDateTime result = zonedDateTime
    .toOffsetDateTime()
    .withOffsetSameInstant(ZoneOffset.UTC);
&lt;/syntaxhighlight&gt;

* [https://stackoverflow.com/questions/30234594/whats-the-difference-between-java-8-zoneddatetime-and-offsetdatetime What's the difference between java 8 ZonedDateTime and OffsetDateTime?]
* [https://docs.oracle.com/javase/8/docs/api/java/time/OffsetDateTime.html OffsetDateTime]
* [https://docs.oracle.com/javase/8/docs/api/java/time/ZonedDateTime.html ZonedDateTime]</text>
      <sha1>fmg2zend98ak2v9mu6q0xksrgd12lci</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:理解Java并发(5)：AQS</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-06-01T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>AQS(AbstractQueuedSynchronizer)是一个机遇FIFO等待队列实现锁的框架，用来实现诸如ReentrantLock、Semaphore等。

```java
public abstract class AbstractQueuedSynchronizer
extends AbstractOwnableSynchronizer
implements Serializable
```</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5614" sha1="13jpi0hg1r8lrvwivqqch7dibejog5d">AQS(AbstractQueuedSynchronizer)是一个机遇FIFO等待队列实现锁的框架，用来实现诸如ReentrantLock、Semaphore等。

&lt;syntaxhighlight lang="java"&gt;
public abstract class AbstractQueuedSynchronizer
extends AbstractOwnableSynchronizer
implements Serializable
&lt;/syntaxhighlight&gt;

= 基本用法=

AQS在类里面维护了一个原子int类型的状态值（这个值的具体含义由子类去定义）。要使用AQS，推荐的做法是在在一个私有的内部类中去实现AQS，然后需要通过AQS本身提供的几个方法：

* `getState()`
* `setState(int)`
* `compareAndSetState(int, int)`

通过调用上述三个方法来维护同步状态，并实现这些方法：

* `tryAcquire(int)`：尝试获取状态
* `tryRelease(int)`：尝试释放状态
* `tryAcquireShared(int)`
* `tryReleaseShared(int)`
* `isHeldExclusively() `: 判断当前线程是否持有排它锁

而其他的同步操作、队列管理等，在AQS中已经完成了。

== 样例：实现简单的锁==

在AQS的文档中给了一个基本的用法:实现一个不可重入的锁。首先需要的就是实现内部的类：
&lt;syntaxhighlight lang="java"&gt;
class Mutex implements Lock, java.io.Serializable {
   private static class Sync extends AbstractQueuedSynchronizer {
     protected boolean isHeldExclusively() {
       return getState() == 1;
     }

     public boolean tryAcquire(int acquires) {
       assert acquires == 1; // Otherwise unused
       if (compareAndSetState(0, 1)) {
         setExclusiveOwnerThread(Thread.currentThread());
         return true;
       }
       return false;
     }

     protected boolean tryRelease(int releases) {
       assert releases == 1; // Otherwise unused
       if (getState() == 0) throw new IllegalMonitorStateException();
       setExclusiveOwnerThread(null);
       setState(0);
       return true;
     }
   }
   // ...
}
&lt;/syntaxhighlight&gt;
然后，加锁、解锁等操作都可以通过这个内部类来完成了：
&lt;syntaxhighlight lang="java"&gt;
class Mutex implements Lock, java.io.Serializable {
   // ...
   private final Sync sync = new Sync();

   // 这里通过获取或者释放状态1（1表示锁定）来实现加锁和解锁的操作
   public void lock()                { sync.acquire(1); }
   public boolean tryLock()          { return sync.tryAcquire(1); }
   public void unlock()              { sync.release(1); }
   public Condition newCondition()   { return sync.newCondition(); }
   public boolean isLocked()         { return sync.isHeldExclusively(); }
   public boolean hasQueuedThreads() { return sync.hasQueuedThreads(); }
   public void lockInterruptibly() throws InterruptedException {
     sync.acquireInterruptibly(1);
   }
   public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException {
     return sync.tryAcquireNanos(1, unit.toNanos(timeout));
   }
 }
&lt;/syntaxhighlight&gt;
= 原理=

AQS内部是使用的基于CLH队列的同步机制。

[[File:AQS_queue.png|600px|AQS CLH]]

== acquire获取状态==

acquire意味着尝试通过获取某个状态从而获取到锁。acquire的过程如下：

* 首先尝试直接通过CAS的方式改变state，如果成功则直接获取到锁
* 如果上一步失败，那么表明其他线程获取到锁，则尝试将当前线程加入到队列末尾进行排队（同样加入到队列末尾也是通过CAS实现）
* 加入到队列后，中断当前线程（但具体线程如何处理中断要看线程自己了）

&lt;syntaxhighlight lang="java"&gt;
public final void acquire(int arg) {
    if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg))
        selfInterrupt();
}
&lt;/syntaxhighlight&gt;

&lt;syntaxhighlight lang="java"&gt;
private Node addWaiter(Node mode) {
    Node node = new Node(Thread.currentThread(), mode);
    Node pred = tail;
    // 如果发现队列不为空，那么尝试一次性快速插入到尾部（如果失败的话则通过enq方法插入）
    // 在没有线程竞争的情况下，会比enq稍微快一点，enq里面还要处理队列为空的情况
    if (pred != null) {
        node.prev = pred;
        if (compareAndSetTail(pred, node)) {
            // 通过CAS设置tail成功，这时候tail已经是当前node，再把之前的tail（pred）连接到自己
            pred.next = node;
            return node;
        }
    }
    // enq的逻辑基本上与上面一样，区别在于1. 处理队列为空的情况，要插入到队列头部；2.CAS失败后会重试直到成功
    enq(node);
    return node;
}
&lt;/syntaxhighlight&gt;

&lt;syntaxhighlight lang="java"&gt;
private Node enq(final Node node) {
    for (;;) {
        Node t = tail;
        if (t == null) {
            // 如果发现队列为空那么首先把head和tail都设置为空节点
            if (compareAndSetHead(new Node()))
                tail = head;
        } else {
            node.prev = t;
            if (compareAndSetTail(t, node)) {
                t.next = node;
                return t;
            }
        }
    }
}
&lt;/syntaxhighlight&gt;

== release操作==

当释放锁的时候，会唤醒一个后继节点，这个节点通常是后一个节点（如果后一个节点cancel了则要从队列尾部遍历直到找到真正的后继节点）。

&lt;syntaxhighlight lang="java"&gt;
public final boolean release(int arg) {
    if (tryRelease(arg)) {
        Node h = head;
        if (h != null &amp;&amp; h.waitStatus != 0)
            unparkSuccessor(h);
        return true;
    }
    return false;
}

&lt;/syntaxhighlight&gt;


* [http://gee.cs.oswego.edu/dl/papers/aqs.pdf The java.util.concurrent Synchronizer Framework]</text>
      <sha1>13jpi0hg1r8lrvwivqqch7dibejog5d</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:使用TDD开发SpringBoot应用</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2019-09-04T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>虽然觉得TDD没什么卵用，但实际工作中还是必须要使用TDD，这不最近就做了一个使用TDD的方式开发SpringBoot的例子。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6111" sha1="rw0j3rgpwp83kammrduq7nyiqr2o4yo">虽然觉得TDD没什么卵用，但实际工作中还是必须要使用TDD，这不最近就做了一个使用TDD的方式开发SpringBoot的例子。

下面阐述一下如何开发一个RestFul的GET请求，从数据库中读取数据并返回。

= 创建Integration Test=
第一步可以从IntegrationTest开始，即模拟真实发送一个HTTP请求，然后检验返回的Response。简单起见，第一步只校验状态值：

&lt;syntaxhighlight lang="java"&gt;
@RunWith(SpringRunner.class)
@SpringBootTest(webEnvironment = WebEnvironment.RANDOM_PORT)
public class IntegrationTest {

  @Autowired
  private TestRestTemplate testClient;

  @Test
  public void should_get_computer_list_when_call_list_computer_api() {
    ResponseEntity&lt;List&lt;ComputerDto&gt;&gt; response = testClient.exchange(
        "/computers",
        HttpMethod.GET,
        null,
        new ParameterizedTypeReference&lt;List&lt;ComputerDto&gt;&gt;() {
        });
    assertEquals(HttpStatus.OK, response.getStatusCode());
  }
}
&lt;/syntaxhighlight&gt;
这里特意使用WebEnvironment.RANDOM_PORT以使得Spring启动一个接近真实的Server，来测试我们的请求。当然这个测试会挂了，因为Controller都还没写呢。所以下一个先来创建Controller，但是呢，TDD通常从测试开始写起，所以来测试Controller吧。

= Controller Test=

测试Controller就是单元测试了，不需要测试其他的组件（比如service什么的)。
&lt;syntaxhighlight lang="java"&gt;
@RunWith(SpringRunner.class)
@WebMvcTest(controllers = ComputerController.class)
public class ComputerControllerTest {

  @Autowired
  private MockMvc mockMvc;

  @Test
  public void should_get_a_list_when_get_computers() throws Exception {
    mockMvc.perform(MockMvcRequestBuilders.get("/computers"))
        .andExpect(status().isOk());
  }
}
&lt;/syntaxhighlight&gt;
然后就是需要创建一个Controller，让测试可以过：

&lt;syntaxhighlight lang="java"&gt;
@RestController("/computers")
public class ComputerController {

  @GetMapping
  public List&lt;ComputerDto&gt; getComputers() {
    return null;
  }
}
&lt;/syntaxhighlight&gt;
到这里基本上集成测试也可以过了。所以你可以先commit一次了。然后，当然我们不能把逻辑放到Controller里面啊，我们需要一个Service来处理业务逻辑。这个Service又会从数据库中读取数据。在用到Repository之前，我们可能需要先改一下我们的controller测试，因为到目前为止并没有校验实际的字段，只是校验了返回状态码，现在可以开始校验了：

&lt;syntaxhighlight lang="java"&gt;
  @MockBean
  private ComputerService computerService;

  @Test
  public void should_get_a_list_when_get_computers() throws Exception {
    given(computerService.getComputers())
        .willReturn(
            Collections.singletonList(
                new ComputerDto(1, "MacBook 2015", "Haifeng Li", "2019-09-10")
            ));
    mockMvc.perform(MockMvcRequestBuilders.get("/computers"))
        .andExpect(status().isOk())
        .andExpect(jsonPath("$", hasSize(1)))
        .andExpect(jsonPath("$[0].id").value(1))
        .andExpect(jsonPath("$[0].type").value("MacBook 2015"))
        .andExpect(jsonPath("$[0].owner").value("Haifeng Li"))
        .andExpect(jsonPath("$[0].createTime").value("2019-09-10"))
        .andDo(print());
  }
&lt;/syntaxhighlight&gt;

这里我们把Service给Mock掉，因此可以控制它的行为，实际的Service就一个空函数就可以了。

= Service=
这时候，可以考虑实现Service了，因为Service需要读取数据库，所以Service需要引入一个Repository来查询数据库，我们可以Mock掉Repository，来测service的逻辑：

&lt;syntaxhighlight lang="java"&gt;
@RunWith(MockitoJUnitRunner.class)
public class ComputerServiceTest {

  @Mock
  private ComputerRepository computerRepository;

  @InjectMocks
  private ComputerService computerService;

  @Test
  public void should_return_computer_list_when_get_all_computers() throws ParseException {
    ComputerEntity stored = new ComputerEntity(1,
        "MacBook 2015",
        "Haifeng Li",
        new SimpleDateFormat("dd/MM/yyyy").parse("01/09/2019"));
    given(computerRepository.findAll()).willReturn(Collections.singletonList(stored));

    List&lt;ComputerDto&gt; computers = computerService.getComputers();

    assertEquals(1, computers.size());
    assertEquals(1, computers.get(0).getId());
    assertEquals("MacBook 2015", computers.get(0).getType());
    assertEquals("Haifeng Li", computers.get(0).getOwner());
    assertEquals("2019-09-01", computers.get(0).getCreateTime());
  }
}
&lt;/syntaxhighlight&gt;
同样Repository里面也就一个空函数就行了，但是这时候得把Service 的逻辑写完，让测试可以通过，这样Service的任务就完成了，其他测试也全部都可以通过。

= Repository=

最后一步就是来实现Repository了，这里需要使用DataJpaTest，用内存数据库进行测试：

&lt;syntaxhighlight lang="java"&gt;
@RunWith(SpringRunner.class)
@DataJpaTest
public class ComputerRepositoryTest {

  @Autowired
  private TestEntityManager entityManager;

  @Autowired
  private ComputerRepository computerRepository;

  @Before
  public void prepareData() throws ParseException {
    entityManager.persistAndFlush(new ComputerEntity(1,
        "MacBook 2015",
        "Haifeng Li",
        new SimpleDateFormat("dd/MM/yyyy").parse("01/09/2019"))
    );
    entityManager.persistAndFlush(new ComputerEntity(2,
        "Desktop",
        null,
        new SimpleDateFormat("dd/MM/yyyy").parse("02/09/2019"))
    );
  }

  @Test
  public void should_return_all_records_in_db_when_find_all() {
    List&lt;ComputerEntity&gt; entities = computerRepository.findAll();
    assertEquals(2, entities.size());
  }
}
&lt;/syntaxhighlight&gt;
因为Spring JPA你只需要写一堆interface，测试这里的逻辑还是十分有必要的。所以到这一步为止，基本上程序的功能就已经实现了，唯一需要做的就是改一下配置来连接到真是的数据库。整个代码可以在我的[https://github.com/soleverlee/computer-inventory.git Github]上面找到。</text>
      <sha1>rw0j3rgpwp83kammrduq7nyiqr2o4yo</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:使用 Antlr 解析配置文件</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2018-05-09T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>在纠结了一阵子 yml,ini,xml甚至 lua 等等 配置文件的格式后，还是决定使用antlr实现了一种我自定义的格式的解析。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5263" sha1="cm33d5nlni1q3nc75cpei1jd3isqsyg">在纠结了一阵子 yml,ini,xml甚至 lua 等等 配置文件的格式后，还是决定使用antlr实现了一种我自定义的格式的解析。
这个格式是这个样子的:
&lt;syntaxhighlight lang="lua"&gt;
// Here is some comment
shared {
    string _baseUrl = "http://localhost:8080";
    string domain   = "riguz.com";
    bool ssl        = false;
    int version     = 19;
    int subVersion  = 25;
    float number  = 19.25;

    string urls         = ["http://localhost:8080", "http://riguz.com:8080"];
    string domains      = ["riguz.com", "dr.riguz.com"];
    bool sslArray       = [true, false];
    int versionArray    = [19, 25];
    float numberArray   = [18.01, 19.25, 20.23];
};

scope dev_db {
    string url = ${domain} .. ":3306/mysql";
    string user = "lihaifeng";
    int connections = 10;
    string password = "iikjouioqueyjkajkqq==";
    string domains = ${domains};
};
&lt;/syntaxhighlight&gt;
其实是一个k-v形式的文本文件，支持的基本类型有：字符串、布尔值、整数、小数、数组。定义的方法类似于Java或者C语言，
&lt;pre&gt;
string _baseUrl = "http://localhost:8080"
&lt;/pre&gt;
前面会限定数据类型。如果要定义数组，则用
&lt;pre&gt;
bool sslArray       = [true, false];
&lt;/pre&gt;
这种形式。

然后使用scope区分不同的配置块。因为可能有些相同的配置会重名，这样我们利用不同的scope去区分就好了。考虑到有些配置中需要共同的变量的使用，所以定义了一个shared的scope，这个是写死的scope，其他scope中只能引用shared scope中的变量。

字符串连接使用```..```操作符。这样可以组装字符串。详细的实现可以在[https://github.com/soleverlee/forks/tree/master/config/src/main forks的子项目config]中找到。

另外还实现了一个类似Play! Framework的路由定义文件的解析，长这个样子的:
&lt;syntaxhighlight lang="lua"&gt;
controllers admin{
package com.riguz.forks.demo.controller
UserController
FileController
}

controllers {
package com.riguz.forks.demo.admin
UserController-&gt;AdminUserController
PostController
}

filters {
package com.riguz.forks.demo.filters
AuthorizationFilter
NocsrfFilter
}

routes admin {
+AuthorizationFilter
get  /users                 UserController.getUsers()
get  /users/:id             UserController.getUser(id: Long)
post /users                 UserController.createUser()
get  /users/:id/files/*name FileUserController.getFile(id: Long, name: String)
}
routes guest {
+NocsrfFilter
get /posts      PostUserController.getPosts()
get /posts/:id  PostUserController.getPost(id: String)
}

routes guest {
+NocsrfFilter
get /posts      PostUserController.getPosts()
get /posts/:id  PostUserController.getPost(id: String)
}
&lt;/syntaxhighlight&gt;
这个文件的解析也在上面的git中可以找到实现。通过Antlr可以很方便的把类似这样的文件解析出来，你甚至可以实现自己的领域语言。在实现过程中，遇到过一些问题，来说下问题吧。

首先是Antlr提供了Listener和Visitor两种方式，起初使用Listener来实现但是感觉比较麻烦，而使用Visitor则可以直接通过返回值来取得AST解析结果。我们解析一个文件的时候，是自顶向下的，一个个的去解析的，比如我们的配置文件的antlr语法定义如下：
&lt;syntaxhighlight lang="lua"&gt;
script
    : shared? scope*
      EOF
    ;
&lt;/syntaxhighlight&gt;
其中shared又是这样的
&lt;syntaxhighlight lang="lua"&gt;
shared
    : SHARED LBRACE (property SEMI)* RBRACE SEMI
    ;

&lt;/syntaxhighlight&gt;
也就是说 ```shared { k=v...} ;```这样的形式，然后又开始到了property:
&lt;syntaxhighlight lang="lua"&gt;
property
    : type NAME ASSIGN expression        #basicProperty
    | type NAME ASSIGN LBRACK
        expression? (COMMA expression)*
      RBRACK                             #arrayProperty
    ;
&lt;/syntaxhighlight&gt;
这样层层往下来看。然后解析的时候也是一样，我们首先有一个顶层的解析器：
&lt;syntaxhighlight lang="java"&gt;
public class ScriptVisitor extends CfParserBaseVisitor&lt;Map&lt;String, ScriptVisitor.Scope&gt;&gt; {
    private static final Logger logger = LoggerFactory.getLogger(ScriptVisitor.class);

    @Override
    public Map&lt;String, Scope&gt; visitScript(CfParser.ScriptContext ctx) {
        ...
    }
&lt;/syntaxhighlight&gt;
这个Visitor负责解析语法文件中定义的script块，然后解析里面的scope：
&lt;syntaxhighlight lang="java"&gt;
ScopeVisitor scopeVisitor = new ScopeVisitor(context);
        ctx.scope().forEach(scopeContext -&gt; {
            logger.debug("Visit scope:{}", scopeContext.getText());
            Scope scope = scopeContext.accept(scopeVisitor);
            scopes.put(scope.name, scope);
        });
&lt;/syntaxhighlight&gt;
这样又实现一个ScopeVisitor去解析scope就好了。详细的实现就不多贴代码了。

另外一个问题是，对于错误的处理，我们在哪一步做？比如```bool s = "123";```这是错误的，我们其实可以在定义grammar的时候就避免这种错误来，但写起来会麻烦一些。目前的实现是在Visitor中去对逻辑进行判断的，前面只做语法检查就可以了。

参考:

* http://jakubdziworski.github.io/java/2016/04/01/antlr_visitor_vs_listener.html</text>
      <sha1>cm33d5nlni1q3nc75cpei1jd3isqsyg</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Java GC小结</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2019-03-03T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>![HotSpot JVM architecture](https://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/images/gcslides/Slide1.png){style="width:400px"}</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5504" sha1="jcertcf8tdrsqvei51xv9r2j7d1zknc">[[File:https://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/images/gcslides/Slide1.png|600px|HotSpot JVM architecture]]{style="width:400px"}


= JVM Generations=

[[File:https://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/images/gcslides/Slide5.png|600px|Hotspot Heap Structure]]{style="width:400px"}

Java的堆被划分成不同的区域：

* young generation：存放新创建的对象，当这个区域占满的时候，会触发minor GC，这时候存活的对象会被标记年龄，最终会移动到old generation。
* old generation：存放存活的比较久的对象。当yound generation存活的对象年龄到达设置的阈值后，就会被移动到这里来。当这个区域满了的时候，会触发major GC。
* permanent generation：存放一些JVM运行所需的元数据，例如类的信息等。full GC的时候也包括对这个区域的GC。
其中，minor GC和major GC都是Stop the World的，即当GC触发的时候，所有的程序线程都会停止等待GC完成。通常minor GC会比major GC快很多，因为major GC会遍历所有的存活对象。

其中，yound generation 又被划分成Eden space, Survivor Space1, Survivor Space2，其中Eden Space占了绝大部分的空间。当Eden space满的时候，GC 会将存活对象移动到其中一个Survivor Space中，两个Survivor Space是为了避免内存碎片，每次将存活的对象（Eden Space以及上一个Survivor Space）移动到另一个Survivor Space中。

[[File:https://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/images/gcslides/Slide9.png|600px|Mnior GC]]{style="width:400px"}

通过Java VisualVM和VisualGC插件可以很直观的看到GC的过程:
[[File:visualVM_GC.png|600px|Visual GC]]

&lt;syntaxhighlight lang="bash"&gt;
java -Xmx50m \
-XX:-PrintGC \
-XX:+PrintHeapAtGC \
-XX:MaxTenuringThreshold=10 \
-XX:+UseConcMarkSweepGC \
-XX:+UseParNewGC TestGC
&lt;/syntaxhighlight&gt;

= Garbage Collectors=

Argument        Description
--------------- ---------------------------------------------------------
-Xms	        Sets the initial heap size for when the JVM starts.
-Xmx	        Sets the maximum heap size.
-Xmn	        Sets the size of the Young Generation.
-XX:PermSize	Sets the starting size of the Permanent Generation.
-XX:MaxPermSize	Sets the maximum size of the Permanent Generation

* **Serial GC**:使用mark-compact算法进行GC，单线程的进行GC，适合单核CPU和在客户端允许的Java程序。
* **Parallel GC(throughput collector)**:多线程进行GC
* **Concurrent Mark Sweep (CMS) Collector**: 在程序运行的时候并发的进行GC，以最大限度减少停止时间
* **G1(Garbage-First) Garbage Collector**: CMS的替代品

其中存在并行(Parallel)和并发(Concurrent)的区别，并行是指垃圾收集器多个线程同时工作，但此时用户线程依然是停止等待的；而并发是指在用户线程工作的同时，垃圾收集器同时执行。


[[File:https://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/f4a5b21d-66fa-4885-92bf-c4e81c06d916/Image/b125abbe194f5608840119eccc9d90e2/collectors.jpg|600px|Java collectors]]{style="width:600px;height:300px;"}

Garbage Collector Type           Algorithm                MultiThread
----------------- -------------- ------------------------ ----------------------
Serial            stop-the-world copying                  No
ParNew            stop-the-world copying                  Yes
Parallel Scavenge stop-the-world copying                  Yes
Serial Old        stop-the-world mark-sweep-compact       No
CMS               low-pause      concurrent-mark-sweep    Yes
Parallel Old      stop-the-world mark-sweep-compact       Yes
G1                               compacting               Yes        


Arguments                Result
------------------------ --------------------------------------------------------
-XX:+UseSerialGC         Serial + Serial Old
-XX:+UseParNewGC         ParNew + Serial Old
-XX:+UseConcMarkSweepGC  ParNew + CMS + Serial Old^["CMS" is used most of the time to collect the tenured generation. "Serial Old" is used when a concurrent mode failure occurs.]
-XX:+UseParallelGC       Parallel Scavenge + Serial Old
-XX:+UseParallelOldGC    Parallel Scavenge + Parallel Old
–XX:+UseG1GC             G1

= GC过程=
== CMS==
CMS 收集器的步骤：

Phase                    Description
------------------------ -------------------------------------------------------
Initial Mark (Stop-Word) 标记老年代中的对象是否可达(reachable)，包括可以从新生代中到达的
Concurrent Marking   
Remark       (Stop-Word)
Concurrent Sweep
Resetting

G1将Heap划分成一些相同大小的区块，但是没有限制不同代的大小。

[[File:https://www.oracle.com/webfolder/technetwork/tutorials/obe/java/G1GettingStarted/images/slide9.png|600px|G1 Heap Allocation]]{style="width:400px"}


References:

* [http://enos.itcollege.ee/~jpoial/allalaadimised/reading/Advanced-java.pdf Advanced Java]
* [https://codeahoy.com/2017/08/06/basics-of-java-garbage-collection/ Basics of Java Garbage Collection]
* [https://www.novatec-gmbh.de/en/blog/g1-action-better-cms/ G1 in Action: Is it better than the CMS?]
* [https://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html Java Garbage Collection Basics]
* [https://www.oracle.com/webfolder/technetwork/tutorials/obe/java/G1GettingStarted/index.html Getting Started with the G1 Garbage Collector]

</text>
      <sha1>jcertcf8tdrsqvei51xv9r2j7d1zknc</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Why I don't use lombok</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2018-01-10T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>很多人，如同我的同事，似乎觉得lombok这玩意就像神一样的存在，“极大”的方便了项目的开发。我个人是不喜欢这玩意的，很简单的理由：

* 生成getter/setter不是多么困难的事情，IDE很简单就能帮你搞定
* 我不喜欢为自己的IDE装一大堆插件，还要为项目手动开启一下Annotation Processing
* 代码不可见，意味着生成的getters/setter方法，以及@AllArgConstructor生成的方法无法维护</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2549" sha1="p50uwtmcp864lp2y48pmaoefuyrrd5b">很多人，如同我的同事，似乎觉得lombok这玩意就像神一样的存在，“极大”的方便了项目的开发。我个人是不喜欢这玩意的，很简单的理由：

* 生成getter/setter不是多么困难的事情，IDE很简单就能帮你搞定
* 我不喜欢为自己的IDE装一大堆插件，还要为项目手动开启一下Annotation Processing
* 代码不可见，意味着生成的getters/setter方法，以及@AllArgConstructor生成的方法无法维护
当我把这些想法告诉同事的时候，同事们都觉得我脑子有问题，理由不充分，”lombok只是一个工具，只是没有找到使用工具的最佳实践“。实际上对于技术人员来说，想说服别人是很困难的事情，然而我们为什么要试图说服别人呢？没有多大的意义。相比于工具，有一些更重要的东西就是：经验和原则。

就我的经验，能简单的事情就不要复杂化，越是复杂越难以维护。当然也有人和我是相同的观点，看了一些有意思的关于为什么不用lombok的讨论，贴出来看看：

&gt; OK, let me put it one more time: this has caused me too many bugs. Let me tell
&gt; you my past experiences with Lombok, as this is the root of the issue.
&gt; 
&gt; On one project, a new version of the Lombok plugin caused the IDE to
&gt; crash (I think this was Intellij). So nobody could work anymore. On
&gt; another project, Lombok made the CI server crash (and would have
&gt; probably caused the production server to crash), as it triggered a bug
&gt; in the JVM On a third project, we achieved 30% performance increase by
&gt; recoding the equals/hashcode from Lombok
&gt; -&gt; In those 3 projects, some developer gained 5 minutes, and I spent hours recoding everything. So yes, a bad experience.
&gt; 
&gt; Then, for JHipster, the story is also that we can't ask people to
&gt; install a plugin on their IDE:
&gt; 
&gt; 1st goal is to have a smooth experience: you generate the app and it
&gt; works in your IDE, by default 2nd goal is that you can use whatever
&gt; IDE you want. And some people have very exotic things, for example I
&gt; just tried https://codenvy.com/ -&gt; no plugin for this one, of course
&gt; 
&gt; Oh, and I just got 2 more:
&gt; 
&gt; Lombok crashing with MapStruct Lombok making Jacoco fails, which meant
&gt; the project didn't pass the Sonar quality gate

参考阅读：

* https://github.com/jhipster/generator-jhipster/issues/398
* https://gist.github.com/ufuk/0ccb87185c22475c64d46801fa160777
* https://stackoverflow.com/questions/3852091/is-it-safe-to-use-project-lombok</text>
      <sha1>p50uwtmcp864lp2y48pmaoefuyrrd5b</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:调试JDK源码</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-01-25T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>最近想要直接调试下JDK的源码却发现有些变量不能显示，像这样：

![IntelliJ](/images/Intellij-no-debuginfo.png)</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2026" sha1="alr6z2tl8w2lt1fl1w713po9jrukk1c">最近想要直接调试下JDK的源码却发现有些变量不能显示，像这样：

[[File:Intellij-no-debuginfo.png|600px|IntelliJ]]


其原因是因为JDK中的rt.jar在release的时候没有附带调试信息。所以解决这个问题的思路是这样的：

* 重新编译你想要调试的类(`javac -g`)
* 将带调试信息的类覆盖原来的
* 重新进行调试

实际上，在IntelliJ中操作起来可能比较简单，可以分为以下几步：

* 将jdk的源码从JDK中拷贝出来，例如/Library/Java/JavaVirtualMachines/jdk1.8.0_231.jdk/Contents/Home/src.zip
* 将源码解压，并将其中`java`,`javax`两个文件夹拷贝到一个空的Java工程中
* 编译源码，IntelliJ默认设置编译是包含了调试信息的。编译完成之后，将其[https://www.jetbrains.com/help/idea/packaging-a-module-into-a-jar-file.html# 导出到jar包]中。
* 将导出的jar包（比如说rt_debug.jar)放入到`$JAVA_HOME/jre/lib/endorsed`文件夹下面即可。如果这个目录不存在，手动创建一下。
* 重新进行调试即可。


[[File:Intellij-with-debuginfo.png|600px|IntelliJ]]

事实上，这里利用了`endorsed-standards override mechanism`这一个JVM特性来重载了Java的类，这个特性已经在Java8中被deprecated了([https://www.java.com/en/download/faq/release_changes.xml release notes for Java 8 Update 40 (8u40) ])，但是Java8中仍然可用。

&gt; The endorsed-standards override mechanism and the extension mechanism are deprecated and may be removed in a future release. There are no runtime changes. Existing applications using the 'endorsed-standards override' or 'extension' mechanisms are recommended to migrate away from using these mechanisms.

Reference:

* [https://stackoverflow.com/questions/1313922/step-through-jdk-source-code-in-intellij-idea Step through JDK source code in IntelliJ IDEA]
* [https://stackoverflow.com/questions/18255474/debug-jdk-source-cant-watch-variable-what-it-is debug jdk source can't watch variable what it is]</text>
      <sha1>alr6z2tl8w2lt1fl1w713po9jrukk1c</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:使用JMH进行Benchmark测试</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2018-05-09T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>JMH是一个测试Java程序性能的工具，比如我们现在要测试一下JDK8自带的Base64和[另一个实现](http://www.java2s.com/Code/Java/Development-Class/AfastandmemoryefficientclasstoencodeanddecodetoandfromBASE64infullaccordancewithRFC2045.htm)的性能。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2349" sha1="lpnshl0gohxyk3s1gepgj01nen8xk6o">JMH是一个测试Java程序性能的工具，比如我们现在要测试一下JDK8自带的Base64和[http://www.java2s.com/Code/Java/Development-Class/AfastandmemoryefficientclasstoencodeanddecodetoandfromBASE64infullaccordancewithRFC2045.htm 另一个实现]的性能。
先看看 build.gradle 中怎么写：
&lt;syntaxhighlight lang="groovy"&gt;
group 'riguz'
version '1.0-SNAPSHOT'

apply plugin: 'java'

sourceCompatibility = 1.8

sourceSets {
    jmh
}
repositories {
    mavenCentral()
}

dependencies {
    jmhCompile project
    jmhCompile 'org.openjdk.jmh:jmh-core:1.21'
    jmhCompile 'org.openjdk.jmh:jmh-generator-annprocess:1.21'
    jmhCompile group: 'junit', name: 'junit', version: '4.12'
    testCompile group: 'junit', name: 'junit', version: '4.12'
}

task jmh(type: JavaExec, description: 'Executing JMH benchmarks') {
    classpath = sourceSets.jmh.runtimeClasspath
    main = 'org.openjdk.jmh.Main'
}

&lt;/syntaxhighlight&gt;
然后写一个类：
&lt;syntaxhighlight lang="java"&gt;
@Benchmark
    @Warmup(iterations = 1, time = 5)
    @Measurement(iterations = 1, time = 5)
    public void encodeWithJdk() {
        final byte[] bytes = Dream.text.getBytes();
        byte[] encoded = Base64.getEncoder().encode(bytes);
        byte[] decoded = Base64.getDecoder().decode(encoded);
        assertTrue(Arrays.equals(bytes, decoded));
    }

    @Benchmark
    @Warmup(iterations = 1, time = 5)
    @Measurement(iterations = 1, time = 5)
    public void encodeWithBase64Codec() throws IOException {
        final byte[] bytes = Dream.text.getBytes();
        byte[] encoded = Base64Codec.encodeToByte(bytes, true);
        byte[] decoded = Base64Codec.decodeFast(encoded, encoded.length);
        assertTrue(Arrays.equals(bytes, decoded));
    }
&lt;/syntaxhighlight&gt;
其中Dream.text是一个很长的字符串。执行gradle的jmh task之后，可以得到结果
&lt;pre&gt;
Benchmark                               Mode  Cnt   Score   Error  Units
Base64BenchMark.encodeWithBase64Codec  thrpt    5  15.296 ± 2.538  ops/s
Base64BenchMark.encodeWithJdk          thrpt    5  13.029 ± 1.563  ops/s
&lt;/pre&gt;
看样子要比JDK的实现强一丢丢，当然只是在上面的这种情况之下。差距并不大。

参考:

* http://tutorials.jenkov.com/java-performance/jmh.html#why-are-java-microbenchmarks-hard
* https://www.jianshu.com/p/192b782c31bc</text>
      <sha1>lpnshl0gohxyk3s1gepgj01nen8xk6o</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:浅析Java中的InvokeDynamic</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2019-12-24T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>Java语言在被编译成class文件后，在class文件中，有专门的一个[“常量池”(Constant Pool)](https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.4)区域来存储一些运行所需要的常量，包括一些写死的变量（比如定义一个字符串`String str = "Hello world"`以及一些符号，例如类和方法的的名称等）。在JVM(se7)规范中，有以下这些类型的常量：

```bash
CONSTANT_Class 	                         CONSTANT_Long 	          
CONSTANT_Fieldref 	                     CONSTANT_Double 	      
CONSTANT_Methodref 	                     CONSTANT_NameAndType 	  
CONSTANT_InterfaceMethodref              CONSTANT_Utf8 	          
CONSTANT_String 	                     CONSTANT_MethodHandle 	  
CONSTANT_Integer 	                     CONSTANT_MethodType 	  
CONSTANT_Float 	                         CONSTANT_InvokeDynamic 	  
```
大部分我们顾名思义，都可以知道是大概是干啥的，比如字符串啊，数字啊，方法名称之类的；但是可以注意到最后面一个是称之为`CONSTANT_InvokeDynamic`的常量，这个就有点陌生了。那么，这是一个什么样的常量？什么情况下会出现这个呢？</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="16608" sha1="b0mnntl5i7h1chutyc7ijzf9b08ehc7">Java语言在被编译成class文件后，在class文件中，有专门的一个[https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.4 “常量池”(Constant Pool)]区域来存储一些运行所需要的常量，包括一些写死的变量（比如定义一个字符串`String str = "Hello world"`以及一些符号，例如类和方法的的名称等）。在JVM(se7)规范中，有以下这些类型的常量：

&lt;syntaxhighlight lang="bash"&gt;
CONSTANT_Class 	                         CONSTANT_Long 	          
CONSTANT_Fieldref 	                     CONSTANT_Double 	      
CONSTANT_Methodref 	                     CONSTANT_NameAndType 	  
CONSTANT_InterfaceMethodref              CONSTANT_Utf8 	          
CONSTANT_String 	                     CONSTANT_MethodHandle 	  
CONSTANT_Integer 	                     CONSTANT_MethodType 	  
CONSTANT_Float 	                         CONSTANT_InvokeDynamic 	  
&lt;/syntaxhighlight&gt;
大部分我们顾名思义，都可以知道是大概是干啥的，比如字符串啊，数字啊，方法名称之类的；但是可以注意到最后面一个是称之为`CONSTANT_InvokeDynamic`的常量，这个就有点陌生了。那么，这是一个什么样的常量？什么情况下会出现这个呢？


= `invokedynamic`指令=
在JVM规范中有说，`CONSTANT_InvokeDynamic`常量是用来给`invokedynamic`指令指定一系列的参数的，那么有必要先了解一下`invokedynamic`这个指令了。这是Java 7引入的一个新指令，也是自Java 1.0以来第一次引入新的指令。

== Java7之前的`invoke-`指令==
实际上，在此之前，已经有一些列的`invoke`开头的指令了：

* `invokevirtual`：用来调用类的实例方法，也就是最普遍的方式
* `invokestatic`：用来调用静态方法
* `invokeinterface`：用来调用通过接口调用的方法
* `invokespecial`：用来调用一些编译时就能够确定的，包括初始化(`&lt;init&gt;`)、类的私有方法，以及父类的方法(`super.someMethod()`)

拿一个简单的Java程序来看看是怎么回事：

&lt;syntaxhighlight lang="java"&gt;
// Foo.java
public class Foo {
    public static void main(String[] args) {
        long now = System.currentTimeMillis();            //静态方法调用

        ArrayList&lt;String&gt; arrayList =  new ArrayList&lt;&gt;(); //构造函数将被调用
        List&lt;String&gt; list = arrayList;

        arrayList.add("hello");                           //调用类实例方法
        list.add("world");                                //通过接口调用
    }
}
&lt;/syntaxhighlight&gt;
通过`javac Foo.java &amp;&amp; javap -v Foo`可以查看编译后生成的class文件，里面可以找到`invoke`相关的指令调用：

&lt;pre&gt;
public static void main(java.lang.String[]);
    descriptor: ([Ljava/lang/String;)V
    flags: ACC_PUBLIC, ACC_STATIC
    Code:
      stack=2, locals=5, args_size=1
         0: invokestatic  #2                  // Method java/lang/System.currentTimeMillis:()J
         3: lstore_1
         4: new           #3                  // class java/util/ArrayList
         7: dup
         8: invokespecial #4                  // Method java/util/ArrayList."&lt;init&gt;":()V
        11: astore_3
        12: aload_3
        13: astore        4
        15: aload_3
        16: ldc           #5                  // String hello
        18: invokevirtual #6                  // Method java/util/ArrayList.add:(Ljava/lang/Object;)Z
        21: pop
        22: aload         4
        24: ldc           #7                  // String world
        26: invokeinterface #8,  2            // InterfaceMethod java/util/List.add:(Ljava/lang/Object;)Z
        31: pop
        32: return
&lt;/pre&gt;
这样就比较好理解了，就跟我们平常调用函数一样，

* `System.currentTimeMillis()`静态函数的调用生成了`invokestatic`指令，这个指令的参数是静态方法（包含类名和方法名）
* `ArrayList`的构造方法调用生成了`invokespecial`指令，这里在`new`指令之后接着使用`invokespecial`指令来进行初始化操作
* 通过`ArrayList`的实例方法`add`调用生成了`invokevirtual`指令
* 而通过`List`接口的`add`方法调用生成了`invokeinterface`指令

尽管已经有了以上的四种指令，这些指令都有一个特点，那就是不管是什么方法，是静态还是实例方法，是子类还是父类的方法，在编译的时候已经能够确定出到底会调用到哪个方法了。有没有一种可能，就是我在编译的时候不能确定，而是在运行的时候才能确定呢？

== 鸭子类型（***Duck Typing***）==

这就是所谓的[https://zh.wikipedia.org/wiki/%E9%B8%AD%E5%AD%90%E7%B1%BB%E5%9E%8B 鸭子类型]了，可能叫***Duck typing***其实更好理解一点，这个名称来源自[https://zh.wikipedia.org/wiki/%E9%B8%AD%E5%AD%90%E6%B5%8B%E8%AF%95 鸭子测试]:

&gt; “当看到一只鸟走起来像鸭子、游泳起来像鸭子、叫起来也像鸭子，那么这只鸟就可以被称为鸭子。”

[[File:https://devopedia.org/images/article/24/2998.1514520209.jpg|600px|A humorous and apt representation of duck typing. Source: Mastracci, 2014.]]

我们知道，Java是一个强类型的语言，有很多的类型检查，比如你要调用某个接口，而被调用的对象没有实现这个接口那么是无法完成的。而Duck Typing正如上面这个图片形象的表示，我并不关心对象本身是个什么东西，而关心这个对象是否支持我所需要的所有属性或者方法。维基百科上的这个伪代码可以更直接的解释：

&lt;pre&gt;
function calculate(a, b, c) =&gt; return (a+b)*c

example1 = calculate (1, 2, 3)
example2 = calculate ([1, 2, 3], [4, 5, 6], 2)
example3 = calculate ('apples ', 'and oranges, ', 3)
&lt;/pre&gt;

这些对象没有使用继承或者其他的方式相互发生联系，但只要它们支持`+`和`*`这两个方法，调用就可以成功。`invokedyanmic`指令从某种程度上来说，就是为了支持*Duck typing*。

看到这里可能细心的读者会注意到，咦，这东西看着好像`lambda`?

没错，事实上`lambda`的确是跟`invokedynamic`有关的，但有意思的是`lambda`是直到Java 8才推出（[https://www.jcp.org/en/jsr/detail?id=335 JSR 335: Lambda Expressions for the JavaTM Programming Language])。在此之前，是无法通过`javac`编译器生成包含这个指令的class的。`invokedynamic`指令是在[https://jcp.org/en/jsr/detail?id=292 JSR 292: Supporting Dynamically Typed Languages on the JavaTM Platform]中被引入的，可以注意到，原本是为了支持基于JVM的动态语言，并不是说要在Java中来做Duck typing，这样就比较合理了。

当然借助于一些字节码操作框架（例如[http://www.csg.is.titech.ac.jp/~chiba/javassist/ Javassit]、[http://asm.ow2.org/ ASM]等，是可以手动创造出含有`invokedynamic`的class的，不过会有些麻烦。


== lambda与invokedynamic==
如果我们用支持`lambda`的Java 8是可以很容易的创建出一个包含`invokedynamic`常量和指令的class的，比如下面这个例子：

&lt;syntaxhighlight lang="java"&gt;
import java.util.function.*;

public class Hello {

    public static void main(String[] args) {
        Supplier&lt;String&gt; welcome = () -&gt; "Hello world!";
        System.out.println(welcome.get());
    }
}
&lt;/syntaxhighlight&gt;
当查看编译后的class文件会发现有下面的部分：

&lt;pre&gt;
Constant pool:
   #1 = Methodref          #9.#20         // java/lang/Object."&lt;init&gt;":()V
   #2 = InvokeDynamic      #0:#26         // #0:get:()Ljava/util/function/Supplier;

public static void main(java.lang.String[]);
    descriptor: ([Ljava/lang/String;)V
    flags: ACC_PUBLIC, ACC_STATIC
    Code:
      stack=2, locals=2, args_size=1
         0: invokedynamic #2,  0              // InvokeDynamic #0:get:()Ljava/util/function/Supplier;
&lt;/pre&gt;

一个就是在常量池中可以看到一个InvokeDynamic类型的常量，指向了`Supplier.get()`方法；另一个就是在`main`方法中对lambda的调用，被编译成`invokedynamic`指令。

= invokedynamic的机制=
来研究一下`invokedyanmic`到底是怎么工作的吧。要了解它怎么工作的，我们先要知道编译生成的class文件中有些什么。刚我们看到，class文件中有两个部分与之相关，一个是常量池中的InvokeDyanmic信息，另一个是方法字节码中的`invokedynamic`指令调用。实际上JVM在引入这个新指令的同时，也在常量池(Constant Pool)和属性表(Attributes)中加入了与之相关的内容，也就是`CONSTANT_InvokeDynamic_info`和`BootstrapMethods_attribute`。得益于class文件的扩展性，这些改动实际上并没有改变class文件本身的结构，仅仅只是加了更多合法的选项在里边。

== 引导函数(Bootstrap method)表==
在class的`Attributes`（属性表）中新加入的一个`BootstrapMethods_attribute`属性，这个属性里面会存储一些函数的相关信息，而且是和`CONSTANT_InvokeDynamic`常量一一对应的。

&lt;syntaxhighlight lang="c++"&gt;
BootstrapMethods_attribute {
    u2 attribute_name_index;
    u4 attribute_length;
    u2 num_bootstrap_methods;
    {   u2 bootstrap_method_ref;
        u2 num_bootstrap_arguments;
        u2 bootstrap_arguments[num_bootstrap_arguments];
    } bootstrap_methods[num_bootstrap_methods];
}
&lt;/syntaxhighlight&gt;

每一个引导函数都包含几个重要的属性： 

* bootstrap_method_ref: 指向一个`CONSTANT_MethodHandle_info`引用，表明实际调用的方法信息
* bootstrap_arguments: 对应这个函数的参数

这个引导函数通常是这个样子：

&lt;syntaxhighlight lang="java"&gt;
static CallSite bootstrapMethod(MethodHandles.Lookup caller, String name, MethodType type);
&lt;/syntaxhighlight&gt;
这个函数返回一个调用点([https://docs.oracle.com/javase/7/docs/api/java/lang/invoke/CallSite.html CallSite])对象，这个对象包含了方法调用所需要的一切信息，用来给`invokedynamic`指令使用。

在Java8的`java.lang.invoke.LambdaMetafactory`类中，对应也增加定义了两个用来支持调用lambda的引导函数：

&lt;syntaxhighlight lang="java"&gt;
static CallSite altMetafactory(MethodHandles.Lookup caller, String invokedName, MethodType invokedType, 
                               Object... args);
static CallSite metafactory(MethodHandles.Lookup caller, String invokedName, MethodType invokedType, 
                            MethodType samMethodType, MethodHandle implMethod, MethodType instantiatedMethodType);
&lt;/syntaxhighlight&gt;

而在上面的例子中，这个引导函数是这样的：

&lt;pre&gt;
BootstrapMethods:
  0: #22 invokestatic java/lang/invoke/LambdaMetafactory.metafactory:(Ljava/lang/invoke/MethodHandles$Lookup;Ljava/lang/String;Ljava/lang/invoke/MethodType;Ljava/lang/invoke/MethodType;Ljava/lang/invoke/MethodHandle;Ljava/lang/invoke/MethodType;)Ljava/lang/invoke/CallSite;
    Method arguments:
      #23 ()Ljava/lang/Object;
      #24 invokestatic Hello.lambda$main$0:()Ljava/lang/String;
      #25 ()Ljava/lang/String;
&lt;/pre&gt;

== InvokeDynamic常量==

InvokeDynamic常量的定义如下：

&lt;syntaxhighlight lang="c++"&gt;
CONSTANT_InvokeDynamic_info {
    u1 tag;
    u2 bootstrap_method_attr_index;
    u2 name_and_type_index;
}
&lt;/syntaxhighlight&gt;

其中`bootstrap_method_attr_index`指向一个引导函数(Bootstrap method)的序号，`name_and_type_index`则表明方法的名称和描述。在上面的例子中，仅有一个引导函数，这个`bootstrap_method_attr_index`自然就是对应到这个引导函数了。而在`invokedynamic`指令调用的地方，是这样的：

&lt;pre&gt;
invokedynamic #2,  0              // InvokeDynamic #0:get:()Ljava/util/function/Supplier;
&lt;/pre&gt;
其中这个`#2`即常量池中的`InvokeDynamic`常量。

== `invokedynamic`指令==

根据[https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-6.html#jvms-6.5.invokedynamic JVM的规范]中的描述可以看到`invokedynamic`指令的格式如下：

&lt;pre&gt;
invokedynamic indexbyte1 indexbyte2 0 0
&lt;/pre&gt;

其中前两个操作数可以通过`(indexbyte1 &lt;&lt; 8) | indexbyte2`的方式合成一个常量池中的索引值，也就是上面的`#2`，而另外两个操作数是固定的0。

= Labmda的JVM实现=

刚才已经看到，Java中对lambda'的调用实际上通过`LambdaMetafactory.metafactory`来完成的，通过了解这个类的实现，可以一看lambda的究竟。

[[File:debug_metafactory.png|600px|Debug metafactory]]

在这个类里面创建了一个匿名类，并通过`UNSAFE.ensureClassInitialized(innerClass)`直接加载到JVM中，没有出现在class文件中，不过通过jvm参数可以输出出来：

&lt;syntaxhighlight lang="bash"&gt;
java -Djdk.internal.lambda.dumpProxyClasses=/Users/hfli/Downloads/tmp Hello
&lt;/syntaxhighlight&gt;
这样会生成一个`Hello$$Lambda$1.class`的文件，反编译这个类可以看到如下的信息：

&lt;syntaxhighlight lang="java"&gt;
final class Hello$$Lambda$1 implements Supplier {
    private Hello$$Lambda$1() {
    }

    @Hidden
    public Object get() {
        return Hello.lambda$main$0();
    }
}
&lt;/syntaxhighlight&gt;

这里实际是包装了一下`Supplier`接口，而具体调用的`Hello.lambda$main$0()`方法，可以在`Hello.class`文件中看到（需要使用`javap -p`选项输出私有方法):

&lt;pre&gt;
private static java.lang.String lambda$main$0();
    descriptor: ()Ljava/lang/String;
    flags: ACC_PRIVATE, ACC_STATIC, ACC_SYNTHETIC
    Code:
      stack=1, locals=0, args_size=0
         0: ldc           #7                  // String Hello world!
         2: areturn
      LineNumberTable:
        line 6: 0
&lt;/pre&gt;

所以大致是这个样子的过程：

* 调用lambda时，首先通过找到对应的引导方法（也就是`metafactory()`)，开始执行
* JVM生成一个匿名类`Hello$$Lambda$1`，这个类中包含了lambda的实际实现
* 创建一个CallSite，绑定到一个MethodHandle指向这个匿名类的实现`Hello$$Lambda$1.get()`。这里引导方法就调用完成了
* 这个MethodHandle指向的方法被执行，调用到`Hello.lambda$main$0()`关联的字节码，得到最终的结果

值得注意的是，引导方法只需要执行一次，如果一个lambda执行了多次，那么只有第一次会去调用引导方法生成CallSite，以后都可以直接拿来使用了。

= 结语=

通过上面的描述，相信大家对`invokedynamic`有了一个粗略的了解，但要真正深入去了解的话，还是有很多东西需要去了解和研究的。虽然`invokedynamic`指令很强大，给了JVM的开发者很大的自由度，但实际上对于Java程序员来说，并没有太多可以操控的东西。如同上面提到的Duck Typing，在C#中可以这样：

&lt;syntaxhighlight lang="c#"&gt;
Object obj = ...; // no static type available 
dynamic duck = obj;
duck.quack();     // or any method. no compiler checking.
&lt;/syntaxhighlight&gt;
可能我们永远也没法使用Java来完成同样的任务，也许有一部分人会比较失望，但本身这是一把双刃剑，我还是倾向[http://www.yinwang.org/blog-cn/2016/01/18/java 给Java说句公道话]。而借助于JVM平台，我们实际上有了越来越多的选择，Scala, Kotlin，Groovy等等。可以说从某个方面来讲，正是Java决策者对于每一个决策的慎重，才造就了今天Java程序员不愁饭吃的局面。

&gt; "When you have 9 million programmers using your language and out of which 1 million programmers know where you live you have to decide things differently."
&gt; ——[https://www.youtube.com/watch?v=1OpAgZvYXLQ&amp;t=1993s Venkat Subramaniam]

参考文章：

* [https://www.infoq.com/articles/Invokedynamic-Javas-secret-weapon/ Invokedynamic - Java’s Secret Weapon]
* [https://stackoverflow.com/questions/6638735/whats-invokedynamic-and-how-do-i-use-it What's invokedynamic and how do I use it?]
* [https://zh.wikipedia.org/wiki/%E9%B8%AD%E5%AD%90%E7%B1%BB%E5%9E%8B Duck Typing - Wiki]
* [https://devopedia.org/duck-typing Duck Typing]
* [http://blog.headius.com/2008/09/first-taste-of-invokedynamic.html A First Taste of InvokeDynamic]
* [https://www.javacodegeeks.com/2012/02/java-7-complete-invokedynamic-example.html Java 7: A complete invokedynamic example]
* [https://my.oschina.net/lt0314/blog/3146028 你不知道Lambda的秘密和陷阱]
* [http://wiki.jvmlangsummit.com/images/9/93/2011_Forax.pdf JSR 292 Cookbook]
* [https://www.jianshu.com/p/d74e92f93752 理解 invokedynamic]

邮件阅读体验不佳，如需更好体验可以移步 [https://riguz.com/it/java/java_invokedynamic/ 在线版本]</text>
      <sha1>b0mnntl5i7h1chutyc7ijzf9b08ehc7</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:理解Java并发(6)：ScheduledExecutorService</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-09-03T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>除了ThreadPoolExecutor之外，还有一种`ScheduledExecutorService`支持按照一定的延迟或者固定的间隔来执行任务。

```java
ScheduledExecutorService executorService = Executors
	  .newSingleThreadScheduledExecutor();
```</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3984" sha1="jfjfkc2oswrtiqjvn8dfmtpj57cp57s">除了ThreadPoolExecutor之外，还有一种`ScheduledExecutorService`支持按照一定的延迟或者固定的间隔来执行任务。

&lt;syntaxhighlight lang="java"&gt;
ScheduledExecutorService executorService = Executors
	  .newSingleThreadScheduledExecutor();
&lt;/syntaxhighlight&gt;

譬如实现一个定时运行的任务：

&lt;syntaxhighlight lang="java"&gt;
public class BeeperControl {
    private final ScheduledExecutorService scheduler =
            Executors.newScheduledThreadPool(1);

    public void beep() {
        scheduler.scheduleAtFixedRate(() -&gt; {
            System.out.println("beep");
        }, 3, 2, TimeUnit.SECONDS);
    }

    public static void main(String[] args) {
        BeeperControl ctl = new BeeperControl();
        ctl.beep();
    }
}
&lt;/syntaxhighlight&gt;
=  Schedule模式=
== FixedRate==
FixedRate允许按照固定的速率来运行，例如：

&lt;syntaxhighlight lang="java"&gt;
 scheduler.scheduleAtFixedRate(() -&gt; {
            System.out.println("beep");
        }, 3, 2, TimeUnit.SECONDS); // 最开始延迟3秒；之后每2秒一次运行
&lt;/syntaxhighlight&gt;

假定开始时间为$T_0$，那么任务的运行时间为：

* $T_1 = T_0 + InitialDelay$
* $T_2 = T_1 + Period \times 1$
* $T_n = T_1 + Period \times (n - 1)$

但是这里存在一个问题，就是如果当任务的执行时间大于Period的时候，会怎样执行？

实际情形就是，如果执行时间超过了period, 那么在运行结束之后，下一个任务会立即执行。而Executor的行为是根据上面的公式创建并提交任务，也就意味着，假设period是2秒钟，而第一次执行花费了5秒，那么在这段时间之内不止一个（下一次的）任务被提交，那么后面若干次都会立刻执行，如下所示：

&lt;pre&gt;
start:1599108257     开始时间
beep :(5)1599108260  第一次执行，花费5秒
beep :(1)1599108265  第二次执行，（以后每次都）花费1秒
beep :(1)1599108266  立即执行
beep :(1)1599108267  立即执行
beep :(1)1599108268  立即执行
beep :(1)1599108270  间隔2秒
beep :(1)1599108272
beep :(1)1599108274
beep :(1)1599108276
beep :(1)1599108278
beep :(1)1599108280
&lt;/pre&gt;

虽然我们实际上可以配置线程池，但是根据JDK文档描述，不会出现同时运行多个任务的情况：

&gt; If any execution of this task takes longer than its period, then subsequent executions may start late, but will not concurrently execute.

所以实际上，这个线程池是为了配置多个scheduler使用的，也就是说，调用多次`scheduler.scheduleAtFixedRate`创建了很多任务，那么这些任务是有可能会同时执行的，这个时候，就会利用到线程池了。假设线程池中只有一个线程，那么如果两个任务都需要执行，而这时候第一个任务没有执行完成，是需要等到第一个任务执行完之后才能得到空闲的线程来执行第二个任务的。

== FixedDelay==
FixedRate允许按照固定的间隔来运行，例如：

&lt;syntaxhighlight lang="java"&gt;
 scheduler.scheduleAtFixedRate(() -&gt; {
            System.out.println("beep");
        }, 3, 2, TimeUnit.SECONDS); // 最开始延迟3秒；之后每次在上一次完成之后延迟2秒执行
&lt;/syntaxhighlight&gt;

假定开始时间为$T_0$，第n次任务执行时间为$P_n$，那么任务的开始执行时间为：

* $T_1 = T_0 + InitialDelay$
* $T_2 = T_1 + P_0 + Delay$
* $T_n = T_(n-1) + P_(n-1) + Delay$

也就是说这个延迟是算上了程序运行的时间的。

= 与timer的区别=

* Timer会受到系统时钟（改变）的影响
* Timer只有一个执行线程，对于长时间运行的任务会导致阻塞后续任务
* Timer中如果抛出异常那么直接就整个都结束了；而ScheduledThreadExecutor中默认处理了异常（会cancel抛出异常的任务），但是其他的任务还可以继续执行。

ref:

* [https://stackoverflow.com/questions/409932/java-timer-vs-executorservice Java Timer vs ExecutorService?]</text>
      <sha1>jfjfkc2oswrtiqjvn8dfmtpj57cp57s</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:一个简单的ETL程序</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-05-04T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>这两天闲着没事准备玩一下社工库，网上有很多以前的社工库（这些都可以下载到，但是实际上已经没有什么太大的价值了，因为暴露时间太久，以及相关的网站都已经做了处理，所以别指望能够找到什么有价值的东西），通过社工库可以了解到的一个实际的数据就是，用户的设置的密码大都是什么样子的。我准备看一下搜云社工库，这个库大概4亿多条数据，主要目的是实践一下大量数据的处理。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7810" sha1="ik4x1r8zp6mv24ncye30h34smer4ovs">这两天闲着没事准备玩一下社工库，网上有很多以前的社工库（这些都可以下载到，但是实际上已经没有什么太大的价值了，因为暴露时间太久，以及相关的网站都已经做了处理，所以别指望能够找到什么有价值的东西），通过社工库可以了解到的一个实际的数据就是，用户的设置的密码大都是什么样子的。我准备看一下搜云社工库，这个库大概4亿多条数据，主要目的是实践一下大量数据的处理。


= 还原原始数据库=
这个库（下载地址请自行搜索）下载完成之后是一个`1.bak`（SQL SERVER的备份文件）文件，接近30G，网上有详细的教程如何导入，总结起来大致如下:

* 安装SQL SERVER 2008 R2（标准版或者更高）
* 将1.bak复制到SQL SERVER的实例备份目录，否则会无法导入
* 因为还原后的数据库占用空间在130G，所以要保证数据库的存储磁盘空间足够

还原完成之后即可得到一个sgk的数据表，里面有423771078条数据。

= 导入到MySQl=
因为最终希望能够在MYSQL中使用数据，所以还得将数据迁移到MySQL中。最初的想法是直接将数据导出到csv，然后再通过MySQL导入csv。但是这个方法尝试了几次之后发现一些问题：

* 数据中可能存在一些非可见字符，导致导出后的csv格式混乱
* 莫名其妙的问题无法解析

索性就直接放弃了这种方式，而是通过手写ETL来完成迁移。于是设计了一个简陋的ETL程序，还挺有意思的。

== 整体架构==

ETL的主要目的是把数据从一个地方转移迁移到另一个地方，这其中可能进行一些其他的操作比如清洗或者格式转换。整体的思路是这样的：

* 有一个source（源数据）和destination（目标数据源）
* ETL执行的时候从源数据查询数据，然后保存到目标数据库中，为了提高效率，每次会查询一批数据，保存的时候也会一批一批保存
* 为进一步提高性能，ETL可以设定多个线程同时处理，为了避免冲突，每个线程不会处理同一条数据，是互斥的

== ETL Engine==

&lt;syntaxhighlight lang="java"&gt;
public class Engine {
    private static final Logger logger = LoggerFactory.getLogger(Engine.class);

    private final int threads;
    private final int totalCount;
    private final int batchSize;

    private final HikariDataSource source;
    private final HikariDataSource destination;

    private final AtomicLong rangeSelector = new AtomicLong(0);

    private final List&lt;Thread&gt; tasks = new ArrayList&lt;&gt;();

    public Engine(int threads, int totalCount, int batchSize) {
        this.threads = threads;
        this.totalCount = totalCount;
        this.batchSize = batchSize;
        this.source = new HikariDataSource(new HikariConfig("src/main/resources/source.properties"));
        this.destination = new HikariDataSource(new HikariConfig("src/main/resources/destination.properties"));
    }
&lt;/syntaxhighlight&gt;

这个ETL引擎担负管理任务的角色，有这些参数：

* threads: 可以设定多少个线程同时运行；
* totalCount: 设定一个totalCount作为任务结束的条件（当然也可以在查询不到数据的时候结束)
* batchSize: 每一批的大小
* rangeSelector: 用来控制每个线程处理的数据，这里将直接用id来区分

同时直接初始化了两个数据库的连接池，这里选用的是Hikari连接池。

&lt;syntaxhighlight lang="java"&gt;
public void run() throws InterruptedException {
    final CountDownLatch countDownLatch = new CountDownLatch(threads);
    for (int i = 0; i &lt; threads; i++) {
        Thread thread = new Thread(new Task(source,
                destination,
                totalCount,
                rangeSelector,
                batchSize,
                countDownLatch));
        tasks.add(thread);
    }
    tasks.forEach(Thread::start);
    countDownLatch.countDown();
    countDownLatch.await();
}
&lt;/syntaxhighlight&gt;

核心逻辑就是，启动N个线程，然后一直等待到每个线程都结束，即完成了ETL任务。

== Task==

Task对应到每个线程，每个线程处理的任务是一样的，只是处理的数据记录不同。

&lt;syntaxhighlight lang="java"&gt;
public void run() {
    logger.info("ETL task {} running...", Thread.currentThread().getId());
    try (Connection sourceConn = source.getConnection();
         Connection destinationConn = destination.getConnection()) {
        while (true) {
            final long startId = rangeSelector.getAndAdd(batchSize);
            final long endId = startId + batchSize;
            if (startId &gt; totalCount) {
                logger.info("Reached end of records:{} , task finished", startId);
                break;
            }
            job.doTransfer(sourceConn, destinationConn, startId, endId);
        }
    } catch (SQLException ex) {
        logger.error("Failed to get data source, ex");
    } finally {
        logger.info("ETL task {} finished.", Thread.currentThread().getId());
        countDownLatch.countDown();
    }

}
&lt;/syntaxhighlight&gt;

处理过程也很简单，拿到一个connection之后，根据rangeSelector得到要获取的id范围，然后委派给具体的Job去处理。当超出最大范围的时候，停止该线程。

== Job==

Job对应到具体每条数据该如何传输，会稍微麻烦一点。但本质还是select然后insert，没什么技术含量。

&lt;syntaxhighlight lang="java"&gt;
public class SeTransferJob implements TransferJob {
    private static final Logger logger = LoggerFactory.getLogger(SeTransferJob.class);
    private static final String query = "select * from sgk where id &gt;? and id &lt;=?";
    private static final String insertSqlTemplate = "insert into se_record_%d(id, user_name, email, password, salt, source, remark) values (?,?,?,?,?,?,?);";

    @Override
    public void doTransfer(Connection source, Connection target, long startId, long endId) {
        logger.info("Transferring records [{}, {}]", startId, endId);

        try {
            target.setAutoCommit(false);
        } catch (SQLException throwables) {
            throw new RuntimeException("Cannot open transaction");
        }
        long tableIndex = (startId / 10000000) + 1;
        String insertSql = String.format(insertSqlTemplate, tableIndex);
        try (PreparedStatement queryStatement = source.prepareStatement(query);
             PreparedStatement saveStatement = target.prepareStatement(insertSql)) {
            queryStatement.setLong(1, startId);
            queryStatement.setLong(2, endId);
            try (ResultSet result = queryStatement.executeQuery()) {
                while (result.next()) {
                    Record record = Record.from(result);
                    if (!record.isValid()) {
                        logger.warn("Found invaid record:{}", record);
                    } else {
                        record.attach(saveStatement);
                        saveStatement.addBatch();
                    }
                }
            }
            saveStatement.executeBatch();
            target.commit();
            logger.info("Records [{}, {}] -&gt; se_record_{} commited", startId, endId, tableIndex);
        } catch (SQLException ex) {
            logger.error("Unexpected sql error:", ex);
            throw new RuntimeException(ex);
        }
    }
}
&lt;/syntaxhighlight&gt;
这里因为数据量太大，做了一个分区的处理，直接按照id来进行分区，每个表控制在千万以下。因此导入完成后，最终会有40多个表，每个表有接近1千万的数据。否则单表过大之后的插入性能会变得很低。

以上就是整个ETL的核心思想，还是有一定的扩展性的，哈哈。</text>
      <sha1>ik4x1r8zp6mv24ncye30h34smer4ovs</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:理解Java并发(4)：Hotspot并发实现浅析</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-05-21T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>学习一下Hotspot中的锁实现。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4949" sha1="awpffyagai73xw4hoz5pacyja0e29t0">学习一下Hotspot中的锁实现。


= Head word=

JVM 中对象头包含两部分信息，一个是Mark word，存储同步、GC等信息；另一部分是存该对象所属的类型的指针：

[[File:https://wiki.openjdk.java.net/download/attachments/11829266/Synchronization.gif?version=4&amp;modificationDate=1208918680000&amp;api=v2|600px|Hotspot synchronization]]



= Hotspot锁实现=
== 轻量级加锁过程（thin lock）==
对于一个未锁定的对象，如果不允许偏向，那么当线程尝试给这个对象加锁的时候，首先尝试使用轻量级锁定，步骤如下：

* 首先在当前线程的桢栈中创建一个lock record信息，保存对象的mark word，然后尝试通过CAS把这个lock record的地址设置到对象的header word中
* 如果CAS成功，那么当前线程成功获取到锁，这时候最后两位是00，表示对象被轻量级锁定
* 如果CAS失败，这时候首先需要判断一下当前线程是否已经持有锁（存在当前线程递归获取锁的场景；但CAS比较的时候，比较的条件是按照对象未锁定的场景去比较的，所以即使对象已经被轻量级锁定了，在CAS之前根本没有去判断），如果是则表明当前线程已经取得锁，可以继续执行
* 如果不是，那么说明有两个线程同时尝试锁定一个对象，这时候需要膨胀为重量级锁

&lt;syntaxhighlight lang="c++"&gt;
// bytecodeInterpreter.cpp
 if (!success) {
   markOop displaced = rcvr-&gt;mark()-&gt;set_unlocked();
   mon-&gt;lock()-&gt;set_displaced_header(displaced);
   if (Atomic:: (mon, rcvr-&gt;mark_addr(), displaced) != displaced) {
     // Is it simple recursive case?
     if (THREAD-&gt;is_lock_owned((address) displaced-&gt;clear_lock_bits())) {
       mon-&gt;lock()-&gt;set_displaced_header(NULL);
     } else {
       CALL_VM(InterpreterRuntime::monitorenter(THREAD, mon), handle_exception);
     }
   }
 }
&lt;/syntaxhighlight&gt;

而当线程运行完成之后，还需要把对象的mark word还原回去：

* 从线程的栈中获取原来的mark word，尝试使用CAS设置到对象上
* 如果成功，那么不需要做其他事情
* 如果失败，表明已经膨胀为重量级锁了，需要通知到等待线程

对于同一个线程递归锁定的场景，如果上一步CAS失败发现已经被自己持有锁，这个时候在栈上的lock record中设置为0，个人理解是如果设置为0那么在解锁的时候，可以控制不采用CAS恢复对象mark word，而是等到第一个lock操作对应的unlock操作的时候去恢复。

== 偏向锁（Store-Free Biased Locking）==
轻量级加锁解决的问题就是，多个线程交替地去获取锁，但实际没有并发争用。在实际的软件中，还有许多场景是，一个对象在生命周期内由始至终只有一个线程会去锁定，那么，在这这种情况下是否可以避免反复的CAS操作，而是直接”偏向“让原来持有锁的线程获取锁呢？

Java对象初始化的时候的header word有会有一个是否允许偏向的标志位：

* 如果该类可以使用偏向锁，则对象包含thread id（初始化位0），biased_lock=1表示允许偏向
* 如果该类不可以使用偏向锁，则对象包含一个hash code，biased_lock被设置位0表示不允许偏向

那么，在尝试加锁的过程中，如果发现允许偏向，则步骤如下：

* 尝试通过CAS，将当前线程ID、epoch等替换到对象头中，这是唯一的一次CAS操作，称之为initial lock
* 当线程持有对象的偏向锁之后，后续该线程的加锁和解锁无需额外的CAS操作或者更新对象头

而当一个线程尝试对一个偏向其他线程的对象加锁的时候，需要撤销偏向锁，并把现场恢复成好像是通过thin lock锁住这个对象一样。这时候进行的步骤如下：

* 停止偏向锁持有线程到安全点
* 遍历偏向锁的持有线程的栈，调整lock record为thin lock的模式；并把最开始的lock record设置到对象的header中
* 恢复线程，按照thin lock的方式执行（包括膨胀机制）



Reference:

* https://docs.huihoo.com/javaone/2006/java_se/JAVA%20SE/TS-3412.pdf
* https://fliphtml5.com/tzor/bqxz/basic
* https://www.artima.com/insidejvm/ed2/index.html
* [https://wiki.openjdk.java.net/display/HotSpot/Synchronization OpenJDK Wiki - Synchronization]
* https://www.zhihu.com/question/53826114
* [https://blogs.oracle.com/dave/biased-locking-in-hotspot Biased Locking in HotSpot]
http://gee.cs.oswego.edu/dl/jmm/cookbook.html
* [https://www.semanticscholar.org/paper/Eliminating-synchronization-related-atomic-with-and-Russell-Detlefs/356a2d9859520c9161d67828d45e758a24ecce20 Eliminating synchronization-related atomic operations with biased locking and bulk rebiasing]
* https://www.javazhiyin.com/24364.html
* https://pdfs.semanticscholar.org/b8e4/cb0c212fd799522817b914ffcd24470f707e.pdf?_ga=2.218049237.2144104280.1590746224-418849090.1590746224</text>
      <sha1>awpffyagai73xw4hoz5pacyja0e29t0</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:理解Java并发(1)：基本机制</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2019-10-28T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>线程是操作系统中进行运算调度的最小单位，它是一个单一顺序的控制流，不论是对于单核还是多核的CPU，都能比较有效的提高程序的吞吐率。在Java中，创建一个线程的唯一方法是创建一个`Thread`类的实例，并调用`start()`方法以启动该线程。然而当多个线程同时执行时，如何保证线程之间是按照我们期待的方式在运行呢？Java提供了多种机制来保证多个线程之间的交互。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7476" sha1="ensoi2ppmqt69a8yp05u4lofd5q4fm3">线程是操作系统中进行运算调度的最小单位，它是一个单一顺序的控制流，不论是对于单核还是多核的CPU，都能比较有效的提高程序的吞吐率。在Java中，创建一个线程的唯一方法是创建一个`Thread`类的实例，并调用`start()`方法以启动该线程。然而当多个线程同时执行时，如何保证线程之间是按照我们期待的方式在运行呢？Java提供了多种机制来保证多个线程之间的交互。

= 同步(Synchronization)与监视器(Monitor)机制=
显而易见最基本最常见的和多线程有关的就是同步`synchronized`关键字了，它底层是使用Monitor实现的。那么究竟什么是`Monitor`呢？根据JavaSE Specification的描述，在Java中，每一个对象都有一个与之关联的monitor，允许线程可以去`lock`或者`unlock`这个monitor。实际上：

* `monitor`是独立于Java语言之上的一个概念（没想到还有另外一个名字`管程`），保证在运行线程之前获取互斥锁
* 在Java中，任何对象(`java.lang.Object`)都可以允许作为一个monitor，所以会有`wait`、`notify`之类的方法

`synchronized`可以作用于代码块或者方法上。如果作用在代码块上，它会尝试去lock这个对象的monitor，如果不成功将会等待直到lock成功。而当执行完毕后，无论是否出现异常，都将会释放这个锁。

如果作用在方法上，唯一的区别在于，如果是实例方法，那么将使用这个实例作为monitor，也就是`this`；如果是静态方法，那么使用的是所在类的`Class`对象。

= Wait/Notify=
每一个Object都包含一个等待线程的集合(Wait set)。当对象创建的时候，这个队列是空的，当调用`Object.wait()`、`Object.nofity()`以及`Object.nofityAll()`方法的时候，会自动添加或者移除队列中的线程。或者当线程的中断状态发生改变的时候，也会引起变化。

== Wait==
调用`wait`方法将使当前线程休眠直到另一个线程通过`notify`或者`notifyAll`来唤醒。当前线程必须持有该对象的锁，调用`wait`后即释放锁。当线程被唤醒时，需要重新取得锁并继续执行。然而，线程被唤醒有可能是因为“虚假唤醒”（spurious wakeups）导致，所以通常都需要将`wait`检测的逻辑包括在一个loop中：

&lt;syntaxhighlight lang="java"&gt;
synchronized (obj) {
    while (&lt;condition does not hold&gt;)
        obj.wait();
    // Perform action appropriate to condition
}
&lt;/syntaxhighlight&gt;
所谓虚假唤醒就是说，本来不该唤醒的时候唤醒了。究其原因是在操作系统层面就性能和正确性做出了权衡，放弃了正确性而选择让程序自己去处理。

&gt; Spurious wakeups may sound strange, but on some multiprocessor systems, making condition wakeup completely predictable might substantially slow all condition variable operations.

== Notify==
调用`notify`将唤醒一个正在等待持有该对象锁的线程，如果有多个对象在等待的话，将会随机唤醒其中的一个。

被唤醒的线程必须等到当前线程释放锁之后，才能开始执行；也就是说`notify`执行完之后，并不会立即释放锁，而是需要等到同步块执行完。

如果调用`notifyAll`的话，所有等待的线程将被唤醒，但同一时间有且仅有一个线程能取到锁并继续执行。

== Interruption==
当调用`Thread.interrupt`时，线程的中断状态呗设置为true。如果该线程在某个对象的waitSet中，则将会被从等待队列中移除，并在取得锁之后抛出`InterruptedException`。实际上，如果线程正在执行的是一些底层的blocking函数例如`Thread.sleep()`, `Thread.join()`, 或者 `Object.wait()`的时候，那么线程将抛出`InterruptedException`，并且`interrupted`状态会被清除；否则只会将`interrupted`状态设置为`true`。

如果一个处于等待队列中的线程同时收到中断和通知，那么可能的行为是：

* 先收到通知，正常唤醒。这时候，`Thread.interrupted`将为`true`，
* 抛出`InterruptedException`并退出

同样，如果有多个线程处于对象m的等待队列中，然后另一个线程执行`m.notify`，那么可能：

* 至少有一个线程正常退出wait
* 所有处于等待队列中的线程抛出`InterruptedException`而退出

需要注意的是，当一个线程中断了另一个线程的时候，被中断的线程并不是需要立即停止执行，程序可以选择在停止之前做一些清理工作之类的。通常如果捕获了`InterruptedException`只需要重新抛出即可，有些时候不能重新抛出的时候，需要将当前线程标记为`interrupted`使得上层堆栈的程序可以选择处理，

&lt;syntaxhighlight lang="java"&gt;
try {
    while (true) {
        Task task = queue.take(10, TimeUnit.SECONDS);
        task.execute();
    }
}catch (InterruptedException e) { 
    Thread.currentThread().interrupt();
}

&lt;/syntaxhighlight&gt;

= 线程的生命周期=
每一个线程有一个生命周期，包含多个状态：

* New: 线程创建还未开始执行，线程创建完之后即为此状态
* Runnable: 在JVM中正在执行的状态。当线程start之后，即变为runnable状态
* Blocked: 线程等待获取锁而被阻塞
* Waiting: 线程等待其他线程
* Timed Waiting: 有超时的等待
* Terminated: 线程已被退出

[[File:https://www.baeldung.com/wp-content/uploads/2018/02/Life_cycle_of_a_Thread_in_Java.jpg|600px|Life cycle of a thread]]

= Sleep / Yield=

当调用线程的`sleep`方法将导致线程暂时停止执行，值得注意的是并不会释放锁。而当线程的`yield`方法被调用时，意味着通知CPU当前线程可以“暂缓”执行的，实际很少使用。

&gt;It is rarely appropriate to use this method. It may be useful
	      for debugging or testing purposes, where it may help to reproduce
	      bugs due to race conditions. 

= Context switching=

在多线程中，CPU会为每个线程分配时间片区执行，即执行当前线程的一部分操作之后，操作系统需要从当前线程切换到其他线程中去。通常在下列的情况下会出现context switching:

* 多任务处理（即多个线程正常执行）
* 中断， 


那么在这个切换的过程中，会发生一些什么事情呢？



参考：

* [https://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html Chapter 17. Threads and Locks]
* [https://stackoverflow.com/questions/3362303/whats-a-monitor-in-java What's a monitor in Java?]
* [https://zh.wikipedia.org/wiki/%E7%9B%A3%E8%A6%96%E5%99%A8_(%E7%A8%8B%E5%BA%8F%E5%90%8C%E6%AD%A5%E5%8C%96 管程])
* [https://stackoverflow.com/questions/1050592/do-spurious-wakeups-in-java-actually-happen Do spurious wakeups in Java actually happen?]
* [https://stackoverflow.com/questions/8594591/why-does-pthread-cond-wait-have-spurious-wakeups Why does pthread_cond_wait have spurious wakeups?]
* [https://www.ibm.com/developerworks/java/library/j-jtp05236/index.html Dealing with InterruptedException]
* [https://docs.oracle.com/javase/7/docs/api/java/lang/Thread.State.html Enum Thread.State]
* [https://www.baeldung.com/java-thread-lifecycle Life Cycle of a Thread in Java]
* [https://en.wikipedia.org/wiki/Context_switch Context switch]
* [https://blog.tsunanet.net/2010/11/how-long-does-it-take-to-make-context.html How long does it take to make a context switch?]</text>
      <sha1>ensoi2ppmqt69a8yp05u4lofd5q4fm3</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:阅读笔记：HashMap</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-03-18T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>HashMap可以算是最常用的数据结构了，而它的实现没想到还挺有学问在里面。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7496" sha1="4culwf6f91ywj9p44wdtobfcu8hlc7x">HashMap可以算是最常用的数据结构了，而它的实现没想到还挺有学问在里面。


= 基本实现=
== 哈希映射==
在HashMap中使用数组来存储元素，根据元素的hash值一一映射到一个节点上。其中使用的哈希方法为：

&lt;syntaxhighlight lang="java"&gt;
static final int hash(Object key) {
    int h;
    // 将哈希值无符号右移16位是因为取index使用了length作为掩码，这样当哈希值在掩码外的部分相同的时候就会发生冲突
    // 这样将高位混杂到低位上，可以尽可能将这种影响消除
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);
}
&lt;/syntaxhighlight&gt;
举例来说对于容量为4的HashMap，插入"a"、"b"、"c"、"d"后在数组中的分布就是：

&lt;syntaxhighlight lang="lua"&gt;
"a".hashCode() = 97 = 00000000000000000000000001100001
hash("a") = 00000000000000000000000001100001 ^ 00000000000000000000000000000000
          = 00000000000000000000000001100001
index = hash("a") &amp; (4 - 1)
      = 00000000000000000000000001100001 &amp; 00000000000000000000000000000011
      = 00000000000000000000000000000001
&lt;/syntaxhighlight&gt;
如此则数组中对应的序号为1，2，3，0。

[[File:HashMap-resize.png|600px|HashMap resize]]

== Load Factor(负载因子)和Threshod（阈值）==
因为HashMap的底层实际上是使用数组进行存储，那么始终存在着一个动态内存分配的问题：数组的大小是固定的，但是HashMap实际存储多少数据是未知的（可以一直向HashMap中进行插入），那么当数组塞满了（实际上还有一个问题是发生哈希冲突）之后如何处理？

解决这个问题最简单的做法就是，一旦数组满了之后，就对数组进行扩容。扩容也很简单，重新申请一个大一点的数组，再把原来数组里面的数据复制过去即可。这里涉及到另外一个问题就是，扩容的时候选择一个怎么样的容量进行扩容呢？这个操作是有代价的，如果频繁的扩容就涉及到频繁的数组复制操作，性能上会受到影响；如果一次扩容选择一个很大的空间，但实际之后这些空间又没有使用到，那么久造成了资源浪费。怎么解决这一个问题呢？

在HashMap的构造中有两个关键的参数：

* `initialCapacity`:初始化容量，即可以装多少条数据
* `loadFactor`：负载因子，用来描述HashMap中可以变得多“满”（到达什么程度开始扩容）

实际上，HashMap并不会根据你提供的`initial capacity`来初始化一个数组，而是找到一个值 $t$ 并满足 $t &gt;= i \&amp;\&amp; t==2^{n}$（比如3对应得到4， 15对应得到16），并在第一次插入的时候进行初始化。

为什么数组在初始化的时候一定是2的倍数？这是因为方便扩容的时候直接将数组大小变成原来的二倍，同时也简化了一些其他的操作，比如如何定位到一个值所在的索引:

&lt;syntaxhighlight lang="java"&gt;
int index = (length - 1) &amp; hash

/*
final Node&lt;K,V&gt; getNode(int hash, Object key) {
    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k;
    if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp;
        (first = tab[(n - 1) &amp; hash]) != null) {
    ...
*/
&lt;/syntaxhighlight&gt;
正常的做法是，`abs(hash) % SIZE`像这样取余操作。但是如果除数是2的n次幂，则可以简化为位运算操作。

而至于为什么默认的负载因子是0.75，有人根据二项式分布算出最佳的load factor是 $log(2)=0.693$ ，然后拍脑袋给出的0.75（乘以容量还可以得个整数...)。

= 树化（红黑树）=
== TREEIFY_THRESHOLD（树化阈值）==
所以使用0.75作为负载因子，那么出现的情况是如果当前容量达到这个值的时候就会resize到原来的两倍。对于一个容量为4的Map来说，理想情况下元素均匀分布，是这样：

&lt;pre&gt;
最好情况                                 极端情况
bucket | elements                      bucket | elements     
-------+---------                      -------+---------    
     0 | Z                                  0 |   
     1 | X                                  1 | Z -&gt; X -&gt; Y 
     2 |                                    2 |  
     3 | Y                                  3 | 

&lt;/pre&gt;

理想状况下（假设基于随机hash算法节点在桶中均匀分布，且节点的个数占桶的50%，那么单个节点出现在桶中的概率为0.5），节点在hash桶中的出现的频率遵循[https://zh.wikipedia.org/wiki/%E6%B3%8A%E6%9D%BE%E5%88%86%E4%BD%88 泊松分布]（ $λ = 0.5$ )

$$
P(X=k)=\frac{e^{-\lambda}\lambda^k}{k!}=\frac{e^{-0.5}0.5^k}{k!}
$$

意味着在load factor=0.75的情况下，hash桶中出现 $k$ 个节点（冲突）的概率大致为：

&lt;pre&gt;
* 0:    0.60653066
* 1:    0.30326533
* 2:    0.07581633
* 3:    0.01263606
* 4:    0.00157952
* 5:    0.00015795
* 6:    0.00001316
* 7:    0.00000094
* 8:    0.00000006
* more: less than 1 in ten million
&lt;/pre&gt;

可见哈希冲突导致一个桶中出现8个节点情况已经几乎小之又小的事情了，这是`TREEIFY_THRESHOLD = 8`的原因，当大于8的时候转换为红黑树。

== Treeify（树化）==
通常情况下，当哈希冲突产生的时候，会被当成链表存储。这个改变是通过[http://openjdk.java.net/jeps/180 JEP 180: Handle Frequent HashMap Collisions with Balanced Trees]引入的。在下面的情况下，会转换为红黑树：

* 链表中的节点数达到TREEIFY_THRESHOLD（8）
* 容量至少达到MIN_TREEIFY_CAPACITY（64），否则只是单纯扩容到到原来的两倍

现实中哈希冲突的场景并不多，不过如果非要测试这种场景也很容易。比如`Aa`和字符串`BB`就拥有相同的哈希值，把他们随机组合到一起，还是一样。于是我们构建了很多个哈希值相同的key值，来演示哈希冲突的场景：


[[File:HashMap-treeify.png|600px|Treeify]]

== 尾插入==

从上面的图可以注意到：哈希冲突的节点在链表中是插入到链表尾部的

在Java8之前是插入到前面的，但是Java8改成插入到尾部了，这样做的原因（据说）是因为扩容时会改变链表的顺序，在多线程条件下会导致形成闭环（从而可能引起死循环）。

= fail-fast机制=
在HashMap中存在一个变量记录修改的次数`modCount`,当这个次数和期待的不一致的时候就会抛出`ConcurrentModificationException`。这种机制被称之为"Fail-Fast”，意味着出现错误的时候尽早结束。通常在`java.util`下面的迭代器都是这类的，如果在迭代的中途数据被其他线程修改了，那么就会（尽可能的，当然并不能保证）触发这个检测。

而`java.util.concurrent`包下的迭代器是"Fail-Safe"的，例如ConcurrentHashMap、CopyOnWriteArrayList等。

= 性能分析=
HashMap对于`get`和`put`操作的复杂度是常数级 $\displaystyle{O(1)}$ ，在最坏的情况下，因为使用了红黑树进行查找，复杂度为 $\displaystyle{O(log(n))}$ 。

* [https://stackoverflow.com/questions/20448477/cant-understand-poisson-part-of-hash-tables-from-sun-documentation Can't understand Poisson part of Hash tables from Sun documentation]
* [https://stackoverflow.com/questions/10901752/what-is-the-significance-of-load-factor-in-hashmap What is the significance of load factor in HashMap?]
* [http://rabbit.eng.miami.edu/class/een318/poisson.pdf Testing a Hash Function using Probability]</text>
      <sha1>4culwf6f91ywj9p44wdtobfcu8hlc7x</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:理解Java并发(3)：CAS</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-05-20T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>CAS：Compare and Swap，即比较再交换。jdk5增加了并发包java.util.concurrent.*,其下面的类使用CAS算法实现了区别于synchronouse同步锁的一种乐观锁。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3909" sha1="2zj2uey1n79ah7f5up3q1tc0kbcr7pu">
CAS：Compare and Swap，即比较再交换。jdk5增加了并发包java.util.concurrent.*,其下面的类使用CAS算法实现了区别于synchronouse同步锁的一种乐观锁。

= 实现机制=

在Java中很多原子操作是通过CAS实现的，是一种无锁的方式，比如AtomicIntger的自增操作：

&lt;syntaxhighlight lang="java"&gt;
public class AtomicInteger extends Number implements java.io.Serializable {
    private static final long serialVersionUID = 6214790243416807050L;

    // setup to use Unsafe.compareAndSwapInt for updates
    private static final Unsafe unsafe = Unsafe.getUnsafe();
    private static final long valueOffset;

    static {
        try {
            valueOffset = unsafe.objectFieldOffset
                (AtomicInteger.class.getDeclaredField("value"));
        } catch (Exception ex) { throw new Error(ex); }
    }

    private volatile int value;
    // ...
    public final int getAndIncrement() {
        return unsafe.getAndAddInt(this, valueOffset, 1);
    }
    // ...
}

&lt;/syntaxhighlight&gt;
这里的操作调用了Unsafe类来实现，而这个Unsafe类里面的实代码是这样的：

&lt;syntaxhighlight lang="java"&gt;
public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5);
public final int getAndAddInt(Object var1, long var2, int var4) {
    int var5;
    do {
        var5 = this.getIntVolatile(var1, var2);
    } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4));
    return var5;
}
&lt;/syntaxhighlight&gt;

而最终是用到了native的方法compareAndSwapInt，这部分的源码可以在OpenJDK源码中找到，而最终是会调用到[https://www.felixcloutier.com/x86/cmpxchg CMPXCHG]这样到CPU指令来完成CAS。

&lt;syntaxhighlight lang="c++"&gt;
UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSwapInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x))
  UnsafeWrapper("Unsafe_CompareAndSwapInt");
  oop p = JNIHandles::resolve(obj);
  jint* addr = (jint *) index_oop_from_field_offset_long(p, offset);
  return (jint)(Atomic::cmpxchg(x, addr, e)) == e;
UNSAFE_END
&lt;/syntaxhighlight&gt;

其中，源码位于：

* [http://hg.openjdk.java.net/jdk8/jdk8/hotspot/file/tip/src/share/vm/prims/unsafe.cpp openjdk8/unsafe.cpp]
* [http://hg.openjdk.java.net/jdk8/jdk8/hotspot/file/tip/src/share/vm/runtime/atomic.cpp openjdk8/atomic.cpp]

= ABA问题=

使用CAS一个问题就是可能出现A-B-A问题，因为CAS到逻辑就是，假设更新到时候判断出值跟期望的一致那么就会进行更改；但实际值跟期望的一致并不能代表值没有发生过变化。可能的场景就是，值被改成别的然后又改回来了，就是从A-B，然后又变成A。

解决这个问题的思路在于，想办法标记出变化。通常的做法是利用版本号（而不是值），保证版本号每次都会变化。JDK中提供了AtomicStampedReference来解决A-B-A问题，实现如下：

&lt;syntaxhighlight lang="java"&gt;
 public boolean compareAndSet(V   expectedReference,
                              V   newReference,
                              int expectedStamp,
                              int newStamp) {
     Pair&lt;V&gt; current = pair;
     return
         expectedReference == current.reference &amp;&amp;
         expectedStamp == current.stamp &amp;&amp;
         ((newReference == current.reference &amp;&amp;
           newStamp == current.stamp) ||
          casPair(current, Pair.of(newReference, newStamp)));
 }
&lt;/syntaxhighlight&gt;

References:

* https://docs.huihoo.com/javaone/2006/java_se/JAVA%20SE/TS-3412.pdf
* https://fliphtml5.com/tzor/bqxz/basic
* https://www.artima.com/insidejvm/ed2/index.html
* [https://wiki.openjdk.java.net/display/HotSpot/Synchronization OpenJDK Wiki - Synchronization]
* https://www.zhihu.com/question/53826114
* [https://blogs.oracle.com/dave/biased-locking-in-hotspot Biased Locking in HotSpot]
* http://gee.cs.oswego.edu/dl/jmm/cookbook.html</text>
      <sha1>2zj2uey1n79ah7f5up3q1tc0kbcr7pu</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:关于 Java泛型</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2017-12-21T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>泛型是Java1.5之后一个比较有用的特性，有点类似于C++的模板。最简单的一个例子：

```java
class Wrapper&lt;T&gt; {
    final T data;

    Wrapper(T data) {
        this.data = data;
    }
}
```
有一些可能不是特别常用的Generics，我们来简单看一下。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7492" sha1="ejdcs1lxkmlrljkfre4sish9c07hdhe">泛型是Java1.5之后一个比较有用的特性，有点类似于C++的模板。最简单的一个例子：

&lt;syntaxhighlight lang="java"&gt;
class Wrapper&lt;T&gt; {
    final T data;

    Wrapper(T data) {
        this.data = data;
    }
}
&lt;/syntaxhighlight&gt;
有一些可能不是特别常用的Generics，我们来简单看一下。

= Bounded Generics=
== Multiple bound==
如果一个类继承了多个接口，是这样的写法：
&lt;syntaxhighlight lang="java"&gt;
interface I {}
interface M {}
abstract class C {}

class Foo extends C implements I,M {}
&lt;/syntaxhighlight&gt;
假如一个方法的泛型参数包含多个Bound，则要这样写了：
&lt;syntaxhighlight lang="java"&gt;
&lt;T extends I &amp; M&gt; void bar(T arg){}
&lt;T extends C&gt; void ooo(T arg){}
&lt;T extends C &amp; I &amp; M&gt; void xxx(T arg){}
&lt;/syntaxhighlight&gt;
== Unbounded wildcards==

使用 ? 修饰符可以用作类型转换，List&lt;?&gt; 意味着是一个未知类型的List，可能是`List&lt;A&gt;` 也可能是`List&lt;B&gt;`
&lt;syntaxhighlight lang="java"&gt;
private final List&lt;String&gt; strList = Arrays.asList("Hello", "World!");
private final List&lt;Integer&gt; intList = Arrays.asList(1, 2, 3);
private final List&lt;Float&gt; floatList = Arrays.asList(1.1f, 2.1f, 3.1f);
private final List&lt;Number&gt; numberList = Arrays.asList(1, 1.0f, 3000L);

public void cast() {
    List&lt;?&gt; unknownList = null;
    unknownList = strList;
    unknownList = intList;
    unknownList = floatList;
    unknownList = numberList;

    for (int i = 0; i &lt; unknownList.size(); i++) {
        // Number item = unknownList.get(i); wrong! 
        Object item = unknownList.get(i);
        System.out.println(item + "(" + item.getClass() + ")");
    }
}
/* output
1(class java.lang.Integer)
1.0(class java.lang.Float)
3000(class java.lang.Long)
*/
&lt;/syntaxhighlight&gt;
== Upper bounded wildcards==

&lt;syntaxhighlight lang="java"&gt;
public static double sumOfList(List&lt;? extends Number&gt; list) {
    double s = 0.0;
    for (Number n : list)
        s += n.doubleValue();
    return s;
}
//...
sumOfList(Arrays.asList(1, 2, 3));
sumOfList(Arrays.asList(1.0f, 2.0f, 3.0f));
&lt;/syntaxhighlight&gt;

== Lower bounded wildcards==

&lt;syntaxhighlight lang="java"&gt;
public static void addNumbers(List&lt;? super Number&gt; list) {
    for (int i = 1; i &lt;= 10; i++) {
        list.add(i);
        list.add(1.0f);
    }
}
addNumbers(new ArrayList&lt;Number&gt;());
&lt;/syntaxhighlight&gt;

= Type erase=
== Type erase process==

Java的泛型是编译时有效的，在运行时，所有泛型参数会被编译器擦除。擦除的规则如下：

* 如果参数是有Bound的，则会替换成这个Bound
* 如果是Unbounded，则会替换成Object

如下所示：

&lt;syntaxhighlight lang="java"&gt;
public class Node&lt;T&gt; {                         // public class Node {
    private T data;                            //     private Object data;
    private Node&lt;T&gt; next;                      //     private Node next;
    public Node(T data, Node&lt;T&gt; next) {        //     public Node(Object data, Node next) {
        this data = data;                      //         this data = data;
        this next = next;                      //         this next = next;
    }                                          //     }
                                               // 
    public T getData() { return data; }        //    	public Object getData() { return data; }
}                                              // }

public class Node&lt;T extends Comparable&lt;T&gt;&gt; {   // public class Node {
    private T data;                            //     private Comparable data;
    private Node&lt;T&gt; next;                      //     private Node next;
    public Node(T data, Node&lt;T&gt; next) {        //     public Node(Comparable data, Node next) {
        this.data = data;                      //         this.data = data;
        this.next = next;                      //         this.next = next;
    }                                          //     }
                                               // 
    public T getData() { return data; }        //     public Comparable getData() { return data; }
}                                              // }
&lt;/syntaxhighlight&gt;
== Bridge method==
按照上面的擦除也会带来问题。考虑下面的例子，如果有一个子类：
&lt;syntaxhighlight lang="java"&gt;
public class MyNode extends Node&lt;Integer&gt; {       // public class MyNode extends Node {
    public MyNode(Integer data) { super(data); }  //     public MyNode(Integer data) { super(data); }
                                                  // 
    public void setData(Integer data) {           //     public void setData(Integer data) {
        System.out.println("MyNode.setData");     //         System.out.println("MyNode.setData");
        super.setData(data);                      //         super.setData(data);
    }                                             //     }
}                                                 // }
&lt;/syntaxhighlight&gt;
然后，我们考虑如下的代码：
&lt;syntaxhighlight lang="java"&gt;
MyNode mn = new MyNode(5);                     // MyNode mn = new MyNode(5);
Node n = mn;                                   // Node n = (MyNode)mn;
n.setData("Hello");                            // n.setData("Hello");
Integer x = mn.data;                           // Integer x = (String)mn.data;
&lt;/syntaxhighlight&gt;
这里调用setData则会参数类型不能匹配。为了解决这个问题，Java编译器会生成一个Bridge method:
&lt;syntaxhighlight lang="java"&gt;
public void setData(Object data) {
    setData((Integer) data);
}
&lt;/syntaxhighlight&gt;

= Q&amp;A=

== List\&lt;?\&gt; vs List\&lt;Object\&gt;==

&gt;It's important to note that List&lt;Object&gt; and List&lt;?&gt; are not the same. You can insert an Object, or any subtype of &gt;Object, into a List&lt;Object&gt;. But you can only insert null into a List&lt;?&gt;.

== extends vs super==

实际上泛型仅仅是为了做一个编译时的检查，从逻辑上确保程序是类型安全的。假设我们有这样的类定义：
Object-&gt;Parent-&gt;T-&gt;Child
我们有这样几种写法：

* ```List&lt;?&gt;``` 代表一种未知类型的List，可能是```List&lt;Object&gt;```，也可能是```List&lt;Child&gt;```，都可以
* ```List&lt;? extends T&gt;``` 代表T或者T的子类的List，可以是```List&lt;T&gt;```，也可以是```List&lt;Child&gt; ```
* ```List&lt;? super T&gt;``` 代表T或者T的父类的List，可以是```List&lt;T&gt;，List&lt;Parent&gt;，List&lt;Object&gt;```

我们有一个事实就是，Child是一定可以转化T或者Parent的，但是一个T不一定能转化成Child，因为可能会是别的子类。
比如我们现在做两个列表的拷贝，
&lt;syntaxhighlight lang="java"&gt;
public static &lt;T&gt; void copy(List dest, List src)
&lt;/syntaxhighlight&gt;
想实现从一个列表拷贝到另一个列表，比如
&lt;syntaxhighlight lang="java"&gt;
List&lt;Parent&gt; parents;
List&lt;T&gt; ts;
List&lt;Child&gt; childs;
&lt;/syntaxhighlight&gt;
基于上面说的类的继承的事实，ts/childs显然是可以转化成parents的，但是ts无法确保能转化成childs。因此我们的拷贝方法要这样定义：
&lt;syntaxhighlight lang="java"&gt;
public class Collections { 
  public static &lt;T&gt; void copy  
  ( List&lt;? super T&gt; dest, List&lt;? extends T&gt; src) {  // uses bounded wildcards 
      for (int i=0; i&lt;src.size(); i++) 
        dest.set(i,src.get(i)); 
  } 
}
&lt;/syntaxhighlight&gt;
因为在desc.set()方法中，需要的是一个能够转化为T的对象的，src中&lt;? extends T&gt; 保证了src中的元素一定是一个T。

See also:

* [https://docs.oracle.com/javase/tutorial/java/generics/index.html Lesson: Generics (Updated)]</text>
      <sha1>ejdcs1lxkmlrljkfre4sish9c07hdhe</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Netty(1)：介绍</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-09-02T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>Netty是一个高性能的异步事件驱动的网络应用框架，本质上是对NIO进行了高层的抽象，使得可以轻松的创建服务器和客户端，极大简化了诸如TCP和UDP套接字的操作。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4306" sha1="5nbyarmi9uo4u3jyeq6jink642q24lh">Netty是一个高性能的异步事件驱动的网络应用框架，本质上是对NIO进行了高层的抽象，使得可以轻松的创建服务器和客户端，极大简化了诸如TCP和UDP套接字的操作。


= 入门=
== 核心概念==

[[File:https://netty.io/3.8/guide/images/architecture.png|600px|Netty architecture]]

Netty中的一些核心概念：

* `Channel`对应到Socket
* `EventLoop`用来控制流、多线程处理以及并发
* `ChannelFuture`用来实现异步通知

== ECHO server==
最简单的例子是构建一个echo server，发过来什么同样返回什么。

首先需要实现一个Handler，定义如何处理消息：
&lt;syntaxhighlight lang="java"&gt;
public class EchoServerHandler extends ChannelInboundHandlerAdapter {
    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {
        // handler需要去释放msg对象(引用计数）
        // 这里不用去手动release msg，因为writeAndFlush里面已经处理了
        ctx.writeAndFlush(msg);
    }

    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception {
        cause.printStackTrace();
        ctx.close();
    }
}
&lt;/syntaxhighlight&gt;

然后利用`ServiceBootStrap`来启动
&lt;syntaxhighlight lang="java"&gt;
public class EchoServer {
    private final int port;

    public EchoServer(int port) {
        this.port = port;
    }

    public void run() throws InterruptedException {
        EventLoopGroup bossGroup = new NioEventLoopGroup();
        EventLoopGroup workerGroup = new NioEventLoopGroup();
        try {
            ServerBootstrap bootstrap = new ServerBootstrap();
            bootstrap.group(bossGroup, workerGroup)
                    .channel(NioServerSocketChannel.class)
                    .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() {
                        @Override
                        protected void initChannel(SocketChannel ch) throws Exception {
                            ch.pipeline().addLast(new EchoServerHandler());
                        }
                    })
                    .option(ChannelOption.SO_BACKLOG, 128)
                    .childOption(ChannelOption.SO_KEEPALIVE, true);

            ChannelFuture f = bootstrap.bind(port).sync();
            f.channel().closeFuture().sync();
        } finally {
            workerGroup.shutdownGracefully();
            bossGroup.shutdownGracefully();
        }
    }

    public static void main(String[] args) throws InterruptedException {
        new EchoServer(9999).run();
    }
}
&lt;/syntaxhighlight&gt;
= 基本原理=
== EventLoop与Server Channel、Channel==
其运行原理如图：

[[File:https://dpzbhybb2pdcj.cloudfront.net/maurer/Figures/03fig04_alt.jpg|600px|EventLoopGroup and Channels]]

* 左边只有一个ServerChannel，代表服务器上监听某个端口的套接字，所以实际上也只需要一个EventLoop就可以了
* 右边代表建立的客户端连接，每一个连接都对应到一个EventLoop，当有很多个连接的时候，这些连接是会共享其中的EventLoop的。

== ChannelPipeline==

每次建立连接的时候，都会调用`ChannelInitializer`，这个类负责安装一些自定义的`ChannelHandler`到`ChannelPipeline`中。
实际上的程序可能对应到多个入站和出站的ChannelHandler，它们的执行顺序是由它们被添加的顺序所决定的，类似这样：

[[File:https://dpzbhybb2pdcj.cloudfront.net/maurer/Figures/03fig03_alt.jpg|600px|Channel Pipeline]]

== ChannelHandler==
ChanelHandler又有很多的类型，比如：

* 编码器和解码器, 例如`ByteToMessageDecoder`、`ProtobufEncoder`等
* `SimpleChannelInboundHandler&lt;T&gt;`，用来处理简单的逻辑比如收到消息后完成业务逻辑，只需要实现其中的`void channelRead0(ChannelHandlerContext ctx, I msg)`方法即可

== Bootstrap==
前面使用`ServerBootstrap`类来启动了服务器上的socket监听，如果是客户端程序可以使用`Bootstrap`类来完成。一个比较明显的区别就是，客户端程序只需要一个`EventLoopGroup`而服务端通常会需要两个。

Ref:

* [https://netty.io/wiki/user-guide-for-4.x.html Netty User guide for 4.x]
* [https://livebook.manning.com/book/netty-in-action Netty in Action]</text>
      <sha1>5nbyarmi9uo4u3jyeq6jink642q24lh</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Vert.X(1)：简介</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-12-07T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>最开始了解到Vert.X是在[Web Framework Benchmarks](https://www.techempower.com)中看到，性能超群的一个web框架，但说实话知名度不是特别高。

![Vert.X Benchmark](/images/Vert.x_Benchmark.png)</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4027" sha1="j948166op64kj9iryillti13rev1ube">最开始了解到Vert.X是在[https://www.techempower.com Web Framework Benchmarks]中看到，性能超群的一个web框架，但说实话知名度不是特别高。

[[File:Vert.x_Benchmark.png|600px|Vert.X Benchmark]]

而官网介绍也谦逊的很，甚至都说自己不算一个web框架，而是一个"tool-kit"：

&gt; Eclipse Vert.x is a tool-kit for building reactive applications on the JVM.

= 特点=

Vert.x采取的是事件驱动的非阻塞异步响应模型，而不是类似Servlet一样为每一个连接分配一个新的线程。这样做的好处就是可以利用少量的线程处理很多的并发连接。而采取blocking IO的线程在等待IO操作完成的时候，线程会被挂起，而后唤醒。但是这个操作本身也是有一定的开销的，当线程数很大的时候这个开销就尤为明显。

简单来说，Vert.x就是JVM版本的NodeJS，但一个比较大的区别就是NodeJS是单线程模型的，而Vert.x可以有多个event loop，因此能够更加有效的利用多核的优势。

Vert.x的另一个特点就是提供了多种语言的绑定（并不仅仅是简单的wrap一下，而且充分利用了各个语言的特点）：

* Java
* JavaScript
* Groovy
* Ruby
* Kotlin
* Scala

== 什么是响应式（Reactive）==

根据[https://www.reactivemanifesto.org/ The Reactive Manifesto]的定义，一个响应式的系统具有四个特点：
[[File:https://www.reactivemanifesto.org/images/reactive-traits.svg|600px|Reactive manifesto]]

* Responsive：The system responds in a timely manner if at all possible. 
* Resilient：The system stays responsive in the face of failure.
* Elastic: The system stays responsive under varying workload. 
* Message-driven: Reactive Systems rely on asynchronous message-passing to establish a boundary between components that ensures loose coupling, isolation and location transparency.


== 组件==

Vert.x又包含了很多个部分：

=== Web组件===

* Core: 包含底层的Http/TCP、文件等的访问功能。
* Web: 可以用来创建Web应用和微服务
* Web Client: http请求客户端
* Web API Contract: 用来实现契约先行的开发模式以及契约测试
* 其他: Web API Service, Web GraphQL Handler等，不过目前都还在Technical Preview阶段

=== 数据访问===
数据访问组件提供了一系列的异步访问client，当然也可以直接使用原始的数据库驱动。支持的数据库有：

* MongoDB client
* Redis client
* Cassandra client
* SQL Common
* JDBC client
* Reactive MySQL/DB2/PostgreSQL client(Technical preview)

=== Reactive===
提供了各种创建响应式应用程序的组件。

* Vert.x Rx: 不喜欢回调可以使用RxJava风格的API
* Reactive streams: 可以与Akka/Project Reactor等其他reactive系统交互
* Vert.x Sync: 用来部署使用fiber(纤程，一种轻量级的线程)的节点，可以编写串行化风格的代码
* Kotlin coroutines: 携程的支持，可以使用`async/await`或者channels。

=== Microservices===
创建微服务的组件：

* service discovery
* circuit breaker
* config

=== MQTT===
提供了MQTT的server和client端组件。

=== Authentication and Authorisation===
认证授权相关：

* Auth common
* JDBC auth
* JWT auth
* Shiro auth
* MongoDB auth
* OAuth2
* .htdigest Auth

=== Messaging===

* AMQP client(Technical preview)
* STOMP client &amp; Server
* RabbitMQ client
* AMQP bridge

=== 其他===

* Kafka client
* Mail client: SMTP 客户端
* Consul client
* JCA Adaptor
* Event Bus bridge:TCP/Apache camel
* Health check
* Metrics
* Shell
* Docker
* Vert.x Unit
* ... 

= 使用=
== Hello World==

&lt;syntaxhighlight lang="java"&gt;
public class VertxEcho {
    public static void main(String[] args) {
        Vertx vertx = Vertx.vertx();

        vertx.createNetServer()
            .connectHandler(socket -&gt; {
                socket.handler(buffer -&gt; {
                    socket.write("Hello:" + buffer);
                });
            })
            .listen(3000);
    }
}
&lt;/syntaxhighlight&gt;</text>
      <sha1>j948166op64kj9iryillti13rev1ube</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:B-Tree算法</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2018-12-18T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>B-Tree(区别于二叉树)是一种平衡多叉搜索树。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9119" sha1="nlel7gmbzaorlrvrs1o35jvyqowhn18">B-Tree(区别于二叉树)是一种平衡多叉搜索树。

= B-Tree的定义=

根据Knuth的定义，${\displaystyle m}$阶的B-Tree有如下的特性：

=. 节点左边的元素都比它小，节点右边的元素都比它大=
=. 每个节点最多有${\displaystyle m}$个子节点=
=. 除了根节点之外，非叶节点（没有孩子的节点）至少有${\displaystyle m/2}$个子节点=
=. 如果根节点不是叶子节点则其至少有两个子节点=
=. 包含${\displaystyle k}$个子节点的节点共有${\displaystyle k-1}$个键=
=. 所有的叶子节点的高度相同=

一般表示B-Tree有两种表示方法：

* B-Tree of order ${\displaystyle d}$ or ${\displaystyle M}$
* B-Tree of degree or ${\displaystyle t}$

== Kuath: B-Tree of Order ${\displaystyle d}$==
其中${\displaystyle M=5}$ 表示每一个节点中*至多有5个子节点*；则有如下的特性：
$$
\begin{align}
&amp;Max(children) = 5 \\
&amp;Min(children) = ceil(M/2) = 3 \\
&amp;Max(keys) = Max(children) - 1 = 4 \\
&amp;Min(keys) = Min(children) - 1 = 2
\end{align}
$$

== CLRS: B-Tree of min degree ${\displaystyle t}$==
而${\displaystyle t=5}$ 则定义了一个节点中*至少有5个子节点*
$$
\begin{align}
&amp;Max(children) = 2t = 10 \\
&amp;Min(children) = t = 5 \\
&amp;Max(keys) = 2t -1 = 9 \\
&amp;Min(keys) = t - 1 = 4
\end{align}
$$

这里degree指的是一个节点中子节点的数目。CLRS中用的最小度，即节点中最少有多少个子节点。例如t=2即表示的是2-3-4 tree，每个子节点中可以有2、3或者4个children。

== CS673: B-tree of maximum degree k==
而也有使用Max degree的，例如[https://www.cs.usfca.edu/~galles/visualization/BTree.html DB Virtualization]里面，使用的就是最大度（即最多有k个子节点），则：

* 所有的interior node最少有k/2个children，最多有k个
* 2-3树即对应到maximum degree of 3

== 对应关系==
这些表示方法的对应关系如下：

Kunth Order, k   CLRS min degree  CS673 max defree (min, max) children
---------------- ---------------- ---------------- --------------------
k=0              -                -                -
k=1              -                -                -
k=2              -                -                -
k=3              -                t=3              (2, 3)
k=4              t=2              t=4              (2, 4)
k=5              -                t=5              (3, 5)
k=6              t=3              t=6              (3, 6)
k=7              -                t=7              (4, 7)
k=8              t=4              t=8              (4, 8)
k=9              -                t=9              (5, 9)
k=10             t=5              t=10             (5, 10)

可见Order实际等价于max degree。

= 算法复杂度=

根据B-Tree的定义（Min Degree t)，如果Btree的高度为${\displaystyle h}$, 考虑最少含有多少个key, 则当：

* Root节点包含1个key
* 其他所有节点有且仅有有${\displaystyle t-1}$ 个key

这种场景时，所包含的key最少：

[[File:btree_height_3.gif|600px|Btree of height 3]]

设${\displaystyle S_{h}}$为Btree第h层的节点数，容易看出:

* 当${\displaystyle depth=0}$ 时，${\displaystyle S_{0}=1}$
* 当${\displaystyle depth=1}$ 时，${\displaystyle S_{1}=2}$
* 当${\displaystyle depth=2}$ 时，${\displaystyle S_{2}=2\cdot t}$
* 当${\displaystyle depth=h}$ 时，${\displaystyle S_{h}=2\cdot t^{h-1}}$

从${\displaystyle h=1}$开始，每一层的key数目即${\displaystyle S(key)_{h}=S_{h}\cdot (t-1)}$，根据等比数列求和公式即可算出总的key数目为：

$$
\begin{aligned}	 
Min(keys) &amp;=1 + \sum_{i=1}^{h}{(t-1)\cdot 2t^{i-1}} \\
    &amp;=1 + (t-1)\sum_{i=1}^{h}{2t^{i-1}} \\
    &amp;=1 + 2(t-1)\sum_{i=1}^{h}{t^{i-1}} \\
    &amp;=1 + 2(t-1){\Big(\frac{1-t^h}{1-t}\Big)} \\
    &amp;=2t^h-1
\end{aligned}
$$

设${\displaystyle n}$ 为B-Tree的所有key数，则有：

$$
\begin{aligned}	
n &amp;\geq Min(keys) \\
  &amp;=2t^h-1
\end{aligned}
$$

可以得：

$$
h \leq log_{t}\frac{1+n}{2}
$$

其算法时间和空间复杂度如下：

             平均                最坏
------------ ------------------ ------------------
空间复杂度     $O(n)$             $O(n)$
查找          $O(log \quad n)$   $O(log \quad n)$
插入          $O(log \quad n)$   $O(log \quad n)$
删除          $O(log \quad n)$   $O(log \quad n)$

= B-tree算法实现=
== search操作==

根据B-tree的定义，左边的key都比其小，右边皆比其大，则不应该存在重复的key。查找算法类似于2叉树的查找，步骤如下：

* 从根节点开始，依次同节点中${\displaystyle k_{i}}$进行比较，如果大于或者等于${\displaystyle k_{i}}$则停止
* 如果找到相等的key，则停止搜索
* 如果没有找到，则到下一级节点中进行查找；如果已经是叶子节点，则查找结束

[[File:Btree-order-5-search.png|600px|在Btree中查找"5"]]

通常当${\displaystyle M}$较小时，我们在节点中查找的时候只需要进行顺序查找即可；如果较大的情况下，可以进行二分查找提高搜索的效率。

&lt;syntaxhighlight lang="ruby"&gt;
B-TREE-SEARCH(x, k)
  i ← 1
  while i ≤ n[x] and k ≥ key[x, i]
    do i ← i + 1
  if i ≤ n[x] and k = key[x, i]
    then return (x, i)
  if leaf[x]
    then return NIL
    else c = DISK-READ(c[x, i])
      return B-TREE-SEARCH(c, k)
&lt;/syntaxhighlight&gt;

== insert操作==
=== 节点的分裂===
在进行insert之前，需要考虑的就是，btree规定了一个节点中最大的child的数目，当一个节点中子节点的数目超过允许的最大值的时候，需要将节点拆分为两个。例如上面的例子，如果再插入20的话，如果直接插入则子元素已经超出了最大允许的数目：

[[File:Btree-order-5-split.png|600px|在Btree中插入"20"]]

在上面的例子中，拆分之后，两个子节点的元素个数正好是平均的，但是，如果order为偶数的情况下是不平均的：

[[File:Btree-order-4-split.png|600px|在Btree中插入"6"]]

值得注意的是，因为每次分裂高度会增加，同时会增加父元素的key个数，那么也可能导致父节点满。所以如果上层节点也满了的话，也是需要递归的分裂的：

[[File:Btree-order-4-multi-split.png|600px|在Btree中插入"10"]]

=== 节点插入过程===

当order为奇数时，插入A~Q的过程如下：

[[File:btree_order_5_insert.png|600px|btree of order 5]]

当order为偶数时，插入A-J的过程如下：

[[File:btree_order_4_insert.png|600px|btree of order 4]]

在Insert的过程中，一般的做法是先将元素插入到叶节点，这时候如果发现叶节点满了，需要将其Split，并将其中一个key提升到父节点中。同时，需要看父节点是否满，如果满了也需要进行拆分，直到根节点。但是这种做法需要插入后再回溯，比较难以实现。[http://staff.ustc.edu.cn/~csli/graduate/algorithms/book6/chap19.htm 另一种方式]则是在插入的过程中，一旦发现节点已经满了，无法再容纳元素，则先将其拆分，然后再继续朝下查找。这样只需要查找一次，再最后插入到叶子节点的时候，能够保证不会溢出。

=== 抢占式分裂（Preemtive Split）===

如上所说，在insert操作的时候，是先插入元素，然后再进行拆分的，这样可能插入之后还需要一直递归到上层节点进行拆分。例如下面的一个场景：

[[File:btree_order_4_insert_normal.png|600px|btree of order 4]]

而Preemtive Split正是在insert之前即进行拆分，当发现一个节点快要满了的时候，就先split之后再插入，自顶向下，不需要再回溯到上一层的节点。

[[File:btree_order_4_insert_preemtive.png|600px|btree of order 4]]

从上面的例子可以看到，两种方式构造出的Btree在插入I之后其实是不大一样的，而当J插入之后则变成一致了。

== 创建一个空的B-tree==

&lt;syntaxhighlight lang="ruby"&gt;
B-TREE-CREATE(T)
  x ← ALLOCATE-NODE()
  leaf[x] ← TRUE
  n[n] ← 0
  DISK-WRITE(x)
  root[T] ← x
&lt;/syntaxhighlight&gt;

== 删除操作==


= B-Tree变种=

* 2-3-4树：Order为4的B-tree又被称之为2-3-4 tree (每个非叶子节点有2个、3个或者4个子节点)
* 2-3树：Order为3的B-tree又被称之为2-3 tree (每个非叶子节点有2个或者3个子节点)
* B+-tree：A B-tree in which keys are stored in the leaves. 
* B*-tree：A B-tree in which nodes are kept 2/3 full by redistributing keys to fill two child nodes, then splitting them into three nodes. 

参考资料:

* [https://en.wikipedia.org/wiki/B-tree Wikipedia - B-tree]
* [https://www.cs.usfca.edu/~galles/cs673/lecture/lecture11.pdf Graduate Algorithms CS673-2016F-11 B-Trees]
* [https://xlinux.nist.gov/dads/HTML/btree.html NIST B-tree]
* [http://staff.ustc.edu.cn/~csli/graduate/algorithms/book6/chap19.htm CHAPTER 19: B-TREES]
* [https://stackoverflow.com/questions/28846377/what-is-the-difference-btw-order-and-degree-in-terms-of-tree-data-structure What is the difference btw “Order” and “Degree” in terms of Tree data structure]</text>
      <sha1>nlel7gmbzaorlrvrs1o35jvyqowhn18</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Burrows-Wheeler变换(Burrows–Wheeler Transform)</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-03-04T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>最近听一个医学专业的同学提到了在进行基因分析中用到BWT算法，觉得挺有意思的，正巧赶上这次疫情在家，于是想研究一下这个算法。这个算法的核心思想在于，调整原来的字符串中字符的顺序（而不改变其长度及内容）从而更多的将重复的字符排列到一起，这样有助于其他的压缩算法获得更高的压缩比。这个算法在基因分析中大有用处也就顺理成章了，想想DNA的双链表示大概都是G-T-A-C会有很多这样的字符，那么运用BWT应该可以有比较好的效果。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9854" sha1="gf36di051u9h3tn70zlp7s27xn2z64m">最近听一个医学专业的同学提到了在进行基因分析中用到BWT算法，觉得挺有意思的，正巧赶上这次疫情在家，于是想研究一下这个算法。这个算法的核心思想在于，调整原来的字符串中字符的顺序（而不改变其长度及内容）从而更多的将重复的字符排列到一起，这样有助于其他的压缩算法获得更高的压缩比。这个算法在基因分析中大有用处也就顺理成章了，想想DNA的双链表示大概都是G-T-A-C会有很多这样的字符，那么运用BWT应该可以有比较好的效果。

= 算法实现=
考虑一个字符串，要想将相同的字符排列到一起，那么最简单的办法就是，将字符串中的字符进行排序。可是单纯的排序之后，虽然还是那么多字符，但是丢失了一个重要的信息就是字符原来的顺序，而BWT的核心思想就是在于排序并想办法保存字符的顺序信息。

== 编码==
编码方式如下：

* 将 ${\displaystyle \$}$ 作为字符串结尾标记加入到原字符串（记为 ${\displaystyle S}$ )末尾
* 将字符串从左到右进行轮换，对于一个长度为 ${\displaystyle N}$ 的字符串，产生 ${\displaystyle N}$ 个新的字符串，记为 ${\displaystyle S_{n}}$ 
* 将 ${\displaystyle S, S_{1}...S_{n}}$ 进行字典序排序， ${\displaystyle \$}$ 权值最小放在最前面，也即 ${\displaystyle S_{n}}$ 在第一个
* 取排序后的所有字符串的最后一个字符，生成一个新的字符串（记为${\displaystyle S^{"}}$ )，即编码完成

以字符串`banana`举例来说：

$$
S = banana, N = 6\\
\begin{cases}
S_{0} = banana\color{blue}{$}\\
S_{1} = anana\color{blue}{$}b\\
S_{2} = nana\color{blue}{$}ba\\
S_{3} = ana\color{blue}{$}ban\\
S_{4} = na\color{blue}{$}bana\\
S_{5} = a\color{blue}{$}banan\\
S_{6} = \color{blue}{$}banana
\end{cases}
\xrightarrow[\text{字典序排序}]{}
\begin{cases}
S_{6} = \color{blue}{$}banana\\
S_{5} = a\color{blue}{$}banan\\
S_{3} = ana\color{blue}{$}ban\\
S_{1} = anana\color{blue}{$}b\\
S_{0} = banana\color{blue}{$}\\
S_{4} = na\color{blue}{$}bana\\
S_{2} = nana\color{blue}{$}ba
\end{cases}
\xrightarrow[\text{获取最后一个字符}]{}
\begin{cases}
S_{6} = $banan\color{red}{a}\\
S_{5} = a$bana\color{red}{n}\\
S_{3} = ana$ba\color{red}{n}\\
S_{1} = anana$\color{red}{b}\\
S_{0} = banana\color{red}{$}\\
S_{4} = na$ban\color{red}{a}\\
S_{2} = nana$b\color{red}{a}
\end{cases}\\
S^{"} = BWT(banana) = annb$aa
$$

可以看出，转换后的字符串`annb$aa`比原来的字符串重复相连的字符的确更多了。实际上[http://www.bzip.org/ bzip]就是应用了BWT结合进行压缩的：

&gt; bzip2 compression program is based on Burrows–Wheeler algorithm.

BWT转换后的重复相连字符更多并不绝对，有时候可能转换后的情况反而更糟，比如这个例子：

$$
BWT(appellee) = e$elplepa
$$

反而不如原始字符串了。

== 解码==
=== 利用还原矩阵法===
解码的过程分为以下几步：

* 根据编码后的字符串 ${\displaystyle S^{"}}$ ，得到还原矩阵
* 根据还原矩阵，逐个还原出原来的顺序

根据编码的过程我们知道，实际上是这样的对应：
$$
\begin{cases}
S_{6} = \color{green}{$}banan\color{red}{a}\\
S_{5} = \color{green}{a}$bana\color{red}{n}\\
S_{3} = \color{green}{a}na$ba\color{red}{n}\\
S_{1} = \color{green}{a}nana$\color{red}{b}\\
S_{0} = \color{green}{b}anana\color{red}{$}\\
S_{4} = \color{green}{n}a$ban\color{red}{a}\\
S_{2} = \color{green}{n}ana$b\color{red}{a}
\end{cases}
\xrightarrow[\text{还原矩阵}]{}
\begin{pmatrix}
$ &amp; a\\
a &amp; n\\
a &amp; n\\
a &amp; b\\
b &amp; $\\
n &amp; a\\
n &amp; a
\end{pmatrix}
$$

得到这个矩阵非常简单，直接将字符串 ${\displaystyle S^{"}}$ 排个序就可以得到：

$$
\begin{cases}
------a\\
------n\\
------n\\
------b\\
------$\\
------a\\
------a
\end{cases}
\xrightarrow[\text{排序}]{}
\begin{cases}
------$\\
------a\\
------a\\
------a\\
------b\\
------n\\
------n
\end{cases}
\xrightarrow[\text{还原矩阵}]{}
\begin{pmatrix}
$ &amp; a\\
a &amp; n\\
a &amp; n\\
a &amp; b\\
b &amp; $\\
n &amp; a\\
n &amp; a
\end{pmatrix}
$$

在这样的一个还原矩阵中，每一个字符对应的就是它最末尾的字符。解码的过程如下：

* 从左边列的 ${\displaystyle S}$ 开始，找到对应的字符作为下一个字符 ${\displaystyle C_{n}}$
* 根据 ${\displaystyle C_{n}}$ 这个字符，在左边列找到对应的字符，其对应的字符即 ${\displaystyle C_{n-1}}$
* 以此类推，直到结尾
* 如果出现了多个相同的字符，那么就从上到下按顺序找就可以了

$$
\begin{pmatrix}
\color{red}{$} &amp; \color{red}{a}\\
a &amp; n\\
a &amp; n\\
a &amp; b\\
b &amp; $\\
n &amp; a\\
n &amp; a
\end{pmatrix}
\xrightarrow[\text{\$a}]{}
\begin{pmatrix}
\color{gray}{$} &amp; \color{gray}{a}\\
\color{red}{a} &amp; \color{red}{n}\\
a &amp; n\\
a &amp; b\\
b &amp; $\\
n &amp; a\\
n &amp; a
\end{pmatrix}
\xrightarrow[\text{\$an}]{}
\begin{pmatrix}
\color{gray}{$} &amp; \color{gray}{a}\\
\color{gray}{a} &amp; \color{gray}{n}\\
a &amp; n\\
a &amp; b\\
b &amp; $\\
\color{red}{n} &amp; \color{red}{a}\\
n &amp; a
\end{pmatrix}
\xrightarrow[\text{\$ana}]{}
\begin{pmatrix}
\color{gray}{$} &amp; \color{gray}{a}\\
\color{gray}{a} &amp; \color{gray}{n}\\
\color{red}{a} &amp; \color{red}{n}\\
a &amp; b\\
b &amp; $\\
\color{gray}{n} &amp; \color{gray}{a}\\
n &amp; a
\end{pmatrix}
\xrightarrow[\text{\$anan}]{}
\begin{pmatrix}
\color{gray}{$} &amp; \color{gray}{a}\\
\color{gray}{a} &amp; \color{gray}{n}\\
\color{gray}{a} &amp; \color{gray}{n}\\
a &amp; b\\
b &amp; $\\
\color{gray}{n} &amp; \color{gray}{a}\\
\color{red}{n} &amp; \color{red}{a}\\
\end{pmatrix}
\xrightarrow[\text{\$anana}]{}
\begin{pmatrix}
\color{gray}{$} &amp; \color{gray}{a}\\
\color{gray}{a} &amp; \color{gray}{n}\\
\color{gray}{a} &amp; \color{gray}{n}\\
\color{red}{a} &amp; \color{red}{b}\\
b &amp; $\\
\color{gray}{n} &amp; \color{gray}{a}\\
\color{gray}{n} &amp; \color{gray}{a}\\
\end{pmatrix}
\xrightarrow[\text{\$ananab}]{}
\begin{pmatrix}
\color{gray}{$} &amp; \color{gray}{a}\\
\color{gray}{a} &amp; \color{gray}{n}\\
\color{gray}{a} &amp; \color{gray}{n}\\
\color{gray}{a} &amp; \color{gray}{b}\\
\color{red}{b} &amp; \color{red}{$}\\
\color{gray}{n} &amp; \color{gray}{a}\\
\color{gray}{n} &amp; \color{gray}{a}\\
\end{pmatrix}\\
S^{'} = \$ananab\\
S = reverse(S^{'}) \\= banana\$
$$

=== 变种===
另一种方式可能更清晰，但实质上是一回事，只是做法看着不一样。在上述构建还原矩阵的过程中，我们实际已知的是最后一列的数据，那么，如果我们想办法把其他的列都构建出来，就可以得到原来的字符串了。

$$
\begin{cases}
------a\\
------n\\
------n\\
------b\\
------$\\
------a\\
------a
\end{cases}
\xrightarrow[\text{想办法变成这样}]{}
\begin{cases}
S_{6} = \color{green}{$}banan\color{red}{a}\\
S_{5} = \color{green}{a}$bana\color{red}{n}\\
S_{3} = \color{green}{a}na$ba\color{red}{n}\\
S_{1} = \color{green}{a}nana$\color{red}{b}\\
S_{0} = \color{green}{b}anana\color{red}{$}\\
S_{4} = \color{green}{n}a$ban\color{red}{a}\\
S_{2} = \color{green}{n}ana$b\color{red}{a}
\end{cases}
\xrightarrow[\text{然后拿到S6就可以了}]{}
S_{6} = \color{green}{$}banan\color{red}{a}
$$

过程是这样的：
$$
\begin{cases}
------a\\
------n\\
------n\\
------b\\
------$\\
------a\\
------a
\end{cases}
\xrightarrow[\text{将最后一列排序，作为第一列}]{}
\begin{cases}
$-----a\\
a-----n\\
a-----n\\
a-----b\\
b-----$\\
n-----a\\
n-----a
\end{cases}
$$

得到这个之后，从又到左得到`a$,na,na,ba,$b, an, an`，再将其排序作为第二列, 以此类推：

$$
\begin{cases}
------a\\
------n\\
------n\\
------b\\
------$\\
------a\\
------a
\end{cases}
\rightarrow
\begin{cases}
$-----a\\
a-----n\\
a-----n\\
a-----b\\
b-----$\\
n-----a\\
n-----a
\end{cases}
\rightarrow
\begin{cases}
a$\\
na\\
na\\
ba\\
$b\\
an\\
an
\end{cases}
\xrightarrow[\text{排序}]{}
\begin{cases}
$\color{green}{b}\\
a\color{green}{$}\\
a\color{green}{n}\\
a\color{green}{n}\\
n\color{green}{a}\\
n\color{green}{a}\\
b\color{green}{a}
\end{cases}
\xrightarrow[\text{得到第三列}]{}
\begin{cases}
$b----a\\
a$----n\\
an----n\\
an----b\\
ba----$\\
na----a\\
na----a
\end{cases}\\
\rightarrow
\begin{cases}
a$b\\
na$\\
nan\\
ban\\
$ba\\
ana\\
ana
\end{cases}
\rightarrow
\begin{cases}
$b\color{green}{a}\\
a$\color{green}{b}\\
an\color{green}{a}\\
an\color{green}{a}\\
ba\color{green}{n}\\
na\color{green}{$}\\
na\color{green}{n}
\end{cases}
\xrightarrow[\text{得到第三列}]{}
\begin{cases}
$ba---a\\
a$b---n\\
ana---n\\
ana---b\\
ban---$\\
na$---a\\
nan---a
\end{cases}
\xrightarrow[\text{如此反复，最终得到全部列}]{}
$$

= 代码实现=
看似复杂的操作，没想到用python可以写的如此简单，不过不见得一看就懂...
&lt;syntaxhighlight lang="python"&gt;
EOL = '$'

def encode(source):
    source = source + EOL
    table = [source[i:] + source[:i] for i in range(len(source))]
    table.sort()

    return ''.join([row[-1] for row in table])

def decode(encoded):
    length = len(encoded)
    table = [''] * length

    for i in range(length):
        table = sorted([encoded[m] + table[m] for m in range(length)])
        print(table)
    s = [row for row in table if row.endswith(EOL)][0]
    return s.rstrip(EOL)
&lt;/syntaxhighlight&gt;

解码的这个循环不大好理解，打出来一看就懂了：
&lt;pre&gt;
['$', 'a', 'a', 'a', 'b', 'n', 'n']
['$b', 'a$', 'an', 'an', 'ba', 'na', 'na']
['$ba', 'a$b', 'ana', 'ana', 'ban', 'na$', 'nan']
['$ban', 'a$ba', 'ana$', 'anan', 'bana', 'na$b', 'nana']
['$bana', 'a$ban', 'ana$b', 'anana', 'banan', 'na$ba', 'nana$']
['$banan', 'a$bana', 'ana$ba', 'anana$', 'banana', 'na$ban', 'nana$b']
['$banana', 'a$banan', 'ana$ban', 'anana$b', 'banana$', 'na$bana', 'nana$ba']
&lt;/pre&gt;

* [https://zh.wikipedia.org/wiki/Burrows-Wheeler%E5%8F%98%E6%8D%A2 维基百科-Burrows-Wheeler变换]
* [https://www.cs.cmu.edu/~ckingsf/bioinfo-lectures/bwt.pdf BWT]</text>
      <sha1>gf36di051u9h3tn70zlp7s27xn2z64m</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Compile live555 for Android</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2017-11-20T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>编译 live555的库在 android 上使用。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1348" sha1="hjlvzhgelzuvfbprk1qyucznhpv3yd3">编译 live555的库在 android 上使用。
首先下载liveMedia库。解压完成可以先在linux环境下编译一遍试试，例如：
&lt;syntaxhighlight lang="bash"&gt;
./genMakefiles macosx
make
&lt;/syntaxhighlight&gt;
然后，可以利用ndk-build将它交叉编译成动态库。这时候，需要新建一个Android.mk和Application.mk文件：
* Application.mk
&lt;syntaxhighlight lang="makefile"&gt;
APP_BUILD_SCRIPT := Android.mk
APP_STL := gnustl_shared
APP_ABI := armeabi-v7a
&lt;/syntaxhighlight&gt;

* Android.mk
&lt;syntaxhighlight lang="makefile"&gt;
LOCAL_PATH := $(call my-dir)

include $(CLEAR_VARS)

LOCAL_MODULE := liblive555
LOCAL_CPPFLAGS += -fexceptions -DXLOCALE_NOT_USED=1 -DNULL=0 -DNO_SSTREAM=1 -UIP_ADD_SOURCE_MEMBERSHIP -DSOCKLEN_T=socklen_t

LOCAL_C_INCLUDES := \
	$(LOCAL_PATH) \
	$(LOCAL_PATH)/BasicUsageEnvironment/include \
	$(LOCAL_PATH)/UsageEnvironment/include \
	$(LOCAL_PATH)/groupsock/include \
	$(LOCAL_PATH)/liveMedia/include \

LOCAL_SRC_FILES := \
	BasicUsageEnvironment/BasicHashTable.cpp         \
        ...(这里把其他cpp、c文件都列到这里）

include $(BUILD_SHARED_LIBRARY)
&lt;/syntaxhighlight&gt;
然后执行ndk-build进行编译：
&lt;syntaxhighlight lang="bash"&gt;
ndk-build NDK_PROJECT_PATH=. NDK_APPLICATION_MK=Application.mk
&lt;/syntaxhighlight&gt;
编译完成就可以得到libgnustl_shared.so liblive555.so了。</text>
      <sha1>hjlvzhgelzuvfbprk1qyucznhpv3yd3</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Android Develop:设置应用图标</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2019-02-20T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>前前后后各种原因耽搁了大半年，终于还是下定决心开始做我的Okapia android应用了，笔者一直从事的是Java后端和Web开发，基本上没有安卓开发的项目经验，正所谓万事开头难，一边学一边做。所以我计划把做的过程中遇到的一些问题都整理记录下来，供读者参考。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3478" sha1="hnlvybm74m1l2s90ira5wr9hvlf2tro">前前后后各种原因耽搁了大半年，终于还是下定决心开始做我的Okapia android应用了，笔者一直从事的是Java后端和Web开发，基本上没有安卓开发的项目经验，正所谓万事开头难，一边学一边做。所以我计划把做的过程中遇到的一些问题都整理记录下来，供读者参考。

= 原型设计=
原型设计是一项比较重要的事情，可以帮助我们在开发之前就理清楚要做什么，现在有比较多的工具可以来做这个事情。我用的是别人推荐的MockingBot（墨刀），一个国产软件，还比较好用。

[[File:mockingbot_ui.png|600px|mockingbot]]

= 图标资源文件夹=
使用android studio生成的项目中，有不少文件夹：

[[File:android_project_res.png|600px|android project resource folder]]

* drawable
* drawable-v24
* mipmap-anydpi-v26
* mipmap-hdpi
* ...

可以看出mipmap-xxxx中其实都是不同分辨率适配的不同大小的图标，唯独mipmap-anydpi-v26中其实是一个xml，将background和forground分开了拼合到了一起。而background和forground实际是一个矢量图，网上资料显示实际是svg的一个简化版本的android实现。这就麻烦了，哪里去做svg矢量图！大概有以下的途径吧：

= 图标生成与编辑=
== 图标编辑工具==

* 将图片转换为SVG [https://www.pngtosvg.com/ pngtosvg]
* 使用SVG编辑工具绘制（推荐macSvg）
* 将SVG转换为android vector [http://inloop.github.io/svg2android/  svg2android]

这样需要生成两张图片，一个背景一个图标。注意图标要适当居中一点，边上要留一些边距。这个可以通过控制viewBox和拖动形状来完成，在macSvg中即可处理。

[[File:macsvg.png|600px|macsvg]]

譬如如上的图片，它的参照系设置是400，这样把图片拖到200的位置，就基本上居中了。如果想让图片缩小一点，可以把参照系设置大一点，比如600，再把形状拖到300的位置，想直接缩小形状貌似是没找到办法。简单的颜色替换什么的，其实直接用文本编辑器就可以了。

== 导入到工程中==

通过在res文件夹上右键:New -&gt; Image Set即可自动生成各个分辨率的图标。

[[File:new_image_set.png|600px|Import image set]]

= 安卓中的图标与名称配置=

在AndroidManifest.xml中配置了图标的路径：
&lt;syntaxhighlight lang="xml"&gt;
&lt;application
        android:allowBackup="true"
        android:icon="@mipmap/ic_launcher"
        android:label="@string/app_name"
        android:roundIcon="@mipmap/ic_launcher_round"
        android:supportsRtl="true"
        android:theme="@style/AppTheme"&gt;
&lt;/syntaxhighlight&gt;

可以看出指定了icon、label和roundIcon等。这里我们保持原有的ic_launcher名称即可，导入图标的时候直接替换掉即可。另一个是app的名称，需要注意的是，如果启动的Activity上有label，这时候app的名称会变成这个activity的名称，例如:

&lt;syntaxhighlight lang="xml"&gt;
&lt;activity
        android:name=".FullscreenActivity"
        android:configChanges="orientation|keyboardHidden|screenSize"
        android:label="@string/title_activity_fullscreen"
        android:theme="@style/FullscreenTheme"&gt;&lt;/activity&gt;
&lt;/syntaxhighlight&gt;
如果这个activity是启动activity，那么app的名称就是@string/title_activity_fullscreen这个值了，要处理这个也很简单，我们直接删掉这个activity的label属性即可。</text>
      <sha1>hnlvybm74m1l2s90ira5wr9hvlf2tro</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Compile Argon2 for Android</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2018-08-03T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>[Keepass](https://keepass.info/help/kb/kdbx_4.html)中使用了一个[Argon2](https://www.argon2.com/)的算法来存储用户主密码，这个算法被认为是下一代的较为安全的密码散列算法。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1727" sha1="sqm1h40af7j6slbi1kv96s8sdejamgl">[https://keepass.info/help/kb/kdbx_4.html Keepass]中使用了一个[https://www.argon2.com/ Argon2]的算法来存储用户主密码，这个算法被认为是下一代的较为安全的密码散列算法。
我们来看看 Argon2算法：
&lt;pre&gt;
hfli@CNhfli ~/Documents/phc-winner-argon2 (master*) $ echo -n "password" | ./argon2 somehelloworld -t 2 -m 16 -p 4 -l 24
Type:		Argon2i
Iterations:	2
Memory:		65536 KiB
Parallelism:	4
Hash:		2748e90a5a301dbcf46067a8784d3e73c7acc8939b4ee02d
Encoded:	$argon2i$v=19$m=65536,t=2,p=4$c29tZWhlbGxvd29ybGQ$J0jpClowHbz0YGeoeE0+c8esyJObTuAt
0.126 seconds
Verification ok
&lt;/pre&gt;
这个算法又分为两个版本,Argon2i和Argon2d:

&gt; Argon2d provides the highest resistance against GPU cracking attacks. Argon2i is designed to resist side-channel attacks.

Keepass中选择的是Argon2d

&gt; Only the Argon2d variant of Argon2 is supported (a strong defense against GPU/ASIC cracking attacks is the most important goal, and Argon2d here is better than Argon2i; side-channel timing attacks are basically irrelevant, because KeePass is a local application, not a remote server)

在安卓上编译可以参考如下步骤:

Application.mk
&lt;pre&gt;
APP_BUILD_SCRIPT := Android.mk
APP_STL := gnustl_shared
APP_ABI := armeabi-v7a
&lt;/pre&gt;
Android.mk
&lt;pre&gt;
LOCAL_PATH := $(call my-dir)

include $(CLEAR_VARS)

LOCAL_MODULE := libargon2
LOCAL_CPPFLAGS += -fexceptions

LOCAL_C_INCLUDES := \
    $(LOCAL_PATH) \
    $(LOCAL_PATH)/include \

LOCAL_SRC_FILES := \
    src/argon2.c \
    src/core.c \
    src/blake2/blake2b.c \
    src/encoding.c \
    src/ref.c \
    src/thread.c \

include $(BUILD_SHARED_LIBRARY)
&lt;/pre&gt;

编译
&lt;pre&gt;
ndk-build NDK_PROJECT_PATH=. NDK_APPLICATION_MK=Application.mk
&lt;/pre&gt;</text>
      <sha1>sqm1h40af7j6slbi1kv96s8sdejamgl</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:flutter中使用ffi调用SqlCipher</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-11-04T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>目前在flutter中调用sqlite有成熟的插件例如sqflite，而我需要sqlcipher，并同时加载fts5扩展，现有的插件并不能直接支持。因此需要创建一个插件来做这个事情。在以前平台集成相当麻烦，而现在有了ffi之后，可以直接调用原生代码，虽然还在试验阶段但终究是大势所趋。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5188" sha1="bu9x2w9smje6romp6b11taft0weis43">目前在flutter中调用sqlite有成熟的插件例如sqflite，而我需要sqlcipher，并同时加载fts5扩展，现有的插件并不能直接支持。因此需要创建一个插件来做这个事情。在以前平台集成相当麻烦，而现在有了ffi之后，可以直接调用原生代码，虽然还在试验阶段但终究是大势所趋。


= 源码编译=
sqlite源码编译比较简单，但是如果要运行其测试，在mac上还是有些折腾。sqlite3运行测试需要tcl8.6，自带的无法使用（/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/System/Library/Frameworks/Tcl.framework/）
下载[https://www.tcl.tk/software/tcltk/download.html Tcl8.6]并编译安装：

&lt;syntaxhighlight lang="bash"&gt;
cd tcl8.6.10/unix
./configure
make
sudo make install
&lt;/syntaxhighlight&gt;
然后可以跑sqlite的测试`make test`。

而要编译sqlcipher，也有一些选项需要设置，

&lt;syntaxhighlight lang="bash"&gt;
cd sqlcipher/build
./configure --enable-tempstore=no \
  --with-crypto-lib=none \
  --enable-fts5 \
  CFLAGS="-DSQLCIPHER_CRYPTO_OPENSSL -DSQLITE_TEMP_STORE=3 -DSQLITE_HAS_CODEC -I/usr/local/opt/openssl/include/" \
  LDFLAGS="/usr/local/opt/openssl/lib/libcrypto.a"
&lt;/syntaxhighlight&gt;

= 在flutter中集成=
== 创建插件==

首先需要创建一个插件：

&lt;syntaxhighlight lang="bash"&gt;
flutter create \
  --platforms=android,ios \
  --org=com.riguz \
  --template=plugin \
  -i swift \
  -a java \
  native_sqlcipher
&lt;/syntaxhighlight&gt;

flutter升级之后，必须指定`--platforms=android,ios`才会生成ios和android的响应工程。


== 平台集成==

=== 生成整合的代码===
最直接的方式是将sqlcipher（其中已经包含了sqlite的代码）整合到一个文件中（称之为“amalgamation”版本，运行速度会更快一些）

&lt;syntaxhighlight lang="bash"&gt;
git clone https://github.com/sqlcipher/sqlcipher.git
cd sqlcipher
mkdir build
../configure --with-crypto-lib=none --enable-fts5
make sqlite3.c
&lt;/syntaxhighlight&gt;

生成完之后，会得到一个sqlite3.c和sqlite3.h。如果希望在mac上也编译出来，可以

&lt;syntaxhighlight lang="bash"&gt;
./configure --enable-tempstore=no \
  --with-crypto-lib=none \
  --enable-fts5 \
  CFLAGS="-DSQLCIPHER_CRYPTO_OPENSSL -DSQLITE_TEMP_STORE=3 -DSQLITE_HAS_CODEC -I/usr/local/opt/openssl/include/" \
  LDFLAGS="/usr/local/opt/openssl/lib/libcrypto.a"
&lt;/syntaxhighlight&gt;

=== ios集成===

将sqlite3.c和sqlite3.h代码拷贝到插件的ios/Classes目录中，然后需要修改插件的podspec文件（单纯在xcode里面修改无法保存，每次pod install之后就会丢失），需要将一些参数加进去：

&lt;syntaxhighlight lang="ruby"&gt;
Pod::Spec.new do |s|
  s.name             = 'native_sqlcipher'
  # ...

  # 尽管之前configure的时候已经指定了SQLITE_ENABLE_FTS5，但是这里还需要再设置一次才能将fts5扩展编译进去
  s.frameworks = 'Security'
  s.xcconfig = { 'OTHER_CFLAGS' =&gt; '-DSQLITE_ENABLE_FTS5 -DSQLITE_HAS_CODEC -DSQLITE_TEMP_STORE=3 -DSQLCIPHER_CRYPTO_CC -DNDEBUG' }
  # ...
end
&lt;/syntaxhighlight&gt;

== 安卓集成==

=== 编译OpenSSL===
下载openssl，当前最新为1.1.1h。其编译说明可以参照NOTES.ANDROID文档。

&lt;syntaxhighlight lang="bash"&gt;
export ANDROID_NDK_HOME=/Users/hfli/Library/Android/sdk/ndk/21.3.6528147
	PATH=$ANDROID_NDK_HOME/toolchains/llvm/prebuilt/darwin-x86_64/bin:$ANDROID_NDK_HOME/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin:$PATH
  ./Configure android-arm64 -D__ANDROID_API__=29
	make
&lt;/syntaxhighlight&gt;

对于其他平台需要重新configure然后编译：

&lt;syntaxhighlight lang="bash"&gt;
#./Configure android-arm -D__ANDROID_API__=29
#./Configure android-arm64 -D__ANDROID_API__=29
#./Configure android-x86 -D__ANDROID_API__=29
./Configure android-x86_64 -D__ANDROID_API__=29
make clean
make
&lt;/syntaxhighlight&gt;
默认生成的so是带有后缀的，（类似*.so.1.1)，因为安卓打包的时候不支持，解决办法是在make的时候覆盖掉参数：

&lt;syntaxhighlight lang="bash"&gt;
make SHLIB_VERSION_NUMBER= SHLIB_EXT=.so
&lt;/syntaxhighlight&gt;

=== 集成sqlcipher===

&lt;!-- tbd --&gt;

== 调用==

参见[https://flutter.dev/docs/development/platform-integration/c-interop Binding to native code using dart:ffi]。

= 兼容性=
SqlCipher依赖一个非标准的选项，但是这个选项[https://discuss.zetetic.net/t/removal-of-sqlite-has-codec-compile-time-option-from-public-sqlite-code/4262 最近已经被移除]了，SqlCipher目前包含的sqlite的版本为3.31.0，而sqlite最新为3.33.0。目前尚不知道后续的支持计划。

参考：

* [https://github.com/petasis/tkdnd/issues/16 configure: error: Can't find Tcl configuration definitions MacOS]
* [https://www.zetetic.net/sqlcipher/ios-tutorial/ Adding SQLCipher to Xcode Projects]
* [https://github.com/dart-lang/sdk/blob/master/samples/ffi/sqlite/ Dart ffi sqlite example]
* [https://github.com/sqlcipher/sqlcipher/issues/132#issuecomment-122912569 error: Library crypto not found. Install openssl!]
* [https://github.com/openssl/openssl/issues/3902 shared library without version suffix for android (feature request)]</text>
      <sha1>bu9x2w9smje6romp6b11taft0weis43</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:创建一个Flutter的插件</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2019-05-13T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>最近需要在Flutter中实现AES加解密和KDF，但搜索了一下貌似网络上没有现成的库可以用，因此尝试手写了一个Flutter的插件，实现两个功能：

* AES256/CBC/NoPadding 加解密
* Argon2（Argon2d)</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6226" sha1="m6djdstayfbyb7iy7bp1u9ywbx893c2">最近需要在Flutter中实现AES加解密和KDF，但搜索了一下貌似网络上没有现成的库可以用，因此尝试手写了一个Flutter的插件，实现两个功能：

* AES256/CBC/NoPadding 加解密
* Argon2（Argon2d)

= 插件定义=
== 创建插件工程==

其实貌似也可以在Flutter项目中直接调用Platform channel相关的实现，考虑到把这一部分剥离出来可以单独维护和造福后人，还是选择创建一个Plugin。首先需要创建一个插件的工程，通过如下的命令：

&lt;syntaxhighlight lang="bash"&gt;
flutter create --org com.riguz --template=plugin encryptions
&lt;/syntaxhighlight&gt;

这样会生成一个项目，值得注意的是，这里Android会使用Java，IOS会使用Objective-C。但Objective-C对于我这种没有基础的人来说看着太麻烦了，我尝试了一些之后放弃了。于是需要切换成Swift。这里有一个小的方法可以只修改IOS的部分：

&lt;syntaxhighlight lang="bash"&gt;
cd encryptions
rm -rf ios examples/ios
flutter create -i swift --org com.riguz .
&lt;/syntaxhighlight&gt;
删除ios的目录后执行这个命令，可以重新生成ios的工程，基于swift的。

== 定义Dart接口==

首先定义出我们要暴露的接口。举个例子，对于AES加密的函数，我们可以这样写：

&lt;syntaxhighlight lang="dart"&gt;
class Encryptions {
  static const MethodChannel _channel = const MethodChannel('encryptions');

  static Future&lt;Uint8List&gt; aesEncrypt(
      Uint8List key, Uint8List iv, Uint8List value) async {
    return await _channel
        .invokeMethod("aesEncrypt", {"key": key, "iv": iv, "value": value});
  }
&lt;/syntaxhighlight&gt;

这里有几点值得注意的：

* MethodChannel是用来调用原生接口，后面各个平台会注册同名的MethodChannel。
* 调用原生方法通过方法名 + 参数调用，参数的对应列表参见官方文档。这里我们希望的是Java中的byte[] 类型，所以用Uint8List
* 参数通过key-value的map传递到原生接口，原生代码通过参数名取得参数值

= Platform实现=
== ios==
首先需要先build一下:
&lt;syntaxhighlight lang="bash"&gt;
cd encryptions/example; flutter build ios --no-codesign
&lt;/syntaxhighlight&gt;
在Xcode中打开项目，有一个SwiftEncryptionsPlugin的类，在这个里面实现即可：

&lt;syntaxhighlight lang="swift"&gt;
public func handle(_ call: FlutterMethodCall, result: @escaping FlutterResult) {
    let args = call.arguments as! [String: Any];
    switch call.method {
    case "aesEncrypt", "aesDecrypt":
        let key = args["key"] as! FlutterStandardTypedData;
        let iv = args["iv"] as! FlutterStandardTypedData;
        let value = args["value"] as! FlutterStandardTypedData;
        
        do {
            let cipher = try handleAes(key: key.data, iv: iv.data, value: value.data, method: call.method);
            result(cipher);
        } catch {
            result(nil);
        };     
        // ...
    }
}
&lt;/syntaxhighlight&gt;
因为需要使用Argon2，需要在swift中调用原生c代码，试了一些办法都不行，后来发现其实比较简单，直接在Supported Files中有一个encryptions-umbrella.h文件中加入引用，就可以直接调用了:

&lt;syntaxhighlight lang="c"&gt;
#import "EncryptionsPlugin.h"
#import "argon2.h"
&lt;/syntaxhighlight&gt;

&lt;syntaxhighlight lang="swift"&gt;
func argon2i(password: Data, salt: Data)-&gt; Data {
    var outputBytes  = [UInt8](repeating: 0, count: hashLength);
    
    password.withUnsafeBytes { passwordBytes in
        salt.withUnsafeBytes {
            saltBytes in
            argon2i_hash_raw(iterations, memory, parallelism, passwordBytes, password.count, saltBytes, salt.count, &amp;outputBytes, hashLength);
        }
    }
    
    return Data(bytes: UnsafePointer&lt;UInt8&gt;(outputBytes), count: hashLength);
}

&lt;/syntaxhighlight&gt;

== Android==

在Android Studio中打开工程（第一次打开是需要build的，```cd encryptions/example; flutter build apk```， ios也类似）。Android中实现起来会简单一点，这里只说一下如何调用c原生代码：

首先在build.gradle中加入额外的步骤：

&lt;syntaxhighlight lang="groovy"&gt;
externalNativeBuild {
    cmake {
        path "src/main/cpp/CMakeLists.txt"
    }
}
&lt;/syntaxhighlight&gt;
然后在CMakeLists.txt中指定编译步骤，我这里需要编译一个argon2的库，以及一个JNI调用的库。

&lt;syntaxhighlight lang="cmake"&gt;
add_library(
        argon2
        SHARED

        argon2/src/argon2.c
        argon2/src/core.c
        argon2/src/blake2/blake2b.c
        argon2/src/encoding.c
        argon2/src/ref.c
        argon2/src/thread.c
)

add_library(
        argon2-binding
        SHARED

        argon2_binding.cpp
)

target_include_directories(
        argon2
        PRIVATE
        argon2/include
)

target_include_directories(
        argon2-binding
        PRIVATE
        argon2/include
)

find_library(
        log-lib
        log)


target_link_libraries(
        native-lib
        ${log-lib})

target_link_libraries(
        argon2-binding

        argon2
        ${log-lib})
&lt;/syntaxhighlight&gt;
然后就通过JNI调用到argon2的方法：

&lt;syntaxhighlight lang="java"&gt;
public final class Argon2 {
    static {
        System.loadLibrary("argon2-binding");
    }
	
	// ...

    private native byte[] argon2iInternal(int iterations, int memory, int parallelism, final byte[] password, final byte[] salt, int hashLength);

    private native byte[] argon2dInternal(int iterations, int memory, int parallelism, final byte[] password, final byte[] salt, int hashLength);
}
&lt;/syntaxhighlight&gt;
详细的代码不再累述。

= Example=
在example工程中，用dart调用一下这些接口，然后可以分别在Xcode和Android Studio中运行起来，看一下不同平台是否都支持。不清楚是否有自动化的测试方法。

[[File:encryptions_example.jpeg|600px|example]]

如果想了解更多，[https://github.com/soleverlee/encryptions 这里]是详细的代码。

参考:

* [https://flutter.dev/docs/development/platform-integration/platform-channels Writing custom platform-specific code]
* [https://flutter.dev/docs/development/packages-and-plugins/developing-packages Developing packages &amp; plugins]</text>
      <sha1>m6djdstayfbyb7iy7bp1u9ywbx893c2</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Flutter性能优化实践</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2019-11-20T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>我的加密软件有一个登录页面，需要用户输入主密码然后验证密码之后才能进入。因为密码转换(Key transform)过程中用到了Argon2算法，而这个算法没有原生的dart实现，所以必须要通过插件的形式来完成，为此我还专门做了一个插件[encryptions](https://pub.dev/packages/encryptions)。调用插件得到秘钥这个过程大概要花个1~4秒钟，最近在安卓真机上测试发现，这个过程中我的进度条竟然出现了卡顿，也就是说本来应该转圈圈的，结果一开始就卡住不动了，那我还需要这个加载动画干嘛呢？为此研究了一番，如何来解决这个问题。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11833" sha1="nc9byghgspej7eli0blnbtn7pvdgajl">我的加密软件有一个登录页面，需要用户输入主密码然后验证密码之后才能进入。因为密码转换(Key transform)过程中用到了Argon2算法，而这个算法没有原生的dart实现，所以必须要通过插件的形式来完成，为此我还专门做了一个插件[https://pub.dev/packages/encryptions encryptions]。调用插件得到秘钥这个过程大概要花个1~4秒钟，最近在安卓真机上测试发现，这个过程中我的进度条竟然出现了卡顿，也就是说本来应该转圈圈的，结果一开始就卡住不动了，那我还需要这个加载动画干嘛呢？为此研究了一番，如何来解决这个问题。

= 背景=
先还是大致介绍一下我的场景。如图所示：

[[File:login_blocked.gif|600px|Blocked Login UI]]

点击完成之后，期待的情况应该是显示圈圈转动的，但很明显这个地方卡住了。代码实现是是采用的mobx作状态管理，然后使用插件调用的argon2的原生c代码。页面的部分如下：

&lt;syntaxhighlight lang="dart"&gt;
children: &lt;Widget&gt;[
  _profileImage(),
  Observer(
    builder: (_) {
      return _userStore.isBusy ? _progressBar() : _input(context);
    },
  )
],
&lt;/syntaxhighlight&gt;
然后是登录事件的处理代码:

&lt;syntaxhighlight lang="dart"&gt;
@action
Future&lt;bool&gt; login(ProtectedValue masterPassword) async {
  setBusy();
  bool success = false;
  try {
    final PasswordCredential credential =
        await _loginService.checkUserCredential(masterPassword);
    _errorMessage = null;
    _userCredential = credential;
    success = true;
  } catch (_) {
    _userCredential = null;
    _errorMessage = "密码验证失败";
  }
  setIdle();
  return success;
}
&lt;/syntaxhighlight&gt;

= 定位问题=
== 使用flutter profile工具==

这就让我犯迷糊了，不是用了future么，async/await这么厉害来着，为毛会卡住呢？因为本身我是dart和flutter的初学者，不清楚一些细节，所以一开始怀疑的是，在点击登录之后设置`busy`状态的这一个步骤没有生效导致的。因为本身用的是mobx框架来处理的，很自然就开始怀疑是不是这个玩意有bug？或者说是我的用法有些问题？然而搜索了很久也没有找到相关的问题，一度有些不知所措。

所以首先想搞清楚的是，到底是什么操作导致了卡顿呢？这时候想到flutter可能有性能分析的工具，能不能帮助定位到具体的代码行呢？于是在Profile模式下运行程序，这是相关的信息：

[[File:login_blocked_overlay.gif|600px|Performance overlay]]

然后还有dev tools中的对应信息：

[[File:dart_devtools.png|600px|Dart dev tools]]

很可惜从这个分析结果中我没有能找到对我有帮助的信息，唯一只能说确定的确会卡顿...

== 换个思路继续试==
因为从这条路感觉已经走不通了，所以决定从其他的地方入手继续查。我注意到在登录的时候，偶尔会出现这样的日志：

&lt;pre&gt;
I/Choreographer(15562): Skipped 107 frames!  The application may be doing too much work on its main thread.
&lt;/pre&gt;

这说明啥？说明的确是卡了....这时候我开始怀疑，如果在登录这个事件里面做的事情很多的情况下，即便我们用了async/await，是不是也会卡顿呢？为了屏蔽掉其他条件的干扰，我们需要将登陆事件变得简单一些：

&lt;syntaxhighlight lang="dart"&gt;
oid onPasswordSubmitted(BuildContext context, String password) async {
  print('login with: ${password}');
  for(int i = 0; i &lt; 100000; i++) {
  }
}
&lt;/syntaxhighlight&gt;
果然发现这样UI一样会卡住，甚至卡的更厉害了，根本连进度圈都出不来了。从这个结果来看，说明async/await并不能保证不会block住UI，顺着这个思路朝下找，于是就找到问题的根本原因了。

= 让事件在单独的线程中运行=
== Flutter线程模型==
原来dart跟JavaScript一样是一个单线程的模型，也就是是说，async/await里面的方法并不是在多个线程中取执行的，而是通过事件机制，在单线程中完成的。那么就很容易解释UI卡顿的场景了，UI更新和事件代码是交替执行的，如果其中事件执行的部分花费了较长时间，UI就没办法去更新，所以界面就会卡在那里，也就是日志里面所说的`Skipped 107 frames!  The application may be doing too much work on its main thread.`了。

[[File:https://cdn.jsdelivr.net/gh/flutterchina/flutter-in-action/docs/imgs/2-12.png|600px|Dart event model]]

从dart的文档中可以了解到，dart内部有两个队列：

* event queue： 包含所有的外部事件例如IO、点击、绘制等
* microtask queue： 微任务通常来自于dart内部或者手动插入`Future.microtask(…)`

microstask队列的优先级是要高于事件队列的。这里我们的登录事件和UI更新都同在事件队列中，很显然是因为我们的登录事件耗时太长从而掉帧了，那么解决的方案也就是，可不可以在新的线程中执行我们的事件呢？

== Isolate机制==
研究了一下发现在dart中不叫线程，如果想达到这种目的需要使用一个称之为`Isolate`的东西，大致相当于新开一个线程来处理。要使用Isolate有两种办法：

* 使用`compute`方法
* 使用`Isolate.spawn`（更低级的操作）

下面是一个例子：

&lt;syntaxhighlight lang="dart"&gt;
Future&lt;List&lt;Photo&gt;&gt; fetchPhotos(http.Client client) async {
  final response =
      await client.get('https://jsonplaceholder.typicode.com/photos');

  // Use the compute function to run parsePhotos in a separate isolate.
  return compute(parsePhotos, response.body);
}
&lt;/syntaxhighlight&gt;

== Compute方法==
看样子使用Isolate看似就像在Java中新建一个线程一样，然后就可以在线程中运行代码了。那我们直接把登录事件的处理挪进去不就得了么？然而实际情况是，这玩意并不是十分好用，有着诸多（恶心）的限制。先来看一下最简单直接的`compute`方法：

&lt;syntaxhighlight lang="dart"&gt;
typedef ComputeCallback&lt;Q, R&gt; = FutureOr&lt;R&gt; Function(Q message);
&lt;/syntaxhighlight&gt;

这个方法有两个参数，一个是callback（相当于java中的Runnable，就是你要执行的方法)，另一个是message，相当于是参数。这些参数有着如下的限制：

* callback必须是顶级的方法或者`static`方法，不能是类的实例方法或者是闭包
* 只有一个参数，那么我的方法需要传多个参数怎么办？

这就有些尴尬了，不仅要求是静态方法，还限制了参数，而我们的事件处理中有很多的依赖项，这可咋放进去呢？那么很可能只能用另一种方式了。

== Isolate.spawn==
这个就显得更为麻烦了，大致的用法是这样的：

&lt;syntaxhighlight lang="dart"&gt;

var ourFirstReceivePort = new ReceivePort();             // 需要一个ReceivePort来接收消息
await Isolate.spawn(echo, ourFirstReceivePort.sendPort); // 创建一个Isolate
var echoPort = await ourFirstReceivePort.first;          // 等待执行完成并接收返回值

// 在Isolate中运行的代码需要将返回值通过sendPort发送过来
sendPort.send(...);
&lt;/syntaxhighlight&gt;
这个`spawn`方法如下：

&lt;syntaxhighlight lang="dart"&gt;
external static Future&lt;Isolate&gt; spawn&lt;T&gt;(
      void entryPoint(T message), T message,
      {bool paused: false,
      bool errorsAreFatal,
      SendPort onExit,
      SendPort onError,
      @Since("2.3") String debugName});
&lt;/syntaxhighlight&gt;
同样有着如下的限制：

* 两个参数，一个entryPoint，另一个是这个entryPoint方法的唯一参数（也就是message)
* entryPoint方法必须是顶级方法或者静态方法
* message参数必须是基本类型、SendPort或者只包含这两者的list或者map。

这样就更加有些尴尬了，如果我们希望调用一个实例方法怎么办呢？我的登录过程中的一个关键步骤是调用argon2，这个方法我希望能在单独的线程之中调用：

&lt;syntaxhighlight lang="dart"&gt;
class Argon2Kdf implements Kdf {
  @override
  Future&lt;Uint8List&gt; derive(Uint8List password, Uint8List salt) async {
    Argon2 argon2 = new Argon2();
    return await argon2.argon2i(password, salt) as Uint8List;
  }
}
&lt;/syntaxhighlight&gt;
这个方法本身是一个实例方法，既然我们只能用顶级方法和实例方法，而且Argon2这个类也没有什么依赖，也罢，那就创建一个好了，没有问题，现在问题来了，我们需要的两个参数是password和salt，这个参数类型不是基本类型，不支持呢，咋办呢？

== Isolate中通信==
原来isolate不像Java的线程一样可以使用共享内存，而是想actor模型一样，只能通过消息进行通信。那么，我们需要将参数转换为支持的基本类型，通过SendPort发送过去：

&lt;syntaxhighlight lang="dart"&gt;
class Argon2Kdf implements Kdf {
  static final Argon2 argon2 = Argon2(iterations: 2);

  static void argon2Call(SendPort replyPort) async {
    final receivePort = ReceivePort();
    replyPort.send(receivePort.sendPort);
    final List&lt;dynamic&gt; params = await receivePort.first;
    final SendPort reportPort1 = params[0];

    final String passwordHex = params[1];
    final String saltHex = params[2];
    final Uint8List password = hex.decode(passwordHex) as Uint8List;
    final Uint8List salt = hex.decode(saltHex) as Uint8List;

    final Uint8List result = await argon2.argon2i(password, salt) as Uint8List;

    reportPort1.send(hex.encode(result));
  }

  @override
  Future&lt;Uint8List&gt; derive(Uint8List password, Uint8List salt) async {
    final ReceivePort response = ReceivePort();
    final isolate = await FlutterIsolate.spawn(argon2Call, response.sendPort);
    final SendPort sendPort = await response.first;
    final ReceivePort response1 = ReceivePort();
    sendPort.send([response1.sendPort, hex.encode(password), hex.encode(salt)]);
    final String result = await response1.first;

    isolate.kill();

    return hex.decode(result);
  }
}
&lt;/syntaxhighlight&gt;
这里有两点需要注意：

* 调用`await response.first`之后这个SendPort就自动解除订阅了，不能用来接收其他消息了
* Isolate.spawn不支持运行插件代码，所以用FlutterIsolate这个库来实现，而使用方法是一致的

如果直接使用`Isolate.spawn`会有如下的报错信息:

&lt;pre&gt;
E/flutter (18071): [ERROR:flutter/runtime/dart_isolate.cc(808)] Unhandled exception:
E/flutter (18071): error: native function 'Window_sendPlatformMessage' (4 arguments) cannot be found
&lt;/pre&gt;

这样最终的效果就是:

[[File:login_unblocked.gif|600px|Login unblocked]]

* [https://medium.com/dartlang/dart-asynchronous-programming-isolates-and-event-loops-bffc3e296a6a Dart asynchronous programming: Isolates and event loops]
* [https://flutter.dev/docs/cookbook/networking/background-parsing#4-move-this-work-to-a-separate-isolate Move this work to a separate isolate]
* [https://pub.dev/packages/flutter_isolate flutter_isolate]
* [https://alvinalexander.com/dart/dart-isolates-example A Dart Isolates example (Actors in Dart)]
* [https://github.com/flutter/flutter/issues/26413 'Window_sendPlatformMessage' (4 arguments) cannot be found]
* [https://stackoverflow.com/questions/54127158/flutter-window-sendplatformmessage-4-arguments-cannot-be-found Flutter - 'Window_sendPlatformMessage' (4 arguments) cannot be found]
* [https://github.com/dart-lang/sdk/issues/35962 Cannot send regular Dart Instance to Isolate spawned with spawnUri]
* [https://github.com/flutter/flutter/issues/34993 Plugins crash with "Methods marked with \@UiThread must be executed on the main thread."]
* [https://book.flutterchina.club/chapter2/thread_model_and_error_report.html Dart单线程模型]</text>
      <sha1>nc9byghgspej7eli0blnbtn7pvdgajl</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Android Develop:横屏布局</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2019-02-21T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>虽然我们可以将UI设计的尽可能的响应式，但是也可以为横屏应用单独进行布局达到更好的效果。横屏布局是通过layout-land文件夹中的同名layout文件实现的。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1859" sha1="liwa8sk4a95kfwcevsuea1u3j4cfb34">虽然我们可以将UI设计的尽可能的响应式，但是也可以为横屏应用单独进行布局达到更好的效果。横屏布局是通过layout-land文件夹中的同名layout文件实现的。

= 创建landscape布局=
在新版的Android Studio中，默认生产的工程中是没有layout-land文件夹的，我们也不必手动创建这样的文件夹。在design界面，可以快捷的创建横屏布局：

[[File:create_land_layout.png|600px|create land layout]]

= 代码中的实现=
值得注意的是，虽然land的布局文件已经加上了，但我发现我的界面在旋转的时候并没有生效（竖屏的时候旋转屏幕），还是显示的竖屏的设计界面，研究了一下发现问题所在：

&lt;syntaxhighlight lang="xml"&gt;
&lt;activity
            android:name=".SplashActivity"
            android:configChanges="orientation|keyboardHidden|screenSize"
            android:theme="@style/Theme.AppCompat.NoActionBar"&gt;
&lt;/syntaxhighlight&gt;

可以看出android:configChanges中有orientation这一个mask，意味着当屏幕旋转时，安卓设备不会自己去处理这个事件，所以也就没有生效。解决方案有两种：

一个是删掉这个android:configChanges中的orientation选项，这样旋转屏幕的时候，android会销毁掉activity并重新创建一个，当然这时候如果有一些数据需要保存的话也就没有了。

另一个是保留这个选项，在代码中处理：

&lt;syntaxhighlight lang="java"&gt;
@Override
public void onConfigurationChanged(Configuration newConfig) {
    super.onConfigurationChanged(newConfig);

    setContentView(R.layout.activity_splash);
}

&lt;/syntaxhighlight&gt;
这样不会销毁这个activity。

[[File:land_layout.png|600px|land layout]]{style="height:200px;width:400px"}

[[File:portrait_layout.png|600px|portrait layout]]{style="width:200px"}</text>
      <sha1>liwa8sk4a95kfwcevsuea1u3j4cfb34</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:在Android 上使用 OpenCV</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2017-11-13T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>如题，本文将记录如何在安卓上调用OpenCV。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6388" sha1="h5c348vma6l5nszoc7qr3wphqq52jge">如题，本文将记录如何在安卓上调用OpenCV。

= 导入OpenCV动态库=
首先当然是下载OpenCV for Android了，然后使用Android Studio创建一个工程并勾选C++ support。

然后，把OpenCV-android-sdk里面的native目录拷贝到工程中，例如app/opencv这个目录，需要修改以下文件：

* app/build.gradle

&lt;syntaxhighlight lang="groovy"&gt;
android {
....
sourceSets {
        main {
            jniLibs.srcDirs = ['opencv/libs']
        }
    }
}
&lt;/syntaxhighlight&gt;
这样做的目的是为了打包的时候能自动将opencv/libs/{arch}/libopencv_java3.so这个文件打包到我们的apk中。

* app/CMakeList.txt
&lt;pre&gt;
set(opencv "${CMAKE_SOURCE_DIR}/opencv")
include_directories(${opencv}/jni/include)
add_library(libopencv_java3 SHARED IMPORTED )
set_target_properties(libopencv_java3 PROPERTIES
                      IMPORTED_LOCATION "${opencv}/libs/${ANDROID_ABI}/libopencv_java3.so")

target_link_libraries( # Specifies the target library.
                       native-lib
                       libopencv_java3
                       ${log-lib} )
&lt;/pre&gt;
这里把opencv作为动态库链接到工程中，并添加了包含目录，否则在编译cpp的时候会找不到opencv的头文件。

= 导入OpenCV Jaba Module=

把opencv sdk下面的java目录作为一个module导入到工程中，并设置app依赖此module，这样就可以在工程中使用opencv提供的java接口了。我们来做一个相机：
&lt;syntaxhighlight lang="java"&gt;
public class MainActivity extends AppCompatActivity implements CameraBridgeViewBase.CvCameraViewListener2
&lt;/syntaxhighlight&gt;
这里首先实现CvCameraViewListener2接口，来实现相机的处理。
&lt;syntaxhighlight lang="java"&gt;
    private CameraBridgeViewBase cameraView;

    private BaseLoaderCallback loaderCallback = new BaseLoaderCallback(this) {
        @Override
        public void onManagerConnected(int status) {
            switch (status) {
                case LoaderCallbackInterface.SUCCESS:
                    cameraView.enableView();
                    break;
                default:
                    super.onManagerConnected(status);
                    break;
            }
        }
    };

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        this.requestPermissions();

        this.getWindow().addFlags(WindowManager.LayoutParams.FLAG_KEEP_SCREEN_ON);
        setContentView(R.layout.activity_main);
        this.cameraView = (CameraBridgeViewBase) this.findViewById(R.id.cameraView);
        this.cameraView.setVisibility(SurfaceView.VISIBLE);
        this.cameraView.setCvCameraViewListener(this);
    }
&lt;/syntaxhighlight&gt;
在create的时候，我们申请权限，然后设置相机view的监听为自身。
&lt;syntaxhighlight lang="java"&gt;
    @Override
    public void onCameraViewStarted(int width, int height) {
    }

    @Override
    public void onCameraViewStopped() {
    }
&lt;/syntaxhighlight&gt;
相机启动停止我们不需要做别的操作。
&lt;syntaxhighlight lang="java"&gt;
    @Override
    protected void onResume() {
        super.onResume();
        if (!OpenCVLoader.initDebug())
            OpenCVLoader.initAsync(OpenCVLoader.OPENCV_VERSION_3_0_0, this, this.loaderCallback);
        else
            this.loaderCallback.onManagerConnected(LoaderCallbackInterface.SUCCESS);
    }
&lt;/syntaxhighlight&gt;
相机继续的时候，我们重新加载OpenCV库。
&lt;syntaxhighlight lang="java"&gt;
    @Override
    public void onPause() {
        super.onPause();
        if (this.cameraView != null)
            this.cameraView.disableView();
    }

    @Override
    public void onDestroy() {
        super.onDestroy();
        if (this.cameraView != null)
            this.cameraView.disableView();
    }
&lt;/syntaxhighlight&gt;
暂停和销毁的时候，我们把相机禁用掉。
&lt;syntaxhighlight lang="java"&gt;
    @Override
    public Mat onCameraFrame(CameraBridgeViewBase.CvCameraViewFrame inputFrame) {
        Mat frame = inputFrame.rgba();
        Core.rotate(frame, frame, Core.ROTATE_90_CLOCKWISE);
        return frame;
    }
&lt;/syntaxhighlight&gt;
这是关键的一步，处理相机的一帧。我们队图像进行了旋转，否则图像的坐标和我们的预期的是不一致的。注意在OpenCV3.2的时候，引入了便捷的rotate函数，如果用之前的方法，可能需要flip和reverse来实现了。
&lt;syntaxhighlight lang="java"&gt;
private void requestPermissions() {
        int permissionCheck = ContextCompat.checkSelfPermission(this, Manifest.permission.CAMERA);
        if (permissionCheck == PackageManager.PERMISSION_GRANTED)
            return;

        ActivityCompat.requestPermissions(this,
                new String[]{Manifest.permission.CAMERA},
                0);

    }
&lt;/syntaxhighlight&gt;
最后是权限的动态申请。当然了，在AndroidManifest.xml中也需要进行设置，我们直接贴代码了：
&lt;syntaxhighlight lang="xml"&gt;
&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;manifest xmlns:android="http://schemas.android.com/apk/res/android"
    package="com.riguz.okapia"&gt;

    &lt;uses-permission android:name="android.permission.WRITE_EXTERNAL_STORAGE" /&gt;
    &lt;uses-permission android:name="android.permission.READ_EXTERNAL_STORAGE" /&gt;
    &lt;uses-permission android:name="android.permission.CAMERA" /&gt;

    &lt;uses-feature android:name="android.hardware.camera" /&gt;
    &lt;uses-feature android:name="android.hardware.camera.autofocus" /&gt;
    &lt;uses-feature android:name="android.hardware.camera.front" /&gt;
    &lt;uses-feature android:name="android.hardware.camera.front.autofocus" /&gt;

    &lt;application
        android:allowBackup="true"
        android:icon="@mipmap/ic_launcher"
        android:label="@string/app_name"
        android:roundIcon="@mipmap/ic_launcher_round"
        android:supportsRtl="true"
        android:theme="@style/AppTheme"&gt;
        &lt;activity android:name=".MainActivity"&gt;
            &lt;intent-filter&gt;
                &lt;action android:name="android.intent.action.MAIN" /&gt;

                &lt;category android:name="android.intent.category.LAUNCHER" /&gt;
            &lt;/intent-filter&gt;
        &lt;/activity&gt;
    &lt;/application&gt;

&lt;/manifest&gt;
&lt;/syntaxhighlight&gt;

参考：[http://blog.codeonion.com/2016/04/09/show-camera-on-android-app-using-opencv-for-android/ Use OpenCV to show camera on android App with correct orientation]</text>
      <sha1>h5c348vma6l5nszoc7qr3wphqq52jge</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:一个数据导入的有趣问题</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2019-01-14T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>相比于传统的单体应用，在基于微服务架构的系统中进行数据导入的操作显得更加复杂一点。通常而言，微服务的架构中包含了多个服务，服务的技术架构也可能大相径庭，同时考虑到拓展的需要，每个服务都有可能会拓展成多个instance。最近遇到一个有趣的问题，进行了一些思考。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3381" sha1="rsezrzeqgrm5fpw94nk4h0wpzzc3fbb">
相比于传统的单体应用，在基于微服务架构的系统中进行数据导入的操作显得更加复杂一点。通常而言，微服务的架构中包含了多个服务，服务的技术架构也可能大相径庭，同时考虑到拓展的需要，每个服务都有可能会拓展成多个instance。最近遇到一个有趣的问题，进行了一些思考。
场景大致是这样的：

* 我们有一个单独的service（以下简称SM），每天定时从一个目录读取文件（一个压缩包）。其中这个包中包括多个文件，分别对应到不同的业务数据，这些数据又影响到多个不同的service（以下简称SA，SB，SC）
* 于是SM读取到文件之后，解析文件，并通过消息发送给SA，SB，SC。收到消息大小的限制，文件中的内容不能一次性发送完成，需要拆分成N个消息（比如每200条数据一个消息）
* SA，SB都是增量更新，因此收到数据后，要么新增，要么更新，就可以了。很完美。
* 但是SC确每次都是全量更新。

问题来了，如果按照SA，SB的做法，SC面临的问题有两个：

* 如果没有办法区分一条数据是新增还是更新，那直接有问题
* 即便可以，如何能删除多余的数据？ 譬如原来有200条，现在过来150条，这150条更新了，多余的50条则没有办法删除

一个直接的办法是在SM开始的时候，先去SC删除所有的数据，然后再将数据发送过去就可以了。但是这样带来的问题就是，万一后面导入失败了，而删除已经做了，会造成以前的数据也不可用。这在我们的业务场景里面是比较致命的，应该至少保证同步失败的时候，保持上一次的数据。基于这个场景，想了一个解决方案:

* 在数据表中加一个version字段标识，表示是哪一批次的数据
* 创建一个视图，根据version来查询出最新的数据
* 同步完成之后，更新视图

下面详细来说。假定我们是一个K-V类似的配置的导入，首先创建表和视图:

&lt;syntaxhighlight lang="sql"&gt;
create table t_raw_config(
id int not null primary key auto_increment,
`version` char(6) not null,
name varchar(50) not null,
value varchar(100)
);

create view t_config
as
select id, name, value from t_raw_config;
&lt;/syntaxhighlight&gt;

在SM中，我们需要做的事情是：

* 在导入开始的时候，生成一个唯一的version，简单一点，我们根据日期来，比如${\displaystyle version=20190101}$
* 假定有1001条数据，每200个拆分成1个message，则有6个${\displaystyle message=\{1, 2, 3, 4, 5, 6\}}$
* SC中，每消费完成一个message，则记录下消费的message到一个列表中，例如${\displaystyle  messages20190101 = \{1, 2\}}$（例如存储在redis中）
* SM中解析完成后，告知SC所有的message，即${\displaystyle message=\{1, 2, 3, 4, 5, 6\}}$
* SC收到告知后，比对${\displaystyle  messages20190101}$是否与${\displaystyle message=\{1, 2, 3, 4, 5, 6\}}$匹配，如果不匹配则等待（可以利用redis的BLPOP实现，如果不匹配则不断去BLPOP）
* 所有的消息都消费完成后，更新视图，将version设置为${\displaystyle version=20190101}$。如果考虑到性能问题，可以再version字段上建索引；考虑空间问题，可以在这一步以前version的数据



</text>
      <sha1>rsezrzeqgrm5fpw94nk4h0wpzzc3fbb</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:关于Event Sourcing架构</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2017-06-23T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>在微服务实践中，也许一致性是最头疼的问题了，因为跨数据库的事物将变得十分困难。我们让每一个微服务的数据存储都私有化来实现服务之间的解耦，无可避免存在很多业务需要操作多个微服务的数据库，可能不仅仅是跨服务的不同表，还可能是不同的数据库类型。如果我们采用一个数据库可能事情就会简单了，但这就脱离了微服务的真正价值了。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3141" sha1="lx8k0kbm6vtdvozhqrj29g76540ap5a">
在微服务实践中，也许一致性是最头疼的问题了，因为跨数据库的事物将变得十分困难。我们让每一个微服务的数据存储都私有化来实现服务之间的解耦，无可避免存在很多业务需要操作多个微服务的数据库，可能不仅仅是跨服务的不同表，还可能是不同的数据库类型。如果我们采用一个数据库可能事情就会简单了，但这就脱离了微服务的真正价值了。

现阶段解决分布式事物大致有这些方案：

* [https://en.wikipedia.org/wiki/Two-phase_commit_protocol 2PC]
* [https://en.wikipedia.org/wiki/Three-phase_commit_protocol 3PC]
* [https://en.wikipedia.org/wiki/Paxos_(computer_science Paxos])和[https://raft.github.io/ Raft]
* [http://cdn.ttgtmedia.com/searchWebServices/downloads/Business_Activities.pdf TCC]

姑且不论实现的复杂性，以上方案大多数可以实现[https://en.wikipedia.org/wiki/Eventual_consistency 最终一致性]。为什么说大多呢？因为太复杂了，我还没研究清楚...

而Event Sourcing目测是一个更简单的实现最终一致性的方案，采用Event Sourcing + CQRS来实现读写分离，具体是什么关系呢?这里引用一点：
&gt;在CQRS中，查询方面，直接通过方法查询数据库，然后通过DTO将数据返回。在操作(Command)方面，是通过发送Command实现，由CommandBus处理特定的Command，然后由Command将特定的Event发布到EventBus上，然后EventBus使用特定的Handler来处理事件，执行一些诸如，修改，删除，更新等操作。这里，所有与Command相关的操作都通过Event实现。这样我们可以通过记录Event来记录系统的运行历史记录，并且能够方便的回滚到某一历史状态。Event Sourcing就是用来进行存储和管理事件的。

再来看一张图:

[[File:https://www.codeproject.com/KB/architecture/555855/CQRS.jpg|600px|CQRS]]

参考文章:

* [https://mp.weixin.qq.com/s?__biz=MzA5Nzc4OTA1Mw==&amp;mid=2659597948&amp;idx=1&amp;sn=754df1597fd042537be8c25d073d3c98&amp;scene=0#rd 基于Event Sourcing和DSL的积分规则引擎设计实现案例 ]
* [https://www.nginx.com/blog/event-driven-data-management-microservices/ Event-Driven Data Management for Microservices]
* [https://en.wikipedia.org/wiki/CAP_theorem CAP]
* [https://martinfowler.com/eaaDev/EventNarrative.html Focusing on Events]
* [https://www.martinfowler.com/eaaDev/RetroactiveEvent.html Retroactive Event]
* [https://www.martinfowler.com/eaaDev/DomainEvent.html Domain Event]
* [https://github.com/cer/event-sourcing-examples Event-Sourcing+CQRS example application]
* [http://kb.cnblogs.com/page/161050/ 领域驱动设计(Domain Driven Design)参考架构详解]
* [http://codebetter.com/gregyoung/2010/02/16/cqrs-task-based-uis-event-sourcing-agh/ CQRS, Task Based UIs, Event Sourcing agh!]
* [www.codeproject.com/Articles/555855/Introduction-to-CQRS Introduction to CQRS]
* [https://ookami86.github.io/event-sourcing-in-practice/ Event Sourcing in practice]
* [http://thinkbeforecoding.com/post/2013/07/28/Event-Sourcing-vs-Command-Sourcing Event Sourcing vs Command Sourcing]</text>
      <sha1>lx8k0kbm6vtdvozhqrj29g76540ap5a</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:分布式系统 ID 生成</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2017-09-27T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>ID生成一直是一个老生常谈的问题，个人不习惯使用ID自增的方式，从0开始...每次递增，原因是因为low...最近研究了一下ID生成的算法，主要用来给我们的订单系统用。首先说一下背景：我们的系统提供给很多经销商使用，每个经销商登录到我们的系统，发生业务、产生订单。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4344" sha1="oomxfsxj09morz34a8uqhl5e7kpnojp">ID生成一直是一个老生常谈的问题，个人不习惯使用ID自增的方式，从0开始...每次递增，原因是因为low...最近研究了一下ID生成的算法，主要用来给我们的订单系统用。首先说一下背景：我们的系统提供给很多经销商使用，每个经销商登录到我们的系统，发生业务、产生订单。

= 设计原则=

* **唯一性**：保证生成的ID不会出现重复，需要考虑分布式部署情况下的唯一性
* **安全性**：不会通过订单号泄露流水信息（比如别人通过订单号知道你一天大概多少交易量）
* **长度**：长度未明确要求，在满足其他需要的情况下应该尽量短
* **字符集**：字符未明确要求，按允许数字 + 英文字母（大写）设计
* **性能要求**：如果按500家商家上线，每家每天1000次订单生成，其中高峰期每分钟10单，系统能支持运行10年的指标来计算，每天的订单数为500 * 1000 = 50W，高峰期每分钟为500 * 10 = 5000单，10年为 500 * 1000 * 365 * 10 = 1825000000单
* **易读性**：如果使用英文字母，则可以考虑去除掉容易混淆的字母：O I S Z 等

= Option 1：Snowflake算法=

参考twitter的snowflake 64bit ID生成算法，设计如下的ID规则：

0 00000000 00000000 00000000 00000000 00000000 0 00000000 00 00000000 0000

* 1  bit 符号位，固定为0，不使用
* 41bit时间戳，即当前时间到某一开始时间的毫秒数
* 10bit 商家标识，可根据其上线时间递增
* 12bit 序列号，每一毫秒内从0开始，如果超过最大值则等待下一毫秒

其中41bit时间戳最大可支持的时间：
(2^41) / (365 × 24 × 60 × 60 × 1000) = 2199023255552 / 31536000000 ≈ 69.7(年)

10bit 标识 最大支持的商家数为：
2^10 = 1024

12bit 序列号最大支持为每秒：
2^12 = 4096 

生成的ID大概像这样：
&lt;pre&gt;
274432
...
278501
278502
278503
278504
...
109996123808346299
109996123808346300
109996123808346301
….
2647243359755833344
2647243359755833345
2647243359755833346
&lt;/pre&gt;
如果需要支持分布式部署，则可考虑将商家标识的10位中插入机器标识，根据最大可能的机器节点数来选取位数。比如分配3位，就可以支持2^3=8台机器。同样，时间戳可以浮动，一般的系统根本不需要考虑超过5年以上的情况，能用两年就不错了...当然我倾向于支持10年，10年后如果世界末日不到，系统还在用，那就再想办法吧。

= Option 2 : 时间戳 + 商家标识 + 随机数=

== 时间（6位）+ 商家标识 （4位） + 随机数（4位）==

170911 0001 8774

其中，商家标识和随机数取值范围为[0-9A-Z]即
T = 26 + 10 = 36(种)

其中，商家标识最大支持的商家数为：
T × T × T × T = T^ 4 = 1679616

随机数支持范围同商家标识。因时间精确到天，因此随机数每日不能重复，每天最大支持的订单数（每个商家）即为：
T^ 4 = 1679616

== 时间（3位）+ 商家标识 （2位） + 随机数（2位）==

 79A 0A F1

其中年、月、日各占一位，其中年可以取年份-2017年的差值，超过9则用字母表示，例如2017年9月25日，可表示为：
09M

这是简短的方案，年份只取一位则最多只可以使用T = 36年，否则会出现重复

其中商家标识最大支持：
T ^ 2 = 1296

每天每一个商家标识最多只能支持T^ 2 = 1296笔订单号，若不能满足业务需求则可以将随机数扩展为3-4位

== 时间（8位）+ 商家标识 （2位） + 序列号（2位）==

65DEF7BE AF 35
65DEF7BE （16进制）= 1709111230（10进制）

因时间精确到分钟，随机数按照每分钟递增，因此每分钟可以支持订单数：
T ^ 2 = 1296
每天可支持的订单数为：
24 × 60 × T^ 2 = 1866240

= Option 3 : 时间戳  + 随机数=

时间戳（6位） + 随机数（4位）
170911 9527

使用4位数字则每日最大只能9999单，使用字母则可以有
T^ 4 = 1679616 单
若按500家商家算平均每个商家每天有3359单

= Option 4 : 渠道号 + 时间戳 +（流水号 +用户标识+随机数）=

R 79A 0011 9527 34

其中，流水号按用户按天生成，用户标识取用户ID后四位，例如：
流水号（4位） + 用户标识（4位） + 随机数（2位）
0011 9527 34 </text>
      <sha1>oomxfsxj09morz34a8uqhl5e7kpnojp</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:使用Spring Cloud Contract进行契约测试</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2017-08-10T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>研究了一下契约测试，这个概念听着很高端，其实解决的是一个很古老的问题：系统间的接口定义。以前我们做系统同其他系统对接的时候需要定义接口，需要去设计，去确认；尤其是当下微服务比较盛行的时候，我们自己的系统之间也增加了接口，伴随着敏捷开发的流程，很多时候接口在一开始根本都不会去设计，想到哪改到哪.....于是就出现了所谓的契约测试的东西。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7328" sha1="po2jjvxhn7xf32aelpp13m0wn4j1qpo">
研究了一下契约测试，这个概念听着很高端，其实解决的是一个很古老的问题：系统间的接口定义。以前我们做系统同其他系统对接的时候需要定义接口，需要去设计，去确认；尤其是当下微服务比较盛行的时候，我们自己的系统之间也增加了接口，伴随着敏捷开发的流程，很多时候接口在一开始根本都不会去设计，想到哪改到哪.....于是就出现了所谓的契约测试的东西。
先来说说契约测试解决的问题吧：

* consumer在依赖的provider接口没有实现的时候可以用stub模拟
* provider可以测试自身的接口是否满足接口定义
* consumer和provider都以契约为准，但接口有变动时修改契约，否则测试通不过...~
* 可以对边界进行测试

大概就是这样吧，我觉得前两条是最重要的Feature，举个例子，比如我们有一个Vehicle的服务，用来根据vin(车辆底盘号）来获取车辆的信息；一个Costomer的服务需要调用这个服务来获取客户的车辆信息。我们的Vehicle接口如下：

&lt;syntaxhighlight lang="java"&gt;
@GetMapping("/vehicle/{vin}")
VehicleDetail getVehicleDetail(@PathVariable String vin){
   VehicleDetail item = this.vehicleService.getVehicle(vin);
   if(item == null)
       throw new VehicleNotFoundException();
   return item;
}
&lt;/syntaxhighlight&gt;

我们在Vehicle服务中定义一个契约：

&lt;syntaxhighlight lang="groovy"&gt;
Contract.make {
    request {
        method 'GET'
        url value('/vehicle/WDC1660631A7506890')
    }
    response {
        status 200
        body([
                vin           : 'WDC1660631A7506890',
                brand         : 'Audi X5',
                owner         : 'James 王',
                registeredDate: 1502347667000,
                mileage       : 1200
        ])
        headers {
            header('Content-Type': value(
                    producer(regex('application/json.*')),
                    consumer('application/json')
            ))
        }
    }
}
&lt;/syntaxhighlight&gt;

这样我们在执行gradle的`generateContractTests`任务的时候会自动生成一个契约测试，我们在测试Vehicle服务的时候，只需要Mock我们的Service，返回对应的模拟信息：

&lt;syntaxhighlight lang="java"&gt;
@Before
public void setUp() throws Exception {
    VehicleDetail i = new VehicleDetail();
    i.setVin("WDC1660631A7506890");
    i.setOwner("James 王");
    i.setBrand("Audi X5");
    i.setRegisteredDate(new Date(1502347667000L));
    i.setMileage(1200);
    RestAssuredMockMvc.webAppContextSetup(context);

    given(vehicleService.getVehicle("WDC1660631A7506890")).willReturn(i);
}
&lt;/syntaxhighlight&gt;

刚刚的契约是一个很固定的数据，我们还可以加上正则表达式的检测：

&lt;syntaxhighlight lang="groovy"&gt;
Contract.make {
    request {
        method 'GET'
        url value(consumer(regex('/vehicle/[A-Z0-9]{18}')),
                producer('/vehicle/WDC1660631A7506890'))
    }
    response {
        status 200
        body([
                vin           : $(producer(regex(/[A-Z0-9]{18}/))),
                brand         : $(producer(anyNonBlankString())),
                owner         : $(producer(anyNonBlankString())),
                registeredDate: $(producer(regex(/[1-9][0-9]{11,12}/))),
                mileage       : $(producer(regex(/[1-9][0-9]{0,10}/)))
        ])
        headers {
            header('Content-Type': value(
                    producer(regex('application/json.*')),
                    consumer('application/json')
            ))
        }
    }
}
&lt;/syntaxhighlight&gt;

以及异常情况下的测试：

&lt;syntaxhighlight lang="groovy"&gt;
Contract.make {
    request {
        method 'GET'
        url value(consumer(regex('/vehicle/\\w.+')),
                producer('/vehicle/XXXXX'))
    }
    response {
        status 404
    }
}
&lt;/syntaxhighlight&gt;

这样每一个groovy文件都会对应着生成一个测试，达到我们测试Provider的目的。那么，对于客户端来说，怎么测试呢？很简单，我们执行gradle的`install`命令，会把生成的stub包放到本地的gradle源中，我们在客户端测试的时候可以这么写：

&lt;syntaxhighlight lang="java"&gt;
@RunWith(SpringRunner.class)
@SpringBootTest
@AutoConfigureStubRunner(ids = "com.riguz:foo:+:stubs:10000", workOffline = true)
public class CustomerServiceTest {

    @Autowired
    private CustomerService customerService;

    @Test
    public void shouldReturnCustomerDetail(){
        CustomerInfo info = this.customerService.getCustomerInfo("123");
        System.out.println(info);
        assertEquals(1, info.getVehicles().size());
        // ...
    }
}
&lt;/syntaxhighlight&gt;
对应着`~/.m2/repository/com/riguz/foo/1.0-SNAPSHOT/foo-1.0-SNAPSHOT-stubs.jar`，`+`表示取最新版本，`10000`是端口号，也就是模拟出了一个远程的服务端。这样如果契约有修改的话，取到新的契约stubs包也会跟着修改了。

另外，如果单纯的想模拟一个服务端怎么办？有办法，我们在provider中执行gradle的`generateClientStubs`命令后，会生成一个mappings目录，在`build/stubs/....`下面。里面有一些json文件，例如我们的：

&lt;syntaxhighlight lang="json"&gt;
{
  "id" : "5dd47b81-a184-4b9e-be02-b6e22c409c81",
  "request" : {
    "url" : "/vehicle/WDC1660631A7506890",
    "method" : "GET"
  },
  "response" : {
    "status" : 200,
    "body" : "{\"vin\":\"WDC1660631A7506890\",\"brand\":\"Audi X5\",\"owner\":\"James \\u738b\",\"registeredDate\":1502347667000,\"mileage\":1200}",
    "headers" : {
      "Content-Type" : "application/json"
    },
    "transformers" : [ "response-template" ]
  },
  "uuid" : "5dd47b81-a184-4b9e-be02-b6e22c409c81"
}
&lt;/syntaxhighlight&gt;
我们可以通过wiremock-standalone来启动一个模拟的服务端。

&lt;syntaxhighlight lang="bash"&gt;
java -jar wiremock-standalone-2.7.1.jar
# 启动后会自动创建一个mappings目录，把我们生成的mappings目录中的内容拷贝进去，再重新运行即可
&lt;/syntaxhighlight&gt;
这样访问`http://localhost:8080/vehicle/WDC1660631A7506890`就可以得到我们的契约里面写的模拟数据了。

好了，如果有疑问请参考[https://github.com/soleverlee/spring-contract-example.git 完整的代码]，建议参考末尾的参考文章，本文不过是跟着写了一下而已。

总结一下吧，其实并没有感觉到Contract Test有多高端，不过很适用与微服务+敏捷开发这种场合。来说说我觉得不足的地方：

* 契约测试依然是测试，无法替代设计，如果设计的接口是一坨*测试的再好又有什么呢；并不是反对测试，而是感觉但凡重视测试的同时容易轻视设计（或者说测试能力要比设计能力强太多...)
* 如果是同三方系统对接，如何来操作呢？
* 对于一些其他的客户端就勉为其难了，比如NodeJS的客户端，无法使用生成的stubs.jar文件，客户端怎么保证得到的东西是自己想要的结果?

参考文章:

* [Consumer-Driven Contract Testing with Spring Cloud Contract
](https://specto.io/blog/2016/11/16/spring-cloud-contract/)
* [http://cloud.spring.io/spring-cloud-contract/spring-cloud-contract.html Spring Cloud Contract Document]</text>
      <sha1>po2jjvxhn7xf32aelpp13m0wn4j1qpo</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:浅谈CDC在微服务中的应用</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2019-06-16T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>CDC(Change Data Capture)是一种通过监测数据变更（变更包括新增、修改、删除等）而对变更的数据进行进一步处理的一种设计模式，通常应用在数据仓库以及和数据库密切相关的一些应用上，比如数据同步、备份、审计、ETL等。实际上，早在二十多年前，CDC就已经用来将应用系统的数据变更实时发送到数据仓库，进一步转换后传递到数据分析系统^[[Streaming Change Data Capture by Itamar Ankorion, Dan Potter, Kevin Petrie](https://www.oreilly.com/library/view/streaming-change-data/9781492032526/ch01.html)]，这样能够在极小地影响生产的情况下，有效而及时地将数据传递到消费方。而在微服务架构逐渐流行的今天，这种古老的技术是否能够焕发新的生机？

![CDC Flow in SQL Server](/images/cdc_flow.png)</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10899" sha1="7w2n21hce9gyaksp73unsycgr347tz9">CDC(Change Data Capture)是一种通过监测数据变更（变更包括新增、修改、删除等）而对变更的数据进行进一步处理的一种设计模式，通常应用在数据仓库以及和数据库密切相关的一些应用上，比如数据同步、备份、审计、ETL等。实际上，早在二十多年前，CDC就已经用来将应用系统的数据变更实时发送到数据仓库，进一步转换后传递到数据分析系统^[https://www.oreilly.com/library/view/streaming-change-data/9781492032526/ch01.html [Streaming Change Data Capture by Itamar Ankorion, Dan Potter, Kevin Petrie]]，这样能够在极小地影响生产的情况下，有效而及时地将数据传递到消费方。而在微服务架构逐渐流行的今天，这种古老的技术是否能够焕发新的生机？

[[File:cdc_flow.png|600px|CDC Flow in SQL Server]]

= CDC实现原理=
在说CDC在微服务中的应用之前，我们有必要先了解一下CDC的基本原理。关键也许就在如何监测数据的变更。拿MySQL来说，我们知道MySQL中有binlog(binary log)可以记录用户对数据库进行的修改事件^[[mysqlbinlog — Utility for Processing Binary Log Files
](https://dev.mysql.com/doc/refman/5.6/en/mysqlbinlog.html)]，顺理成章，一个最简单高效的CDC实现就可以利用binlog来完成。当然现在已经有很多[https://github.com/wushujames/mysql-cdc-projects/wiki 开源的MySQL CDC实现]，开箱即用。使用binlog并不是唯一的途径，至少对于MySQL而言，甚至利用数据库触发器也能完成类似的功能，但从效率上以及对数据库影响的层面来看可能就相形见绌了。

通常而言，CDC捕获到数据库的变更之后，会将变更事件发布到消息队列中供消费者消费，例如[https://debezium.io/docs/tutorial/#what_is_debezium Debezium]，将MySQL（也支持PostgreSQL、Mongo等）的变更持久化到Kafka中，我们通过订阅Kafaka中的事件，来实现我们需要的功能。

= 微服务解耦=
我们刚才已经了解到，通过CDC，我们可以把数据库的变更转变为各个“事件”，从而可以只关心这些事件来处理。对于传统的大型单体应用，我们可以通过这种方式来进行解耦，从而拆分出微服务出来。同样，如果已经是微服务架构，有时候也可以运用CDC来简化服务间的调用。

举个例子，我们在奔驰OTR中有这样的场景：

* 系统中会创建客户服务的预约，预约将分配给一个用户去处理
* 这些预约可能是用户手动创建的，也可能是通过第三方系统发送过来的
* 当系统中创建了预约、或者预约修改后，相关的用户会收到通知信息

很容易我们可以拆分出预约、通知两个服务出来，为了实现通知的功能，如果采用直接的做法，我们在通知的服务中定义了一个接口是给某用户发送通知，这样我们在所有预约创建、修改的地方都需要进行相应的逻辑判断，并调用这个接口来发送通知。而实际上，我们有好几处地方都在创建或者修改预约信息，当这些业务代码需要修改的时候，我们都需要关心通知的部分是否需要作出修改。虽然我们很小心的在维护这部分代码，但还是很容易会漏掉一些地方的通知逻辑，或者出现业务上不一致的情况。

试想如果我们利用CDC的方式，不是直接显示的在预约变更的地方调用通知接口，而是监控预约表的变化，然后有一个地方统一的进行处理并发送通知，这样可以极大的降低业务代码的复杂度。同时要考虑的是，这样不再是一个同步的操作，这个延时对于业务场景是否能接受。在我们这个案例中，消息发送（通过第三方平台）本身就已经是一个异步调用了，业务上并没有发生变化，是可以考虑的。

= Data replication=

Martin Fowler在他关于Event-Driven Architecture的演讲中提到一个使用事件传递业务变更来解决跨服务的信息共享问题^[https://www.youtube.com/watch?v=STKCRSUsyP0 [GOTO 2017 • The Many Meanings of Event-Driven Architecture • Martin Fowler]]。
[[File:event_carried_state_transfer.png|600px|Event carried data transfer]]

上图的架构中在Insurance Quoting服务中保存了一份customer的信息，这样当有需要查询的时候，不需要去调用customer management服务，而是直接从自己的副本中进行查询，这样做有一些好处：

* 提高了查询性能，直接从数据库里面拿，省去了远程调用
* 不用担心另外一个服务挂掉或者性能造成的影响，customer management挂了还可以用
* 缓解了customer management的压力

虽然好处不少，但是实现起来也不是那么容易，最大的问题就在于如何保证数据的一致（同步）：数据发生了变化，如何告诉给我们？使用CDC来完成这个操作是比较合适了。通过CDC，我们可以将依赖系统的数据（只需要处理我们关心的部分）replicate到自身系统中，来支持自身系统的业务需要。

本质上，这样和MySQL的主从复制类似；好处在于，对于异构的系统，能够有较为统一的方式完成数据的同步。比如customer management存储在mysql，而我们可能使用mongodb或者postgresql，只要customer management能支持CDC，是可以很方便的将customer management的数据迁移过来。

= 实现CQRS=
刚才提到的一个场景是消息通知，和类似的还有用户积分变更、数据统计及报表、用户行为分析等，通常这类业务对实时性要求不高，但又经常伴随着实时性要求较高的业务而发生，可以认为是一些用户行为导致的副作用。在上面的例子中，我们通过维护一个只读的customer数据库来进行查询操作，从某种意义上来讲算是读写分离了。CQRS^[https://martinfowler.com/bliki/CQRS.html [Command Query Responsibility Segregation]]正是一种读写分离的策略，对于查询和写操作分别用不同的模型，来优化查询的表现。

[[File:https://medium.com/eleven-labs/cqrs-pattern-c1d6f8517314|600px|Example of CQRS^[[CQRS Pattern]]]](/images/CQRS_demo.png)

如图所示，我们分别用两个不同的数据库来支持查询和写入的部分，将其分割开来。这样对于查询来说，是可以进行优化的，比如可以选择NoSQL来结构化视图，查询的时候不需要进行太多额外的处理，并可以考虑根据读操作和写操作不同的性能要求进行伸缩。

在这样应用CQRS的架构中，CDC就可以用来将写入的事件同步到查询的数据库中，在上图中左侧的Events位置，我们不需要在业务代码中去显示地发布事件，只需要通过CDC来监测写库中的改变即可。

= CDC与Event Sourcing=

其实说到CRQS,通常都会和[https://martinfowler.com/eaaDev/EventSourcing.html Event Sourcing]结对出现。Event Sourcing可能是事件驱动架构的终极实现了，在这种架构的系统中，只存储客观事实也就是事件，而业务数据的状态，是通过"播放"事件而得到的。Event Sourcing是一种较为复杂的架构，通常DDD + EventSourcing搭配起来效果更好，但要完整的实现这样一个架构不是一件容易的事情。好在有一些开源的实现，可以供我们学习和参考。

[[File:https://github.com/eventuate-local/eventuate-local|600px|Eventuate Local Architecture^[[Eventuate Local]]]](https://raw.githubusercontent.com/eventuate-local/eventuate-local/master/i/Eventuate%20Local%20Big%20Picture.png)

上图是Eventuate Local的架构，这是一个CQRS+EventSourcing的开源实现。对于EventSourcing来说比较重要的是需要一个Event Store，它有几个比较重要的功能：

* 将事件可靠的持久化（只能新增新的事件而不能改变已经存在的事实）
* 根据一个ID查出一个实体上的所有有序的事件
* 将事件广播出去（当然这不应该是Event Store的职责，我们姑且认为是一体的，不然后面不好写了）

利用RDBMS来保存事件是比较简单的一个操作了，需要注意的是需要保证事件的强一致性，在并发情况下，同一个聚合下多个事件同时发生的时候，需要保证这些事件依然是有序的，这里可以采取乐观锁的方式实现。

Eventuate中有一个CDC Service的服务，实现了Event Store同时支持了事件的发布。通过对Event表进行监测，新产生的事件被发布到Kafka中，供其他service消费，这样一个过程比较自动，不用过多担心持久化和手动进行事件发布中间有一个环节出错的情况了。

= Puncturing encapsulation with change data capture=

从上面的例子中可以看到CDC是一个比较有用的设计模式，在微服务架构中大有用武之地。那为啥技术雷达要把这个列到HOLD中^[https://www.thoughtworks.com/cn/radar/techniques [technology radar]]？

关键不在于CDC，而是打开方式不对。技术雷达中有提到:

&gt;We're seeing some projects use CDC for publishing row-level change events and directly consuming these events in other services.

意思是说有一些项目在实现CDC的时候，直接将底层的事件暴露出来，这个”底层“意思是指没有经过处理的、原始的、和上游系统强绑定的。举个例子，在上面的Event Store的实现中，我们很容易就能实现一个存储Event的表^[https://cqrs.wordpress.com/documents/building-event-storage/ [Building an Event Storage]]：

Column         Type
-------------- -----------
AggregateId    UUID
Type           Varchar
Content        Varchar
Version        Integer

而我们是在存储Event的内容的时候，可以选择以Json的方式存储，也可能会直接序列化成二进制格式，如果我们不加修改就直接将这个表的变化广播出去，那么下游系统就会依赖于我们的存储结构了，而且需要自己进行数据加工才能得到自己想要的数据。换而言之，如果我们以后发生改变，那所有的订阅者都得跟着改，这个是一个很大的隐患，会使得服务间的集成相当脆弱。

那么，更好的做法是什么呢？我觉得DDD可能是解决事件驱动架构问题的一个好的途径，通过DDD的方式，我们需要思考清楚真正业务场景中的聚合与事件，建立正确的模型，从而隔离原始的数据存储，当服务底层发生变化时，只需要修改这一层的实现即可无缝迁移。

以上都是纸上谈兵，到目前为止还没有在项目中实际运用过，不清楚我司的项目是不是有使用CDC的例子？比较好奇的是技术雷达的作者们是咋知道有些项目"use CDC for publishing row-level change events"？</text>
      <sha1>7w2n21hce9gyaksp73unsycgr347tz9</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:再谈Micro Services Architecture</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2017-06-24T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>再来谈一点我对微服务的理解。首先是微服务的一个服务划分问题。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4927" sha1="71oo591z547lpwbu604ct3bjrw76p1b">
再来谈一点我对微服务的理解。首先是微服务的一个服务划分问题。
[[File:https://cdn-1.wp.nginx.com/wp-content/uploads/2016/04/Richardson-microservices-part1-2_microservices-architecture.png|600px|Micro Services]]

这张架构图我觉得比价典型了，在设计微服务架构的时候，个人认为应该有以下可以考虑的：

* 每个服务应该按照业务功能去划分，相对要独立、小，但又不能过分细微否则微服务的数量和数据共享将变得复杂
* 每个微服务有自己的数据存储，不同的微服务不应该共享数据库存储；当然如果是对某一个微服务进行多部署和负载均衡，那么这些微服务可能会共享一个数据库或者数据库集群
* 微服务间应该只通过接口调用，不应该去直接读取其他接口的数据例如数据库
* 通常情况下，微服务内部不需要进行权限认证；但必须通过API Gateway暴露给外部系统
* 微服务接口应该添加版本号，这样如果接口定义有变动，可以在不影响系统的情况下实现逐步切换

微服务对外必须走Api Gateway的理由是：

* 可以通过Gateway实现介入的权限认证等
* 可以实现负载均衡，或者内部服务的切换而对外部来说是感觉不到变化的
* 对外提供一个统一的入口

再来说说一个我认为不好的设计。首先我们来说说`前端`和`后端`，在我看来，对于B/S架构来说，前端单单就是在浏览器运行的这一部分东西，但通常人们也会把微服务中的Front End称之为前端，比如一个NodeJS实现的HTML5客户端，那这里NodeJS的东西也会被认为是前端了。虽然这两个定义究竟是对是错没什么价值所在，但我发现我们的`前端`开发人员有着一个不好的习惯：

* 直接在前端的JS中调用微服务，通过一个api的代理直接访问了后端的一个服务（类似于Gateway的一个服务但是存在很多逻辑）

什么意思呢，就是说，把逻辑都写在客户端执行的Javascript中，而不是传统意义上的`后端`。这样做的实际问题有（我发现的）：

* 逻辑混乱，对于微服务来说不大可能会直接提供可用的接口，往往需要多次调用和自行处理，这样页面会请求很多接口，所有逻辑都在页面上，调试只能依赖于浏览器
* 性能问题，如果加载大量的数据对浏览器来说是噩梦，处理起来会依赖于客户端的性能
* 安全问题，相当于直接暴露了微服务的接口出来，这样本来可以在session中做的事情需要到cookie中做了

这里其实会有争议，那么如果是有这样的业务逻辑，应该写在哪里？假设现有的微服务都是很基础的微服务。

* 肯定不应该直接写在页面上
* 写在页面对应的后端上？
* 新建一个含有逻辑的微服务，在页面上调用这个微服务？

我觉得后两种都是可以考虑的，或者说可以同时存在的。首先从部署来讲，后端和微服务应该是部署在同一个网络内，从后端直接访问微服务时是不需要进行权限验证的；因此业务逻辑应该写在后端中，这样页面访问的接口只会得到一个结果而你不能看到具体的过程，这样在一定程度上是更安全和有效的。其次，如果业务逻辑比较复杂，或者说有需要在其他地方也使用，而现有的微服务没有直接的接口时，需要考虑新建一个处理业务的微服务来处理这些逻辑。即使是这样，仍然最好在服务器端来调用这些微服务。


参考：

* [https://www.nginx.com/blog/introduction-to-microservices/ Introduction to Microservices]
* [https://www.nginx.com/blog/building-microservices-using-an-api-gateway/ Building Microservices: Using an API Gateway]
* [https://www.nginx.com/blog/building-microservices-inter-process-communication/ Building Microservices: Inter-Process Communication in a Microservices Architecture]
* [https://www.nginx.com/blog/service-discovery-in-a-microservices-architecture/ Service Discovery in a Microservices Architecture]
* [https://www.nginx.com/blog/event-driven-data-management-microservices/ Event-Driven Data Management for Microservices]
* [https://www.nginx.com/blog/deploying-microservices/ Choosing a Microservices Deployment Strategy]
* [https://www.nginx.com/blog/refactoring-a-monolith-into-microservices/ Refactoring a Monolith into Microservices]
* [http://microservices.io/patterns/data/database-per-service.html Pattern: Database per service]
* [https://martinfowler.com/bliki/PolyglotPersistence.html PolyglotPersistence]
* [https://www.infoq.com/articles/microservices-aggregates-events-cqrs-part-1-richardson Developing Transactional Microservices Using Aggregates, Event Sourcing and CQRS - Part 1]
* [http://chrisrichardson.net/learnmicroservices.html Learn microservices]</text>
      <sha1>71oo591z547lpwbu604ct3bjrw76p1b</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Microservice中的安全策略</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-12-04T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>最近在思考微服务中的安全问题，在读Chris Richardson的文章的时候，发现自己对微服务有了一些新的认识和理解:p。

如图所示是一个典型的微服务架构：

![Problem](/images/microservice_security_problem.png)

这个架构下有如下的需求场景：

* 不同的APP会消费微服务，通常需求会有差别
* 有些微服务还会希望暴露给第三方的系统进行消费
* 具体的服务通常在设计API的时候会尽量考虑通用，但是实际消费服务的时候并不能完全满足需求，还需要二次加工，或者将多个服务的调用进行组合。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3317" sha1="nql7fl4eb0waw1p0b6mcykxbqzzix64">最近在思考微服务中的安全问题，在读Chris Richardson的文章的时候，发现自己对微服务有了一些新的认识和理解:p。

如图所示是一个典型的微服务架构：

[[File:microservice_security_problem.png|600px|Problem]]

这个架构下有如下的需求场景：

* 不同的APP会消费微服务，通常需求会有差别
* 有些微服务还会希望暴露给第三方的系统进行消费
* 具体的服务通常在设计API的时候会尽量考虑通用，但是实际消费服务的时候并不能完全满足需求，还需要二次加工，或者将多个服务的调用进行组合。

= BFF和API Gateway=
在微服务架构中，有两个比较常用的概念是”API Gateway“和”Backend for Frontend(BFF)“，实际上是为了解决不同的问题而产生的解决方法：

* API Gateway解决了消费端消费微服务没有统一的入口的问题。通过API Gateway的路由，消费者只需要一个看起来“统一”的入口就可以调用所需的服务，而不是分别调用多个服务
* BFF解决问题是，有一些API并不是能够直接满足需求，需要进行组合、切割以及一些额外的逻辑进行处理，但将这些操作放在客户端解决可能会存在性能问题并对前端引入额外的复杂性。尤其是对于使用Native app技术（区别于传统的server side）的应用来说，欠缺在前端处理较为复杂的业务逻辑的能力，有的时候，微服务暴露的协议可能并不能直接在浏览器中支持，这会带来更大的麻烦。这些时候，一个server side的后端是很有必要的。

一直以来比较困扰我的是，API Gateway和BFF这两个东西究竟应该如何使用，是应该把API Gateway放在入口，让所有消费者像消费一个单体服务一样来消费微服务；还是应该将"BFF"本身作为一个微服务，跟其他服务一样藏在API gateway之后呢？

[[File:api_gateway_bff_confused.png|600px|API Gateway vs BFF]]

之前的实践方式通常为一个API Gateway用Zuul实现，一个BFF就是一个正常的微服务，正是受限于这种方式导致一直以来我的误解是，API Gateway和BFF是两个不同的东西，是分开部署的；而没有思考这两个东西其实是解决不同维度的问题而产生的，其实是可以共存的一个概念。

[[File:microservice_bff_and_api_gateway.png|600px|API Gateway vs BFF]]

如上所示，第一种方法是对于不同的应用分别定制不同的BFF，这里实际上BFF也可以充当API Gateway的作用；而第二种对于不同的APP使用相同的API Gateway，同样也可以在其中实习定制化的逻辑，为消费者提供优化后的API。

= 安全控制=

[[File:api_gateway_oauth.png|600px|OAuth2 in Microservices]]

如图所示，是一个使用Oauth2标准的授权访问流程，这里有几点值得关注：

* API Gateway负责了access_token的验证以及刷新
* 后端service需要通过OAuth2 Server来校验access_token，因为access_token中并不包含任何的身份信息

&lt;!-- tbd --&gt;

References:

* [https://microservices.io/patterns/apigateway.html Pattern: API Gateway / Backends for Frontends]
* [https://livebook.manning.com/book/microservices-security-in-action/welcome/v-8/  Microservices Security in Action]</text>
      <sha1>nql7fl4eb0waw1p0b6mcykxbqzzix64</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:关于随机数</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2018-12-03T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>随机数顾名思义就是你无法确定的一个数（但是你可以设定一个范围），就好比彩票摇号一样，所有可能的组合是知道的，但是到底会摇出个什么数字出来，谁都不知道。否则我早就买彩票去了&#x1f602; 那随机数是怎么生成出来的？</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4120" sha1="nh8so1c2nr4yf3h7x8gw98rv7wzft8u">随机数顾名思义就是你无法确定的一个数（但是你可以设定一个范围），就好比彩票摇号一样，所有可能的组合是知道的，但是到底会摇出个什么数字出来，谁都不知道。否则我早就买彩票去了&#x1f602; 那随机数是怎么生成出来的？

= 随机数的定义=
引用维基百科，

&gt; 根据密码学原理，随机数的随机性检验可以分为三个标准：

&gt; * 统计学伪随机性。统计学伪随机性指的是在给定的随机比特流样本中，1的数量大致等于0的数量，同理，“10”“01”“00”“11”四者数量大致相等。类似的标准被称为统计学随机性。满足这类要求的数字在人类“一眼看上去”是随机的。
&gt; * 密码学安全伪随机性。其定义为，给定随机样本的一部分和随机算法，不能有效的演算出随机样本的剩余部分。
&gt; * 真随机性。其定义为随机样本不可重现。实际上衹要给定边界条件，真随机数并不存在，可是如果产生一个真随机数样本的边界条件十分复杂且难以捕捉（比如计算机当地的本底辐射^[本体辐射是指人类生活环境本来存在的辐射，主要包括宇宙射线和自然界中天然放射性核素发出的射线。]波动值），可以认为用这个方法演算出来了真随机数。但实际上，这也只是非常接近真随机数的伪随机数，一般认为，无论是本地辐射、物理噪音、抛硬币……等都是可被观察了解的，任何基于经典力学产生的随机数，都只是伪随机数。

&gt; 相应的，随机数也分为三类：

&gt; * 伪随机数：满足第一个条件的随机数。
&gt; * 密码学安全的伪随机数：同时满足前两个条件的随机数。可以通过密码学安全伪随机数生成器计算得出。
&gt; * 真随机数：同时满足三个条件的随机数。

= Linux系统中的随机数设备=

Linux以及一些类Unix系统中有随机数的特殊文件，一般如下：

* /dev/random :提供基于当前系统熵池^[指设备驱动程序或其它来源的背景噪声计算出来的某种结果]的真随机数
* /dev/urandom:是非阻塞的随机数生成器

两者都是CSPRNG^[Cryptographically Secure Pseudorandom Number Generator，加密安全的伪随机数生成器]，可以使用以下命令来输出：

&lt;syntaxhighlight lang="bash"&gt;
od -An -N1 -i /dev/random
&lt;/syntaxhighlight&gt;

= 一些伪随机数生成算法=

== 平方取中法==

这个算法比较简单，由冯·诺伊曼在1946年提出。 算法步骤如下：

* 选择一个 ${\displaystyle m}$ 位数 ${\displaystyle N_{i}}$ 作为种子
* 计算 ${\displaystyle N_{i}^{2}}$
* 若 ${\displaystyle N_{i}^{2}}$不足 ${\displaystyle 2m}$个位，在前补0。在这个数选中间 ${\displaystyle m}$个位的数，即 ${\displaystyle 10^{\lfloor {\frac {m}{2}}\rfloor +1}} {\displaystyle 10^{\lfloor {\frac {m}{2}}\rfloor +1}}$至 ${\displaystyle 10^{\lfloor {\frac {m}{2}}\rfloor +m}} {\displaystyle 10^{\lfloor {\frac {m}{2}}\rfloor +m}}$的数，将结果作为 ${\displaystyle N_{i+1}}$


== 线性同余法==

这个算法根据递归公式计算:

$$
X_{n+1}=\left(aX_{n}+c\right)~~{\bmod {~}}~m
$$

Java中的Random类就是使用就是这种算法。但这个不是密码学安全的随机数算法，如果要生成密码学安全的随机数，需要使用SecureRandom类来生成。

== Blum Blum Shub==

采用如下的递归公式计算：

$$
x_{n+1}=x_{n}^{2}{\bmod  M}
$$

其中：$M=p\cdot q$是两个大素数p和q的乘积

例如令${\displaystyle p=11}$, ${\displaystyle q=19}$, ${\displaystyle s=3}$，则：

=. ${\displaystyle x_{0}=3^{2}{\bmod 209}=9}$=
=. ${\displaystyle x_{1}=9^{2}{\bmod 209}=81}$=
=. ${\displaystyle x_{2}=81^{2}{\bmod 209}=82}$=
=. ${\displaystyle x_{3}=82^{2}{\bmod 209}=36}$=
=. ...=

除此之外，还有一些其他的随机数算法，便不过多介绍。

参考: 

* [http://www.2uo.de/myths-about-urandom/ Myths about /dev/urandom]
* [http://www.cnblogs.com/Geometry/archive/2011/01/25/1944582.html 一个生成伪随机数的超级算法]</text>
      <sha1>nh8so1c2nr4yf3h7x8gw98rv7wzft8u</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Mac上的C++/CMake开发环境抉择</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2020-04-08T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>之前写过一篇文章[Visual Studio Code for C++ development on MacOS](../visual_studio_code_cpp_ide), 因为在Mac上一直没有找到免费且又比较好用的C++开发工具。但相比起Visual Studio而言，Visual Studio Code还是太简陋了。而今时不同往日了，现在比较倾向于使用CMake来构建项目，所以希望能支持CMake，所以又把各种开发工具尝试了一下。</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5726" sha1="9epehdgjtyxkcifvr8cig9rj38t407o">之前写过一篇文章[../visual_studio_code_cpp_ide Visual Studio Code for C++ development on MacOS], 因为在Mac上一直没有找到免费且又比较好用的C++开发工具。但相比起Visual Studio而言，Visual Studio Code还是太简陋了。而今时不同往日了，现在比较倾向于使用CMake来构建项目，所以希望能支持CMake，所以又把各种开发工具尝试了一下。

= Eclipse CDT=
最近Eclipse-CDT又更新了一下，再次体验了一下，而且根据之前的经验，eclipse是直接支持CMake了的，所以是我的首选目标。

== 导入CMake工程==
Eclipse CDT默认是支持CMake的，所以不用使用cmake生成eclipse的工程，直接就可以用。新建项目的时候是可以直接选用CMake项目的，那么，怎么导入一个现有的CMake工程呢？

实际上，并不能使用"Import"来导入，而是要用"File -&gt; New -&gt; C/C++ Project -&gt; Empty or Existing CMake Project"，然后就可以选择现有项目，导入到Eclipse中去了。是不是很坑？

[[File:CDT_import_cmake_project.png|600px|Import project]]

== 运行Google Test测试==
可以直接在Eclipse中运行Google Test，因为本身编译测试出来是一个应用，其实可以直接运行，但是eclipse集成的Unit Test可以让结果更好看一点：

[[File:CDT_run_tests.png|600px|Run tests]]

要运行这样的测试必须在Run Configuration中新建一个Unit Test的目标，并选择编译出来的test程序。

== Mac下字体设置==
Eclipse在Mac下有一个问题就是默认的UI字体实在太小了，看着眼睛疼。

[[File:CDT_smallfont.png|600px|默认字体]]

有一个解决办法就是安装一个[https://www.bresink.com/osx/TinkerTool.html TinkerTool]，然后设置“Help tags”字体的大小。参见这个[https://bugs.eclipse.org/bugs/show_bug.cgi?id=56558 Bug]。

[[File:CDT_14px.png|600px|更改后的字体]]

== 缺点==
也许Eclipse最大的问题就在于调试了，动不动就卡死在96%上，或者好不容易进去了，却告诉你没有调试信息，要不要看汇编？尝试了很久也没有找到解决办法，除此之外，别的都能接受。但是这个问题很致命啊... 另外一个问题就是，（即使gdb支持的也不好），eclipse是不直接支持lldb的。

= Visual Studio Code=
看来还是把希望寄托在Visual Studio Code上了。

== 安装插件==
有以下的几个插件需要安装：

* ms-vscode.cpptools: C/C++插件
* ms-vscode.cmake-tools: CMake tools
* webfreak.debug: Native debug插件

安装完成之后，就可以打开CMake工程了。使用Command+Shift+P打开命令窗口，可以看到有很多CMake相关的选项：

* CMake:Configure 配置支持CMake工程
* CMake:Clean 清理build
* ...

也可以通过系统下面的状态栏来设置，设置好之后，一般长这个样子：

[[File:Vscode-cmaketools.png|600px|Visual Studio Code]]

可以看到下面的状态栏已经集成了CMake的target。

== 调试==

=== 使用lldb-mi从状态栏启动调试===
调试有两种方式：一种是通过状态栏下面的"调试“图标启动的，不需要launch.json，但是默认情况下是不能工作的，需要做如下设置：

新建`.vscode/settings.json`，将cpptools下面lldb-mi的路径配置进去，比如我的：

&lt;syntaxhighlight lang="json"&gt;
{
    "cmake.debugConfig": {
        "miDebuggerPath": "/Users/hfli/.vscode/extensions/ms-vscode.cpptools-0.27.0/debugAdapters/lldb-mi/bin/lldb-mi"
    }
}
&lt;/syntaxhighlight&gt;

[[File:Vscode-debug-lldb-mi.png|600px|lldb-mi]]

但是这个玩意感觉支持很有限，我调试Googletest测试的的时候就直接挂掉了。


=== 创建launch.json调试===

另外一个方式是创建launch.json，这样可以从左边的调试菜单那里开始：

&lt;syntaxhighlight lang="json"&gt;
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "(gdb) Launch",
            "type": "cppdbg",
            "request": "launch",
            // Resolved by CMake Tools:
            "program": "${command:cmake.launchTargetPath}",
            "args": [],
            "stopAtEntry": false,
            "cwd": "${workspaceFolder}/build",
            "environment": [],
            "externalConsole": true,
            "MIMode": "gdb",
            "MIDebuggerPath": "/Users/hfli/.vscode/extensions/ms-vscode.cpptools-0.27.0/debugAdapters/lldb-mi/bin/lldb-mi",
            "setupCommands": [
                {
                    "description": "Enable pretty-printing for gdb",
                    "text": "-enable-pretty-printing",
                    "ignoreFailures": true
                }
            ]
        }
    ]
}
&lt;/syntaxhighlight&gt;
但上面照样使用的是lldb-mi，有着跟上面的方法同样的问题。换成gdb后就更是奇怪的到处乱跳了。


=== 使用CodeLLDB插件===

另一个选项就是使用CodeLLDB插件，安装完成之后配置`launch.json`：

&lt;syntaxhighlight lang="json"&gt;
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Launch debug",
            "type": "lldb",
            "request": "launch",
            "program": "${command:cmake.launchTargetPath}",
            "args": [],
            "stopAtEntry": false,
            "cwd": "${workspaceFolder}/build",
            "environment": []
        }
    ]
}
&lt;/syntaxhighlight&gt;

这样终于不会在我调试googletest的时候死掉了，

[[File:Vscode-debug-codelldb.png|600px|CodeLLDB]]


= Qt Creator=

= KDevelop=


* [https://github.com/microsoft/vscode-cmake-tools/issues/965 Can't debug in Visual Studio Code #965]
* [https://vector-of-bool.github.io/docs/vscode-cmake-tools/debugging.html#debugging Target Debugging and Launching]</text>
      <sha1>9epehdgjtyxkcifvr8cig9rj38t407o</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:C++中的NRVO</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2019-09-25T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>对于C++这种需要精细管理对象的语言来说有时候真是比较复杂，一个看似简单的问题一直在困惑着我：到底可不可以在方法中返回局部变量呢？</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3002" sha1="bik5r17ui8ytzky8cdyw6vbvzwoczb8">对于C++这种需要精细管理对象的语言来说有时候真是比较复杂，一个看似简单的问题一直在困惑着我：到底可不可以在方法中返回局部变量呢？

= 可以返回临时变量=

答案是肯定的，如果我们在一个方法中返回了临时变量，这个临时变量实际上是在栈里面的，当执行完方法后栈就销毁了，那么为什么我们还可以这样做呢？来看一个例子：

&lt;syntaxhighlight lang="c++"&gt;
#include &lt;iostream&gt;
using namespace std;

class Value {
    public:
        Value(int _m):m(_m) { std::cout &lt;&lt; "test constructor" &lt;&lt; m &lt;&lt; std::endl; }
        Value(const Value&amp; t) { 
            std::cout &lt;&lt; "test copy constructor" &lt;&lt; m &lt;&lt; std::endl; 
            this-&gt;m = t.m;
        }
        ~Value() { std::cout &lt;&lt; "test destructor" &lt;&lt; m &lt;&lt; std::endl; }
        void print() { std::cout &lt;&lt; "m:" &lt;&lt; m &lt;&lt; std::endl; }
    private:
        int m;
};

class Producer {
public:
    Value produce(int i) {
        Value t(i);
        return t;
    }
};

int main(int argc, char* argv[]) {
    std::cout &lt;&lt; "Hello world!" &lt;&lt; std::endl;
    Producer p;
    Value t = p.produce(100);
    t.print();
    return 1;
}

&lt;/syntaxhighlight&gt;
执行的结果是：

&lt;pre&gt;
Hello world!
test constructor100
m:100
test destructor100
&lt;/pre&gt;
结果证明这样做其实是可以取到我们定义的值的，这么做可行的原因是，实际上，编译器会帮我们把临时变量拷贝一份出来，所以即便栈销毁了，我们也能够拿到新的值。

= 不要返回临时变量的引用=

那么，如果我们返回临时变量的引用呢？
&lt;syntaxhighlight lang="c++"&gt;
class Producer {
public:
    Value* produce(int i){
        Value t(i);
        return &amp;t;
    }
};
&lt;/syntaxhighlight&gt;
这样做得到的结果是不对的：
&lt;pre&gt;
test.cpp:21:17: warning: address of stack memory associated with local variable 't' returned [-Wreturn-stack-address]
        return &amp;t;
                ^
1 warning generated.
Hello world!
test constructor100
test destructor100
m:-327065280
&lt;/pre&gt;
编译器会有一个警告，尽管我们仍旧可以运行我们的代码，但是实际上我们得到的值是不对的。

= NRVO机制=
那么，既然我们返回临时对象的值，实际上会得到一个拷贝的对象，那么如果我们有拷贝构造函数，是不是应该被调用呢？

然而在前面的例子中，拷贝构造函数并没有被调用到，这又是为什么呢？答案就是因为NRVO(Return Value Optimization)。这是c++11中的特性。我们首先可以尝试禁用掉这个特性，看看会发生什么。

&lt;pre&gt;
hfli@CNhfli ~ $ g++ -fno-elide-constructors test.cpp
hfli@CNhfli ~ $ ./a.out
Hello world!
test constructor100
test copy constructor0
test destructor100
test copy constructor0
test destructor100
m:100
test destructor100
&lt;/pre&gt;
可以看出，拷贝构造函数调用了两次，第一次是在produce函数中返回的时候，第二次是我们在给变量赋值的时候。</text>
      <sha1>bik5r17ui8ytzky8cdyw6vbvzwoczb8</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:Visual Studio Code for C++ development on MacOS</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2018-12-18T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>I've tried lot's of c++ IDEs on MacOS X, but none of them is as powerful as VS Studio on Windows. It's always hard for me to choose an IDE before I want to write some code, as it's called the *Selection phobia*. Generally we have the following choose:

* Vim/Emacs (I'm familiar with VIM but it's still not an easy way, for me)
* CodeBlocks (not good maintained on MacOS)
* CodeLite (it's a good choose!)
* XCode (I just don't like it, **Heavy** and ugly, can't get used to it)
* QT Creator (it's useful especially when developing Qt projects)
* Eclipse CDT
* NetBeans
* CLion (maybe the best c++ IDE on MaxOS, unfortunately does not have a free version)
* Textmate

Recently I tried Visual Studio Code, it's really a good choose for those who want to write some c++ code in a lightweight IDE.</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2665" sha1="lmjwv4s8840n9ab5uel27wjghr1brra">I've tried lot's of c++ IDEs on MacOS X, but none of them is as powerful as VS Studio on Windows. It's always hard for me to choose an IDE before I want to write some code, as it's called the *Selection phobia*. Generally we have the following choose:

* Vim/Emacs (I'm familiar with VIM but it's still not an easy way, for me)
* CodeBlocks (not good maintained on MacOS)
* CodeLite (it's a good choose!)
* XCode (I just don't like it, **Heavy** and ugly, can't get used to it)
* QT Creator (it's useful especially when developing Qt projects)
* Eclipse CDT
* NetBeans
* CLion (maybe the best c++ IDE on MaxOS, unfortunately does not have a free version)
* Textmate

Recently I tried Visual Studio Code, it's really a good choose for those who want to write some c++ code in a lightweight IDE.

= Setup Visual Studio Code for C++ development=

First we have to install [https://code.visualstudio.com/ Visual Studio Code], and a few extensions:

* C/C++
* Easy C++ projects

After installed those extensions, reload the editor to activate them. Now we can create a project:

* Choose File &gt; Open folder to open a work directory
* Press F1 and type "c++", then select "Create new C++ project" command.

After the above steps a new project with Makefile is generated.

= Common usage=
To debug or run the project, just click the button on the bottom status bar, it's easy:

[[File:vscode_debugging.png|600px|visual code studio snapshot]]

to run other commands, you could just press F1 and guess, for example, to format the code, just search "format" and then you got a choice.


= Update: How to setup eclipse CDT in MacOX=

Recently I tried to use eclipse-cdt with cmake build system, there are a few tips:

* Need to install `cmake` and `ninjia`
* Need to start eclipse from command line, this is a [https://www.wfbsoftware.de/2019/01/12/eclipse-cdt-c-cmake-on-mac/ bug]

In order to debug, basiclly eclipse only supports gdb, which is has been replaced with lldb in MacOS, so a few setps needed to make it working:

* install gdb via mac ports, `sudo port install gdb`
* after installed, it's located in `/opt/local/bin/ggdb`
* create an alias for gdb in your bash profile(eg. `~/.zshrc`), `alias gdb=ggdb`
* codesign for gdb, following [https://www.thomasvitale.com/how-to-setup-gdb-and-eclipse-to-debug-c-files-on-macos-sierra/ How to setup gdb and Eclipse to debug C++ files on macOS Mojave]
* create gdb init file: `echo "set startup-with-shell off" &gt; ~/gdbinit`
* need to start eclipse from terminal, otherwise it will not recongnize gdb

Reference:

* [https://dev.to/acharluk/developing-c-with-visual-studio-code-4pb9 Developing C++ with Visual Studio Code]</text>
      <sha1>lmjwv4s8840n9ab5uel27wjghr1brra</sha1>
    </revision>
  </page>
  <page>
    <title>Blog:IEEE 754浮点数转换</title>
    <ns>3000</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2019-06-27T00:00:00Z</timestamp>
      <contributor>
        <username>Riguz</username>
        <id>1</id>
      </contributor>
      <comment>一个小数的二进制是怎么样的呢？我们先看看一个二进制的小数怎么转换成十进制：
$$
\begin{aligned}	
11101.01011_{10} &amp;= 1 \times 2^{4} + 1 \times 2^{3} + 1 \times 2^{2} + 0 \times 2^{1} + 1 \times 2^{0} + 0 \times 2^{-1} + 1 \times 2^{-2} + 1 \times 2^{-3} + 1 \times 2^{-4} + 1 \times 2^{-5} \\
  &amp;= 16 + 8 + 4 + 0 + 1 + 0 + \frac{1}{2} + 0 + \frac{1}{16} + \frac{1}{32} \\
  &amp;= 29.34375
\end{aligned}
$$</comment>
      <origin>1</origin>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5766" sha1="ttvlg43eqp79csoosnpmknz0zbp6dd9">一个小数的二进制是怎么样的呢？我们先看看一个二进制的小数怎么转换成十进制：
$$
\begin{aligned}	
11101.01011_{10} &amp;= 1 \times 2^{4} + 1 \times 2^{3} + 1 \times 2^{2} + 0 \times 2^{1} + 1 \times 2^{0} + 0 \times 2^{-1} + 1 \times 2^{-2} + 1 \times 2^{-3} + 1 \times 2^{-4} + 1 \times 2^{-5} \\
  &amp;= 16 + 8 + 4 + 0 + 1 + 0 + \frac{1}{2} + 0 + \frac{1}{16} + \frac{1}{32} \\
  &amp;= 29.34375
\end{aligned}
$$

= IEEE 754=
[https://en.wikipedia.org/wiki/IEEE_754 IEEE 754] 标准中规定了浮点数在计算机中的表示方法，主要就是单精度(float)和双精度(double):

&lt;pre&gt;
              S Exp      Fraction
Single(32bit) ▯▮▮▮▮▮▮▮▮▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯
Double(64bit) ▯▮▮▮▮▮▮▮▮▮▮▮▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯
&lt;/pre&gt;
其计算公式为：
$$
x = (-1)^{S}\times(1+ Fraction)\times 2^{(Exponent-Bias)}
$$
其中，Bias为[https://zh.wikipedia.org/wiki/IEEE_754#%E6%8C%87%E6%95%B8%E5%81%8F%E7%A7%BB%E5%80%BC 指数偏移值]，是一个固定值，即$Bias=2^{e-1} - 1$ 其中e为指数部分的比特长度。

* 单精度$Bias = 2^{7} - 1 = 127$
* 双精度$Bias = 2^{10} -1 = 1023$

举个例子，刚才我们算出来的小数可以这样表示：

$$
\begin{aligned}	
29.34375 &amp;= 11101.01011 = 11101.01011_{2} \times 2^{0} \\
    &amp;= 1.110101011_{2} \times 2^{4} \\
    &amp;= (-1)^{0} \times (1 + 0.110101011_{2}) \times 2^{131 - 127} \\
    &amp;= (-1)^{0} \times (1 + 0.110101011_{2}) \times 2^{10000011_{2} - 127} \\
    &amp;= (-1)^{0} \times (1 + 0.110101011_{2}) \times 2^{1027 - 1023} \\
    &amp;= (-1)^{0} \times (1 + 0.110101011_{2}) \times 2^{10000000011_{2} - 127} \\
\end{aligned}
$$

因此在计算机中，表示为：
&lt;pre&gt;
Float:
▯▮▮▮▮▮▮▮▮▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯
010000011110101011..............

Double:
▯▮▮▮▮▮▮▮▮▮▮▮▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯
010000000011110101011...........................................
&lt;/pre&gt;
不足的部分补上0，即：$0100000111101010110000000000000_{2}=41eac000{16}$

除了正常的浮点数外，还有几个比较特殊的：

Description         Float(32bit)
------------------  --------------------------------------------  
Zero	            0 00000000 00000000000000000000000
Negative Zero	    1 00000000 00000000000000000000000
Infinity	        0 11111111 00000000000000000000000
Negative Infinity	1 11111111 00000000000000000000000
Not a Number (NaN)	0 11111111 00001000000000100001000

= 十进制与二进制转换=
计算方式为，将小数的整数部分与2取余倒序排列；将小数部分与2取整正序排列。例如，将3.14转换为float:

- 首先将整数部分直接转换为二进制 $3_{10} = 11_{2}$
    - $3\mod2 = \fbox{1}$
    - $1\mod2 = \fbox{1}$
- 小数部分为0.14，不断乘以2后取整数部分，然后用小数继续乘以2直到值为1
    - $0.14 \times 2 = \fbox{0}.28$ 
    - $0.28 \times 2 = \fbox{0}.56$
    - $0.56 \times 2 = \fbox{1}.12$
    - $0.12 \times 2 = \fbox{0}.24$
    - $0.24 \times 2 = \fbox{0}.48$
    - $0.48 \times 2 = \fbox{0}.96$
    - $0.96 \times 2 = \fbox{1}.92$
    - $0.92 \times 2 = \fbox{1}.84$ 
    - .....
    - 重复以上步骤，得到$0.14_{10}=0.001000111101011100001010001111010..._{2}$

= 舍入操作=
对于尾数多余精度的情况，需要舍去多余的部分，但不是按照四舍五入的方式，而是按照”向偶舍入“的方式，意思就是，如果多余的部分大于0.5($0.5_{10} = 0.1_{2})$则最低位进1；如果小于0.5则舍去；如果正好是等于0.5则根据最低位判断，如果最低位是1则进位，否则舍去。这样按照统计学来看，对于一个小数有相同的机会进位或者被舍去。

例如对于上例中的3.14，我们可以得到：

$$
\begin{aligned}	
3.14 &amp;= 11.001000111101011100001010001111010..._{2} \\
    &amp;= 1.1001000111101011100001010001111010..._{2} \times 2^{1} \\
    &amp;= (-1)^{0} + (1 + 0.1001000111101011100001010001111010..._{2}) \times 2^{128 - 127} \\
    &amp;= (-1)^{0} + (1 + 0.10010001111010111000010{\color{blue}{10001111010...}}_{2}) \times 2^{10000000_{2} - 127} \\
    &amp;\approx (-1)^{0} + (1 + 0.1001000111101011100001{\color{red}1}_{2}) \times 2^{10000000_{2} - 127}
\end{aligned}
$$

因此3.14的float表示为：

&lt;pre&gt;
▯▮▮▮▮▮▮▮▮▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯
01000000010010001111010111000011
&lt;/pre&gt;

= 还原=
那么，对于一个存储在磁盘上的浮点数，我们怎么将它加载到内存中来？对于C来说，实际也是采用的IEEE754标准(float, double)，所以实际上浮点数在内存中的表示是一致的，直接转换即可：

&lt;syntaxhighlight lang="c++"&gt;
struct ConstantFloat {
		mutable u4 bytes;
		float &amp;getValue() const {
			return *reinterpret_cast&lt;float *&gt;(&amp;bytes);
		}
	};
&lt;/syntaxhighlight&gt;



参考：

* [http://sandbox.mc.edu/~bennet/cs110/flt/dtof.html Decimal to Floating-Point Conversions]
* [http://cs.boisestate.edu/~alark/cs354/lectures/ieee754.pdf IEEE 754 FLOATING POINT REPRESENTATION]
* [https://www.h-schmidt.net/FloatConverter/IEEE754.html IEEE-754 Floating Point Converter]
* [https://www.rapidtables.com/convert/number/binary-to-decimal.html Binary to Decimal converter]
* [https://www.jianshu.com/p/e5d72d764f2f IEEE754表示浮点数]
* [http://www.binaryconvert.com/result_double.html?decimal=050057046051052051055053 Online Binary-Decimal Converter]</text>
      <sha1>ttvlg43eqp79csoosnpmknz0zbp6dd9</sha1>
    </revision>
  </page>
</mediawiki>
